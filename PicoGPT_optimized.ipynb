{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copyright joshuah.rainstar@gmail.com\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "class ZLSGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # smooth gate\n",
    "        x = 4.0 * x\n",
    "        sp = F.softplus(x)\n",
    "        sa = torch.sigmoid(0.5 * x)\n",
    "        ba = sa * (1.0 - sa)\n",
    "        z = sp - 2.772588722239781 * ba  # 4 * ln(2)\n",
    "        return 1.0 - torch.exp(-(z + 1e-8))\n",
    "\n",
    "def _norm(v, eps: float = 1e-12):\n",
    "    return torch.linalg.vector_norm(v, dim=-1, keepdim=True).clamp_min(eps)\n",
    "\n",
    "\n",
    "def _unit(v, eps: float = 1e-12):\n",
    "    return v / _norm(v, eps)\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def phase_transport_between(\n",
    "    curr: torch.Tensor,\n",
    "    prev: torch.Tensor,\n",
    "    tau: float = 1e-6,          # semantic threshold (unchanged)\n",
    "    eps: float = 1e-12          # numeric epsilon (NEW: decoupled from tau)\n",
    ") -> torch.Tensor:\n",
    "    assert curr.shape == prev.shape and curr.dim() == 3\n",
    "    B, T, C = curr.shape\n",
    "\n",
    "    # Units (reuse norms) â€” clamp with eps (NOT tau)\n",
    "    nu = torch.linalg.vector_norm(curr, dim=-1, keepdim=True).clamp_min(eps)   # (B,T,1)\n",
    "    nv = torch.linalg.vector_norm(prev, dim=-1, keepdim=True).clamp_min(eps)   # (B,T,1)\n",
    "    u = curr / nu\n",
    "    v = prev / nv\n",
    "\n",
    "    w = curr - prev\n",
    "    c = (u * v).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "\n",
    "    # Masks (semantic thresholds use tau)\n",
    "    near_pos = (c >  1.0 - tau)                                                # (B,T,1)\n",
    "    near_neg = (c < -1.0 + tau)                                                # (B,T,1)\n",
    "    small_u  = (nu < tau)                                                      # (B,T,1)\n",
    "    small_v  = (nv < tau)                                                      # (B,T,1)\n",
    "    trivial  = near_pos | small_u | small_v                                    # (B,T,1)\n",
    "\n",
    "    # General branch\n",
    "    denom = (1.0 + c).clamp_min(eps)                                           # (B,T,1)\n",
    "    a = (v * w).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "    b = (u * w).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "    Kw  = u * a - v * b                                                        # (B,T,C)\n",
    "    K2w = u * (a * c - b) + v * (b * c - a)                                    # (B,T,C)\n",
    "    y_gen = w - Kw + (K2w / denom)                                             # (B,T,C)\n",
    "\n",
    "    # Antipodal candidate\n",
    "    if C == 1:\n",
    "        y_neg = -w\n",
    "    else:\n",
    "        # Keep this normalization stable with eps as well\n",
    "        idx = torch.argmin(v.abs().reshape(-1, C), dim=1, keepdim=True)        # (B*T,1)\n",
    "        s = v.reshape(-1, C).gather(1, idx)                                    # (B*T,1)\n",
    "        p = -s * v.reshape(-1, C)\n",
    "        onehot = F.one_hot(idx.squeeze(-1), num_classes=C).to(s.dtype)\n",
    "        p = p + onehot\n",
    "        n = torch.linalg.vector_norm(p, dim=1, keepdim=True).clamp_min(eps)\n",
    "        p = (p / n).view(B, T, C)\n",
    "        proj_v = (v * w).sum(dim=-1, keepdim=True) * v                         # (B,T,C)\n",
    "        proj_p = (p * w).sum(dim=-1, keepdim=True) * p                         # (B,T,C)\n",
    "        y_neg = w - 2.0 * proj_v - 2.0 * proj_p\n",
    "\n",
    "    # Fuse selections\n",
    "    y = torch.where(trivial, w, y_gen)\n",
    "    y = torch.where(near_neg, y_neg, y)\n",
    "    return y\n",
    "\n",
    "# ===========================================================\n",
    "# Multi-scale features (vectorized pyramid)\n",
    "# ===========================================================\n",
    "class CausalCentroidPyramid(nn.Module):\n",
    "    \"\"\"Identical outputs to CausalCentroidPyramid, but faster.\n",
    "\n",
    "    Key changes:\n",
    "    - Builds all dyadic centroids directly via cumsum (no sequential dependency).\n",
    "    - Computes all cluster deltas in a single batched call to phase_transport_between.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, tau: float = 1e-6):\n",
    "        super().__init__()\n",
    "        assert num_scales >= 1\n",
    "        self.K = num_scales\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor, mask_early: bool = True) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "    \n",
    "        # token-level PT (scale-1)\n",
    "        prev_tok = torch.zeros_like(x)\n",
    "        if T > 1:\n",
    "            prev_tok[:, 1:, :] = x[:, :-1, :].contiguous()\n",
    "        d1 = phase_transport_between(x, prev_tok, tau=self.tau)  # (B,T,C)\n",
    "        if mask_early:\n",
    "            d1[:, :1, :].zero_()\n",
    "        if self.K == 1:\n",
    "            return d1.unsqueeze(2)\n",
    "    \n",
    "        # constants (avoid .item() / data-dependent Python ints)\n",
    "        K1 = self.K - 1\n",
    "        W_vec = (2 ** torch.arange(1, self.K, device=device, dtype=torch.long))  # (K1,)\n",
    "        Wmax = (1 << (self.K - 1)) if self.K > 1 else 1  # Python int\n",
    "    \n",
    "        # dyadic centroids via windowed means (vectorized)\n",
    "        csum = torch.cumsum(x, dim=1)  # (B,T,C)\n",
    "        csum_pad = torch.cat([torch.zeros(B, 1, C, device=device, dtype=dtype), csum], dim=1)  # (B,T+1,C)\n",
    "    \n",
    "        t_end = torch.arange(1, T + 1, device=device, dtype=torch.long)                         # (T,)\n",
    "        idx_start_jt = (t_end.unsqueeze(0) - W_vec.unsqueeze(1)).clamp_min(0)                  # (K1,T)\n",
    "        idx_start_tk = idx_start_jt.transpose(0, 1).contiguous()                                # (T,K1)\n",
    "        idx_end_tk = t_end.unsqueeze(1).expand(T, K1).contiguous()                              # (T,K1)\n",
    "    \n",
    "        csum_ext = csum_pad.unsqueeze(2).expand(B, T + 1, K1, C)                                # (B,T+1,K1,C)\n",
    "    \n",
    "        gather_shape = (B, T, K1, C)\n",
    "        idx_start = idx_start_tk.unsqueeze(0).unsqueeze(-1).expand(gather_shape)                # (B,T,K1,C)\n",
    "        idx_end = idx_end_tk.unsqueeze(0).unsqueeze(-1).expand(gather_shape)                    # (B,T,K1,C)\n",
    "    \n",
    "        start_vals = torch.gather(csum_ext, dim=1, index=idx_start)\n",
    "        end_vals = torch.gather(csum_ext, dim=1, index=idx_end)\n",
    "        window_sums = end_vals - start_vals                                                     # (B,T,K1,C)\n",
    "        mu_all = window_sums / W_vec.to(dtype).view(1, 1, -1, 1)                                # (B,T,K1,C)\n",
    "    \n",
    "        if mask_early:\n",
    "            t_idx = torch.arange(T, device=device).unsqueeze(1)                                 # (T,1)\n",
    "            valid_mu = (t_idx >= (W_vec - 1).view(1, -1))                                       # (T,K1)\n",
    "            mu_all = mu_all * valid_mu.view(1, T, -1, 1)\n",
    "    \n",
    "        # previous centroids (shift by W per scale), vectorized with padding\n",
    "        mu_pad = torch.cat([torch.zeros(B, Wmax, K1, C, device=device, dtype=dtype), mu_all], dim=1)  # (B,Wmax+T,K1,C)\n",
    "        idx_prev_tk = torch.arange(T, device=device).unsqueeze(1) - W_vec.view(1, -1) + Wmax          # (T,K1)\n",
    "        idx_prev = idx_prev_tk.unsqueeze(0).unsqueeze(-1).expand(gather_shape)                        # (B,T,K1,C)\n",
    "        prev_mu_all = torch.gather(mu_pad, dim=1, index=idx_prev)                                     # (B,T,K1,C)\n",
    "    \n",
    "        # all cluster deltas in one batched PT call\n",
    "        mu_flat = mu_all.reshape(B * K1, T, C).contiguous()\n",
    "        prev_flat = prev_mu_all.reshape(B * K1, T, C).contiguous()\n",
    "        d_flat = phase_transport_between(mu_flat, prev_flat, tau=self.tau)                            # (B*K1,T,C)\n",
    "        d_clusters = d_flat.view(B, T, K1, C)\n",
    "    \n",
    "        if mask_early:\n",
    "            valid_d = (torch.arange(T, device=device).unsqueeze(1) >= W_vec.view(1, -1))              # (T,K1)\n",
    "            d_clusters = d_clusters * valid_d.view(1, T, -1, 1)\n",
    "    \n",
    "        return torch.cat([d1.unsqueeze(2), d_clusters], dim=2)  # (B,T,K,C)\n",
    "\n",
    "        # ----- STREAMING STATE FOR INFERENCE -----\n",
    "class CausalPyramidState:\n",
    "    \"\"\"\n",
    "    O(K) step-time updates, no recompute.\n",
    "    For level â„“ we keep a ring buffer of length 2^â„“ storing Î¼_â„“ (with Î¼_0=x).\n",
    "    That suffices both to:\n",
    "      - build Î¼_{â„“+1}(t) from Î¼_â„“(t) and Î¼_â„“(t-2^â„“)\n",
    "      - compute deltas at scale s=â„“ via Î¼_s(t-2^s)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, C: int, device, batch_size: int = 1, tau: float = 1e-6):\n",
    "        self.K = num_scales\n",
    "        self.C = C\n",
    "        self.B = batch_size\n",
    "        self.device = device\n",
    "        self.tau = float(tau)\n",
    "        self.t = 0  # number of tokens processed so far\n",
    "\n",
    "        # ring buffers: list over levels â„“ = 0..K-1, each [B, L=2^â„“, C]\n",
    "        self.buffers = []\n",
    "        self.ptrs = []\n",
    "        for l in range(self.K):\n",
    "            L = 1 << l\n",
    "            self.buffers.append(torch.zeros(self.B, L, C, device=device))\n",
    "            self.ptrs.append(0)\n",
    "\n",
    "    def _read_lookback(self, level: int, r: int):\n",
    "        \"\"\"return Î¼_level(t - r); zeros if not enough history yet\"\"\"\n",
    "        if self.t < r:\n",
    "            return torch.zeros(self.B, self.C, device=self.device)\n",
    "        L = self.buffers[level].size(1)\n",
    "        idx = (self.ptrs[level] - r) % L\n",
    "        return self.buffers[level][:, idx, :]\n",
    "\n",
    "    def _push(self, level: int, value: torch.Tensor):\n",
    "        \"\"\"write current Î¼_level(t) and advance ptr\"\"\"\n",
    "        L = self.buffers[level].size(1)\n",
    "        self.buffers[level][:, self.ptrs[level], :] = value\n",
    "        self.ptrs[level] = (self.ptrs[level] + 1) % L\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x_t: (B, C)\n",
    "        returns d(t): (B, K, C)  [token PT + (K-1) cluster PTs]\n",
    "        \"\"\"\n",
    "        B, C = x_t.shape\n",
    "        feats = []\n",
    "\n",
    "        # ------- token PT (read BEFORE any push) -------\n",
    "        prev_x = self._read_lookback(level=0, r=1)  # Î¼0(t-1)\n",
    "        d1 = phase_transport_between(x_t[:, None, :], prev_x[:, None, :], tau=self.tau).squeeze(1)\n",
    "        if self.t == 0:\n",
    "            d1.zero_()\n",
    "        feats.append(d1)\n",
    "\n",
    "        # ------- (A) compute all Î¼_s(t) with pre-push lookbacks -------\n",
    "        mu_curr = [None] * self.K\n",
    "        mu_curr[0] = x_t                      # Î¼0(t)\n",
    "        mu_prev = x_t\n",
    "        for s in range(1, self.K):\n",
    "            W1 = 1 << (s - 1)\n",
    "            W  = 1 << s\n",
    "            mu_back = self._read_lookback(level=s-1, r=W1)   # Î¼_{s-1}(t - 2^{s-1})  (pre-push!)\n",
    "            mu_s_t  = 0.5 * (mu_prev + mu_back)              # Î¼_s(t)\n",
    "            if self.t < (W - 1):                             # early mask (global t)\n",
    "                mu_s_t.zero_()\n",
    "            mu_curr[s] = mu_s_t\n",
    "            mu_prev = mu_s_t\n",
    "\n",
    "        # ------- (B) compute all deltas d_s using Î¼_s(tâˆ’W) (pre-push) -------\n",
    "        for s in range(1, self.K):\n",
    "            W = 1 << s\n",
    "            mu_prevW = self._read_lookback(level=s, r=W)     # Î¼_s(t - 2^s)  (pre-push!)\n",
    "            d_s = phase_transport_between(mu_curr[s][:, None, :], mu_prevW[:, None, :], tau=self.tau).squeeze(1)\n",
    "            if self.t + 1 <= W:\n",
    "                d_s.zero_()\n",
    "            feats.append(d_s)\n",
    "\n",
    "        # ------- (C) push Î¼_â„“(t) for all levels, exactly once -------\n",
    "        self._push(level=0, value=mu_curr[0])\n",
    "        for s in range(1, self.K):\n",
    "            self._push(level=s, value=mu_curr[s])\n",
    "\n",
    "        self.t += 1\n",
    "        return torch.stack(feats, dim=1)  # (B, K, C)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SemanticClusterFeaturesCausal(nn.Module):\n",
    "    \"\"\"\n",
    "    Unified wrapper:\n",
    "      - forward(x): vectorized for training\n",
    "      - step(x_t, state): single-step for inference with cache\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, tau: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.pyramid = CausalCentroidPyramid(num_scales=num_scales, tau=tau)\n",
    "        self.K = num_scales\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pyramid(x)  # (B,T,K,C)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor, state: CausalPyramidState) -> torch.Tensor:\n",
    "        return state.step(x_t)  # (B,K,C)\n",
    "\n",
    "\n",
    "class GroupedChannelMLP(nn.Module):\n",
    "    def __init__(self, k_dim: int, c_dim: int):\n",
    "        super().__init__()\n",
    "        hidden_dim = c_dim *4\n",
    "        self.k_dim = k_dim\n",
    "        self.c_dim = c_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # shapes chosen for direct einsum without expands\n",
    "        # fc1: (K, H, C)   fc2: (K, C, H)   b2: (K, C)\n",
    "        self.fc1_weight = nn.Parameter(torch.empty(k_dim, hidden_dim, c_dim*2))\n",
    "        self.fc2_weight = nn.Parameter(torch.empty(k_dim, c_dim, hidden_dim))\n",
    "        self.fc2_bias   = nn.Parameter(torch.empty(k_dim, c_dim))\n",
    "        self.act=ZLSGate()\n",
    "    \n",
    "        nn.init.kaiming_uniform_(self.fc1_weight, a=5**0.5)\n",
    "        nn.init.kaiming_uniform_(self.fc2_weight, a=5**0.5)\n",
    "        nn.init.zeros_(self.fc2_bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, K, C) or (B, K, C)\n",
    "        returns: same leading dims, last two dims (K,C)\n",
    "        \"\"\"\n",
    "        squeeze_time = False\n",
    "        if x.dim() == 3:  # (B,K,C)\n",
    "            x = x.unsqueeze(1)  # -> (B,1,K,C)\n",
    "            squeeze_time = True\n",
    "        elif x.dim() != 4:\n",
    "            raise ValueError(\"Input must be (B,K,C) or (B,T,K,C)\")\n",
    "\n",
    "        # (B,T,K,C) x (K,H,C) -> (B,T,K,H)\n",
    "        h = torch.einsum('btkc,khc->btkh', x, self.fc1_weight)\n",
    "        h = self.act(h)\n",
    "\n",
    "        # (B,T,K,H) x (K,C,H) -> (B,T,K,C)\n",
    "        y = torch.einsum('btkh,kch->btkc', h, self.fc2_weight) + self.fc2_bias\n",
    "\n",
    "        if squeeze_time:\n",
    "            y = y[:, 0, :, :]  # (B,K,C)\n",
    "        return y\n",
    "        \n",
    "        \n",
    "class Cell(nn.Module):\n",
    "    def __init__(self, dim_in: int, hidden: int,dim_out:int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, hidden, bias=True) \n",
    "        self.fc2 = nn.Linear(hidden, dim_out, bias=True)\n",
    "        self.act = ZLSGate()\n",
    "    def forward(self, x):\n",
    "      \n",
    "        return self.fc2(self.act(self.fc1(x))) \n",
    "\n",
    "\n",
    "class GPTSemanticBlock(nn.Module):\n",
    "    def __init__(self, config: GPTConfig,features):\n",
    "        super().__init__()\n",
    "        C = config.n_embd\n",
    "        self.C = C\n",
    "        self.K = config.n_scales\n",
    "        # L = number of feature groups concatenated: token (1) + K scales\n",
    "        self.L = 1 + self.K\n",
    "        self.features = features #reuse to reduce param/mechanism counts\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.ln = nn.LayerNorm(self.C)\n",
    "        self.mlp = Cell(self.C*self.L,self.C*4,self.C)\n",
    "  \n",
    "\n",
    "    # vectorized\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        feats = self.features(x)               # (B, T, K, C)\n",
    "        feats = feats.reshape(B,T,self.K*self.C)\n",
    "        x_in = torch.cat([x, feats], dim=-1) #location hint must be issued\n",
    "        out = self.drop(self.ln(self.mlp(x_in)))\n",
    "        return out\n",
    "\n",
    "    # single-step incremental\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor, feat_state: CausalPyramidState) -> torch.Tensor:\n",
    "        # x_t: (B, C)\n",
    "        B, C = x_t.shape\n",
    "        feats_t = self.features.step(x_t, feat_state)  # (B, K, C)\n",
    "        feats_t = feats_t.reshape(B,self.K*self.C)\n",
    "        x_in = torch.cat([x_t, feats_t], dim=-1) #location hint must be issued\n",
    "        out = self.drop(self.ln(self.mlp(x_in)))\n",
    "        return out\n",
    "\n",
    "        #todo: figure out the semantic grabber/preservation mechanism\n",
    "        #this is more complex than simple.\n",
    "        #especially if it evolves- for everywhere in the state-\n",
    "        #as the context grows.\n",
    "\n",
    "\n",
    "def _is_prime(n: int) -> bool:\n",
    "    if n < 2: return False\n",
    "    if n % 2 == 0: return n == 2\n",
    "    r = int(n**0.5)\n",
    "    for f in range(3, r+1, 2):\n",
    "        if n % f == 0: return False\n",
    "    return True\n",
    "\n",
    "def _factorize(n: int):\n",
    "    f, cnt = [], {}\n",
    "    d = 2\n",
    "    while d * d <= n:\n",
    "        while n % d == 0:\n",
    "            cnt[d] = cnt.get(d, 0) + 1\n",
    "            n //= d\n",
    "        d += 1 if d == 2 else 2\n",
    "    if n > 1: cnt[n] = cnt.get(n, 0) + 1\n",
    "    return list(cnt.keys())\n",
    "\n",
    "def _primitive_root(p: int) -> int:\n",
    "    # p must be prime\n",
    "    phi = p - 1\n",
    "    factors = _factorize(phi)\n",
    "    for g in range(2, p):\n",
    "        ok = True\n",
    "        for q in factors:\n",
    "            if pow(g, phi // q, p) == 1:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            return g\n",
    "    raise RuntimeError(\"no primitive root found\")\n",
    "\n",
    "def _welch_costas_perm(V: int, device=None):\n",
    "    \"\"\"\n",
    "    Welch Costas permutation Ïƒ on {0..V-1}, where V = p-1 for prime p.\n",
    "    Ïƒ[i] = g^(i+1) mod p, mapped to 0..V-1 by subtracting 1.\n",
    "    \"\"\"\n",
    "    p = V + 1\n",
    "    if not _is_prime(p):\n",
    "        return None\n",
    "    g = _primitive_root(p)\n",
    "    sigma = torch.empty(V, dtype=torch.long, device=device)\n",
    "    for i in range(V):\n",
    "        sigma[i] = pow(g, i + 1, p) - 1\n",
    "    return sigma  # permutation of 0..V-1\n",
    "\n",
    "def _coprime_mul_perm(V: int, device=None):\n",
    "    \"\"\"\n",
    "    Fallback: Ïƒ[i] = (a*i + b) % V with gcd(a, V)=1 and a not â‰¡ Â±1 mod V.\n",
    "    Not Costas, but non-monotone and well-distributed.\n",
    "    \"\"\"\n",
    "    # pick a\n",
    "    a = None\n",
    "    for cand in range(2, V):\n",
    "        if math.gcd(cand, V) == 1 and cand % V not in (1, V-1):\n",
    "            a = cand\n",
    "            break\n",
    "    if a is None:\n",
    "        a = 1  # degenerate small V\n",
    "    b = V // 3\n",
    "    i = torch.arange(V, device=device)\n",
    "    return ((a * i + b) % V).long()\n",
    "\n",
    "def _perm_inverse(sigma: torch.Tensor) -> torch.Tensor:\n",
    "    inv = torch.empty_like(sigma)\n",
    "    inv[sigma] = torch.arange(sigma.numel(), device=sigma.device)\n",
    "    return inv\n",
    "\n",
    "class FlatRollEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Replacement for nn.Embedding that maps token id i -> cyclic roll^i of a base\n",
    "    length-V vector whose non-DC spectrum is flat (DC=0). Requires V == n_embd.\n",
    "    Weights are frozen by default.\n",
    "    this yields an optimal embedding that is considered perfect.\n",
    "    The 'eye' is mixed at 0.5 and then rows are permuted by a Costas-like order\n",
    "    to maximize uniqueness while keeping even collapse.\n",
    "    but wait, you're asking, my embeds/vocab is not orthagonal!\n",
    "    the solution is simple, clever, efficient- \n",
    "    use  Smooth full-space rotation matrix via Lie algebra exponential map.\n",
    "        A = exp(tÂ·G), where G âˆˆ so(D) is skew-symmetric and full-rank.\n",
    "    partition vocab idx space by modulo over chosen block size, use different rotation\n",
    "    range from 0 to pi(evenly divided) for all partitions, use ONE embed matrix,\n",
    "    embed->shift. Minimizes necessary parameter count. up-project to desired embed dim.\n",
    "    for decoder, you're operating over a larger dimensional space as-is. that's fine.\n",
    "    if you like, you can try down-project and repeat-decode invert on all blocks,\n",
    "    and use stiefel inverting by transpose but use two sets of slices of rotation ranges\n",
    "    so that you have blue noise coverage with a partition going from original bound to bound\n",
    "    but also overlap going from mid to mid, try decode on all, hard route to one,\n",
    "    take logits from that one- > bam, no learned decode either.\n",
    "    down-projection tied to up-projection and you have a learned high efficiency mapping.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config, scale: str = \"box\", seed: int = 0,\n",
    "                 freeze: bool = True, dtype=None, device=None):\n",
    "        super().__init__()\n",
    "        assert config.n_embd == config.vocab_size, (\n",
    "            f\"Expected n_embd == vocab_size, got {config.n_embd} != {config.vocab_size}\"\n",
    "        )\n",
    "        V = int(config.vocab_size)\n",
    "        dtype = dtype or torch.float32\n",
    "\n",
    "        eye = torch.eye(V, dtype=dtype, device=device)\n",
    "        weight = self._make_weight(V, scale=scale, seed=seed,\n",
    "                                   dtype=dtype, device=device)  # [V, V]\n",
    "        M = int(torch.argmax(weight[0]))        # index of max in base x (row 0)\n",
    "        pm = weight[0, M]                       # scalar\n",
    "        N = 1.0 / pm\n",
    "        \n",
    "        eye = torch.roll(eye, shifts=M, dims=1) # shift spike position within each row\n",
    "        eye = eye * N\n",
    "        mixed =  weight + eye  # add identity towers\n",
    "\n",
    "        # --- compute a strong-scramble row order (Costas if possible) ---\n",
    "        sigma = _welch_costas_perm(V, device=device)\n",
    "        if sigma is None:\n",
    "            sigma = _coprime_mul_perm(V, device=device)\n",
    "        # We want ones at (row = Ïƒ[i], col = i). For row-permutation via index_select,\n",
    "        # use r_idx = Ïƒ^{-1} so that new_row j pulls old_row r_idx[j] with 1 at column j=Ïƒ[i].\n",
    "        r_idx = _perm_inverse(sigma)\n",
    "\n",
    "        # keep for reference / decoding\n",
    "        self.register_buffer(\"row_perm\", r_idx, persistent=False)\n",
    "        self.register_buffer(\"sigma\", sigma, persistent=False)\n",
    "\n",
    "        mixed = mixed.index_select(0, r_idx)\n",
    "        self.embed = nn.Embedding.from_pretrained(mixed, freeze=freeze)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _row_perm_max_equidistant(V: int, device=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Row permutation that evenly offsets the identity's '1' away from the diagonal.\n",
    "        Uses a single cyclic shift by k = floor(V/2).\n",
    "        \"\"\"\n",
    "        if V <= 1:\n",
    "            return torch.arange(V, device=device, dtype=torch.long)\n",
    "        k = V // 2\n",
    "        if k == 0:  # only happens when V == 1, handled above; keep for safety\n",
    "            k = 1\n",
    "        return ((torch.arange(V, device=device) + k) % V).long()\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_weight(V: int, scale: str = \"box\", seed: int = 0,\n",
    "                     dtype=torch.float32, device=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns a (V, V) tensor whose rows are cyclic rolls of a base vector x in R^V\n",
    "        with |FFT(x)|^2 flat for k=1..V-1 and DC=0.\n",
    "        scale:\n",
    "          - \"unit\": ||x||_2 = 1\n",
    "          - \"box\":  max|x_i| = 1\n",
    "        \"\"\"\n",
    "        # build on CPU, move at end\n",
    "        complex_dtype = torch.complex64 if dtype == torch.float32 else torch.complex128\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "        X = torch.zeros(V, dtype=complex_dtype)\n",
    "        # DC bin\n",
    "        X[0] = torch.tensor(0, dtype=complex_dtype)\n",
    "\n",
    "        if V % 2 == 0:\n",
    "            # bins 1..V/2-1 are complex-conjugate pairs; Nyquist bin must be real\n",
    "            for k in range(1, V // 2):\n",
    "                phi = torch.rand((), generator=g) * (2 * math.pi)\n",
    "                val = torch.cos(phi) + 1j * torch.sin(phi)\n",
    "                X[k] = val\n",
    "                X[V - k] = torch.conj(val)\n",
    "            X[V // 2] = 1.0 if torch.rand((), generator=g) < 0.5 else -1.0\n",
    "        else:\n",
    "            for k in range(1, (V - 1) // 2 + 1):\n",
    "                phi = torch.rand((), generator=g) * (2 * math.pi)\n",
    "                val = torch.cos(phi) + 1j * torch.sin(phi)\n",
    "                X[k] = val\n",
    "                X[V - k] = torch.conj(val)\n",
    "\n",
    "        x = torch.fft.ifft(X).real  # real length-V base vector\n",
    "\n",
    "        if scale == \"unit\":\n",
    "            x = x / (x.norm() + 1e-12)\n",
    "        elif scale == \"box\":\n",
    "            x = x / (x.abs().max() + 1e-12)\n",
    "        else:\n",
    "            raise ValueError(\"scale must be 'unit' or 'box'\")\n",
    "\n",
    "        rows = [torch.roll(x, shifts=r, dims=0) for r in range(V)]\n",
    "        W = torch.stack(rows, dim=0).to(dtype=dtype)\n",
    "        if device is not None:\n",
    "            W = W.to(device)\n",
    "        return W\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        # (batch, seq_len, V)\n",
    "        return self.embed(input_ids)\n",
    "\n",
    "\n",
    "        \n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 66 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 6\n",
    "    n_head:int = 6\n",
    "    n_embd: int = 66 #tied to vocab\n",
    "    n_scales:int = 3\n",
    "    #1 is 2, 2=4,3=8,4=16,5=32,5=64,6=128,7=256,8=512, 9 = 1024\n",
    "    #we keep signals/patterns relevant to 1024 token frames(train on 2x).\n",
    "    #however, *in PRACTICE*, the higher levels are irrelevant.\n",
    "    #we at most have a use for :\n",
    "    #1 is bigrams, 2 is syllablic-level token equivalent,\n",
    "    #3 is maybe words. We can maybe learn three.\n",
    "    #beyond that, there are no realistic concepts, rather, we need capacity to keep abstract information.\n",
    "    #furthermore we need means to *gather* meaningful concepts above this level.\n",
    "    dropout: float = 0.1\n",
    "\n",
    "'''\n",
    "sat 27 2025\n",
    "plans: okay, first layer essentially learns a structured up-projection to semantic atoms.\n",
    "think of this GPTSemanticBlock as an intelligent tokenizer.\n",
    "just need to adjust the way it behaves a little bit so it emits something higher than C,\n",
    "and have it digest all bottleneck products in parallel to act as a distilling codebook instead of a phase shift.\n",
    "C*K+1 -> ? -? Q width. LMhead is learned at that point.\n",
    "\n",
    "\n",
    "problem- even with this phaseblock innovation,\n",
    "model picks up coarse patterns juliet -> nurse -> romeo but by mid-loss DESTROYS them\n",
    "is unable to preserve meaningful structure. this- which we observed- remains true.\n",
    "i think its because model has no capacity at all to segment and then segregate but use this context later.\n",
    "it needs to be able to semantically bifurcate and then independently preserve and make decisions for filtering,\n",
    "maybe using convolution, re/block/dynamic state content that is used and updated at every step.\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Constrain(nn.Module):\n",
    "    \"\"\"\n",
    "    Works with (B, C) or (B, T, C); all inputs must be the same shape.\n",
    "    forward(y, x, hidden_state) -> updated x\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.lin_y = nn.Linear(dim, dim, bias=True)  # input_w, input_b\n",
    "        self.lin_x = nn.Linear(dim, dim, bias=True)  # hidden_w, hidden_b\n",
    "        self.lin_c = nn.Linear(dim, dim, bias=True)  # candidate_w, candidate_b\n",
    "        self.sigmoid = ZLSGate()\n",
    "\n",
    "    def forward(self, y: torch.Tensor, x: torch.Tensor, hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "        u_t = self.sigmoid(self.lin_y(y) + self.lin_x(x))\n",
    "        c_tilde_t = torch.tanh(self.lin_c(u_t * x))\n",
    "        x = (1.0 - u_t) * hidden_state + u_t * c_tilde_t\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.n_embd = config.n_embd\n",
    "        self.features = SemanticClusterFeaturesCausal(num_scales=config.n_scales, tau=1e-6)\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = FlatRollEmbed(config),\n",
    "            h = nn.ModuleList([GPTSemanticBlock(config,self.features) for _ in range(config.n_layer)]),\n",
    "            c = nn.ModuleList([Constrain(config.n_embd) for _ in range(config.n_layer)]),\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(self.config.n_embd, self.config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.transformer.wte.embed.weight #tie\n",
    "        \n",
    "    # ---------- forward ----------\n",
    "    def forward(self, idx, targets=None, eprint=False):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        x = self.transformer.wte(idx) \n",
    "        hidden = torch.zeros_like(x)\n",
    "        for i, block in enumerate(self.transformer.h):\n",
    "                x_t= block(x)\n",
    "                x = x + self.transformer.c[i](x_t , x, hidden) \n",
    "                hidden = hidden + x_t.clone()\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_greedy(model: nn.Module, idx: torch.LongTensor, max_new_tokens: int, block_size: int):\n",
    "        \"\"\"\n",
    "        model: your GPT with:\n",
    "           - transformer.wte (embedding)\n",
    "           - transformer.h : list[GPTSemanticBlock]\n",
    "           - lm_head\n",
    "        idx: (B, T0) prompt token ids\n",
    "        \"\"\"\n",
    "        device = next(model.parameters()).device\n",
    "        B = idx.size(0)\n",
    "        # per-block feature caches\n",
    "        feat_states = [CausalPyramidState(model.config.n_scales, model.config.n_embd, device, batch_size=B)\n",
    "                       for _ in model.transformer.h]\n",
    "    \n",
    "        # 1) prime caches with the prompt (causal, one step at a time)\n",
    "        x_all = model.transformer.wte(idx)  # (B,T0,C); fixed embeddings in your code\n",
    "        for t in range(idx.size(1)):\n",
    "            x_t = x_all[:, t, :]\n",
    "            hidden = torch.zeros_like(x_t)\n",
    "            i = 0\n",
    "            for blk, st in zip(model.transformer.h, feat_states):\n",
    "                x_r = blk.step(x_t, st)      # per-block step\n",
    "                x_t = x_t + self.transformer.c[i](x_r , x_t, hidden) \n",
    "                hidden = hidden + x_r.clone()\n",
    "                i = i + 1\n",
    "                \n",
    "        # 2) roll out new tokens\n",
    "        out = [idx]\n",
    "        cur = idx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # last token embedding\n",
    "            last_idx = cur[:, -1]                      # (B,)\n",
    "            x_t = model.transformer.wte(last_idx)      # (B,C)\n",
    "            hidden = torch.zeros_like(x_t)\n",
    "            i = 0\n",
    "            for blk, st in zip(model.transformer.h, feat_states):\n",
    "                x_r = blk.step(x_t, st)                # (B,C)\n",
    "                x_t = x_t + self.transformer.c[i](x_r , x_t, hidden) \n",
    "                hidden = hidden + x_r.clone()\n",
    "                i = i + 1\n",
    "\n",
    "            logits = model.lm_head(x_t)                # (B,V)\n",
    "            next_idx = torch.argmax(logits, dim=-1, keepdim=True)  # greedy; swap to sampling if you like\n",
    "            out.append(next_idx)\n",
    "            cur = torch.cat([cur, next_idx], dim=1)\n",
    "            # keep only last block_size tokens in cur (typical AR convenience)\n",
    "            if cur.size(1) > block_size:\n",
    "                cur = cur[:, -block_size:]\n",
    "        return torch.cat(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFGVJvlN_yfW",
    "outputId": "f11f6493-0761-458d-9be0-4ebc604e53e1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Downloading aochildes.txt...\n",
      "ðŸ“¥ Downloading cbt.txt...\n",
      "ðŸ“¥ Downloading children_stories.txt...\n",
      "ðŸ“¥ Downloading gutenberg.txt...\n",
      "ðŸ“¥ Downloading qed.txt...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m out_path = os.path.join(target_dir, fname)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“¥ Downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"ðŸ“¥ Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"âŒ Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"âœ… Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0fFuL2a_sAF",
    "outputId": "79c1170f-c818-4568-cf7b-939e02bc33e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Char tokenizer finalized.\n",
      "ðŸ§¾ Train tokens: 1016242 | Val tokens: 99152\n",
      "ðŸ”¤ Vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,#\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"âœ… Char tokenizer finalized.\")\n",
    "print(f\"ðŸ§¾ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"ðŸ”¤ Vocab size: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g42l_Fa8_v9z",
    "outputId": "7bbfe691-e965-43cd-e53e-a4438ff8d7ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 2048\n",
    "batch_size = 8\n",
    "\n",
    "# === Load tokenizer metadata ===\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "\n",
    "# === Load mmap edata (char-level tokens, uint16) ===\n",
    "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
    "\n",
    "# === Efficient GPU Batch Sampler ===\n",
    "class GPUBatchDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size, batch_size, device, jitter=63, p_aligned=0.5, pad_len=0):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.pad_len = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len  # X length\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = self.total // self.sample_len\n",
    "        self.jitter = int(jitter)          # small random offset added to aligned start\n",
    "        self.p_aligned = float(p_aligned)  # mix aligned and jittered\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = np.empty((self.batch_size, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # choose a base aligned block\n",
    "            base_block = np.random.randint(0, self.n_blocks)\n",
    "            start = base_block * self.sample_len\n",
    "\n",
    "            # with probability, add a small jitter (keeps cache-friendly contiguous reads)\n",
    "            if np.random.rand() > self.p_aligned:\n",
    "                j = np.random.randint(0, self.jitter + 1)\n",
    "                start = min(start + j, self.total)  # stay in range\n",
    "\n",
    "            X[i] = self.data[start : start + self.sample_len]\n",
    "            # targets correspond to the final block_size visible steps\n",
    "            Y[i] = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + self.block_size]\n",
    "\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
    "            torch.from_numpy(Y).to(self.device, non_blocking=True)\n",
    "        )\n",
    "\n",
    "\n",
    "config = GPTConfig(\n",
    "    vocab_size=len(stoi),\n",
    "    n_layer=2,\n",
    "    n_embd=vocab_size,\n",
    "    block_size=block_size,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "train_dataset = GPUBatchDataset(train_ids, block_size, batch_size, device, pad_len=0)\n",
    "# === DataLoader ===\n",
    "train_loader  = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "model = GPT(config)\n",
    "model = torch.compile(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(file_path)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.079197645187378\n",
      "1.8665930032730103\n",
      "1.7288258075714111\n",
      "1.737014651298523\n",
      "1.6280440092086792\n",
      "1.634819746017456\n",
      "1.5726745128631592\n",
      "1.610018253326416\n",
      "1.5827571153640747\n",
      "1.5700984001159668\n",
      "1.5128073692321777\n",
      "1.5476913452148438\n",
      "1.5392965078353882\n",
      "1.5891072750091553\n",
      "1.5176829099655151\n",
      "1.5685250759124756\n",
      "1.5460467338562012\n",
      "1.4723989963531494\n",
      "1.5299241542816162\n",
      "1.4655452966690063\n",
      "1.4778978824615479\n",
      "1.449035406112671\n",
      "1.4312952756881714\n",
      "1.507636308670044\n",
      "1.4653981924057007\n",
      "1.469456434249878\n",
      "1.4074029922485352\n",
      "1.5303070545196533\n",
      "1.4929152727127075\n",
      "1.3911460638046265\n",
      "1.4651567935943604\n",
      "1.4126240015029907\n",
      "1.4155683517456055\n",
      "1.4090063571929932\n",
      "1.4266811609268188\n",
      "1.396214485168457\n",
      "1.4390685558319092\n",
      "1.4203524589538574\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m optimizer.zero_grad()\n\u001b[32m     10\u001b[39m it = it + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss = loss\n\u001b[32m     13\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:375\u001b[39m, in \u001b[36mOptimizedModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.nn.modules.module._has_any_global_hook():\n\u001b[32m    366\u001b[39m     warnings.warn(\n\u001b[32m    367\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `torch.compile(module)` when there are global hooks on \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodules (e.g., from `register_module_forward_hook`); this will\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    374\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:736\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    733\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 645\u001b[39m, in \u001b[36mGPT.forward\u001b[39m\u001b[34m(self, idx, targets, eprint)\u001b[39m\n\u001b[32m    642\u001b[39m     \u001b[38;5;28mself\u001b[39m.lm_head.weight = \u001b[38;5;28mself\u001b[39m.transformer.wte.embed.weight \u001b[38;5;66;03m#tie\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[38;5;66;03m# ---------- forward ----------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx, targets=\u001b[38;5;28;01mNone\u001b[39;00m, eprint=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    646\u001b[39m     device = idx.device\n\u001b[32m    647\u001b[39m     b, t = idx.size()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1241\u001b[39m, in \u001b[36maot_module_simplified.<locals>.forward\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1239\u001b[39m full_args.extend(params_flat)\n\u001b[32m   1240\u001b[39m full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:370\u001b[39m, in \u001b[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m    366\u001b[39m         torch.autograd._force_original_view_tracking(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    367\u001b[39m         torch.enable_grad(),\n\u001b[32m    368\u001b[39m     ):\n\u001b[32m    369\u001b[39m         record_runtime_wrapper_prologue_exit(cm)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         all_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001b[39;00m\n\u001b[32m    375\u001b[39m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[32m    378\u001b[39m     \u001b[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001b[39;00m\n\u001b[32m    379\u001b[39m     grad_enabled = torch.is_grad_enabled()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001b[39m, in \u001b[36mmake_boxed_func.<locals>.g\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mg\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:576\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    575\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2074\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001b[39m\u001b[34m(ctx, *deduped_flat_tensor_args)\u001b[39m\n\u001b[32m   2065\u001b[39m     args = (*args, *fwd_rng_states)\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2072\u001b[39m \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[32m   2073\u001b[39m \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2074\u001b[39m fw_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_fw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2080\u001b[39m num_outputs = CompiledFunction.metadata.num_outputs\n\u001b[32m   2081\u001b[39m num_outputs_aliased = CompiledFunction.metadata.num_outputs_aliased\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:556\u001b[39m, in \u001b[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    549\u001b[39m     out = \u001b[38;5;28mself\u001b[39m._functionalized_rng_runtime_epilogue(\n\u001b[32m    550\u001b[39m         runtime_metadata,\n\u001b[32m    551\u001b[39m         out,\n\u001b[32m    552\u001b[39m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    553\u001b[39m         runtime_metadata.num_forward_returns,\n\u001b[32m    554\u001b[39m     )\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:750\u001b[39m, in \u001b[36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    747\u001b[39m     args = [*([\u001b[38;5;28;01mNone\u001b[39;00m] * num_tokens), *args]\n\u001b[32m    748\u001b[39m     old_args.clear()\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m outs = \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/ij/cij3m7xl524izhs6afivk3jvn7dtrpl2or4obsnleooia2qxhysf.py:3897\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m   3895\u001b[39m buf93 = reinterpret_tensor(buf94, (\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m66\u001b[39m), (\u001b[32m405504\u001b[39m, \u001b[32m198\u001b[39m, \u001b[32m66\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m66\u001b[39m)  \u001b[38;5;66;03m# alias\u001b[39;00m\n\u001b[32m   3896\u001b[39m buf96 = reinterpret_tensor(buf97, (\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m198\u001b[39m), (\u001b[32m540672\u001b[39m, \u001b[32m264\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m66\u001b[39m)  \u001b[38;5;66;03m# alias\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3897\u001b[39m \u001b[43mcpp_fused__to_copy_add_arange_argmin_bitwise_or_cat_clamp_min_div_eq_gather_gt_linalg_vector_norm_lt_mul_neg_sub_sum_where_6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf91\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf74\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf77\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf15\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf80\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf94\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf75\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf76\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf78\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf79\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf82\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf87\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf89\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf81\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf83\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf84\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf85\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf90\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf93\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf96\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3898\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m buf15\n\u001b[32m   3899\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m buf75\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    it = 0\n",
    "    for xb, yb in train_loader:\n",
    "          xb, yb = xb[0], yb[0]  # unwrap batch dimension\n",
    "          optimizer.zero_grad()\n",
    "          it = it + 1\n",
    "          logits, loss = model(xb, yb)\n",
    "          loss = loss\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "          optimizer.step()\n",
    "          total_loss += loss.item()\n",
    "          losses.append(loss.item())\n",
    "          if it%100==0: print(loss.item()) \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5869231224060059\n",
      "1.5327808856964111\n",
      "1.5128729343414307\n",
      "1.5264091491699219\n",
      "1.5360016822814941\n",
      "1.5110175609588623\n",
      "1.4978361129760742\n",
      "1.4761021137237549\n",
      "1.4639396667480469\n",
      "1.4139299392700195\n",
      "1.4155921936035156\n",
      "1.4781222343444824\n",
      "1.434190273284912\n",
      "1.3913090229034424\n",
      "1.3877649307250977\n",
      "1.4638370275497437\n",
      "1.4361367225646973\n",
      "1.3830711841583252\n",
      "1.4057782888412476\n",
      "1.4554815292358398\n",
      "1.4043946266174316\n",
      "1.4072433710098267\n",
      "1.3971418142318726\n",
      "1.4467896223068237\n",
      "1.3549902439117432\n",
      "1.369402289390564\n",
      "1.327133297920227\n",
      "1.3223388195037842\n",
      "1.376537561416626\n",
      "1.3253755569458008\n",
      "1.4278180599212646\n",
      "1.3397783041000366\n",
      "1.3271937370300293\n",
      "1.3419313430786133\n",
      "1.3649258613586426\n",
      "1.3753366470336914\n",
      "1.3410391807556152\n",
      "1.3182134628295898\n",
      "1.3611254692077637\n",
      "1.3536961078643799\n",
      "1.3214341402053833\n",
      "1.376505732536316\n",
      "1.3474347591400146\n",
      "1.3484331369400024\n",
      "1.3247432708740234\n",
      "1.347876787185669\n",
      "1.3651046752929688\n",
      "1.3076739311218262\n",
      "1.3503267765045166\n",
      "1.2889927625656128\n",
      "1.2910735607147217\n",
      "1.3577179908752441\n",
      "1.317771315574646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m logits, loss = model(xb, yb)\n\u001b[32m     12\u001b[39m loss = loss\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     15\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:311\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    310\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2259\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   2257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   2258\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2245\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimpl_fn\u001b[39m(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     out = \u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _backward_epilogue_functional(\n\u001b[32m   2247\u001b[39m         CompiledFunction.metadata,\n\u001b[32m   2248\u001b[39m         CompiledFunction.maybe_subclass_metadata,\n\u001b[32m   2249\u001b[39m         out,\n\u001b[32m   2250\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2376\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2360\u001b[39m     torch._functorch.config.donated_buffer\n\u001b[32m   2361\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[32m   2362\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata.bw_donated_idxs != []\n\u001b[32m   2363\u001b[39m ):\n\u001b[32m   2364\u001b[39m     torch._check(\n\u001b[32m   2365\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2366\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m   2373\u001b[39m         ),\n\u001b[32m   2374\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2376\u001b[39m out = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/j2/cj25cuu65dlvwog3k2iky3gc6dmvhopiorkzucdlhvwxudbvusal.py:1813\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m   1811\u001b[39m buf40 = empty_strided_cpu((\u001b[32m3\u001b[39m, \u001b[32m16384\u001b[39m, \u001b[32m132\u001b[39m), (\u001b[32m2162688\u001b[39m, \u001b[32m132\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n\u001b[32m   1812\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [], Original ATen: [aten.bmm]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1813\u001b[39m \u001b[43mextern_kernels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf38\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m264\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m264\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m792\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_17\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuf40\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m primals_17\n\u001b[32m   1815\u001b[39m buf41 = reinterpret_tensor(buf6, (\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m66\u001b[39m), (\u001b[32m135168\u001b[39m, \u001b[32m66\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m0\u001b[39m); \u001b[38;5;28;01mdel\u001b[39;00m buf6  \u001b[38;5;66;03m# reuse\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: ROMEO: ROMEO: Juliet! My Juliet! come forth to me! grace:\n",
      "Then, may: nevery grown nestalls! I tears, was, it--but Aumerll the truth rarician withness' so he's I fouple,\n",
      "For I revellainKe\n",
      "So multitch, digreence pervenant friend, now, bell your fie, Gremio, I beseech rotb33monfictsed his land.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Are you speak.\n",
      "\n",
      "HASTINGS:\n",
      "May thrown'd,\n",
      "Is fair alracks.\n",
      "Bess, as friends up thy thearte god, legs deall live from my vow'd Romeo, there's natness\n",
      "And hy:\n",
      "My grity\n",
      "I knext lies,\n",
      "And would revenge and Friar, and a foolish Mowbray you bear these would not wink your his pride!\n",
      "But, SiJ extriumph.\n",
      "Thou art and the secret shamse! eight! he well; I'll seems my woman's for a honest Jest your witneas of Barnardial my thered slagEDWW-hent know shall not in the rest.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "WithiJJ Hads:\n",
      "Stay, and would\n",
      "If struct to if men, trouble these to thee's he a keep a king my on the his know! is comemead me, I am\n",
      "anze one in a stand, thou inseth hest, away, sometime, O, it, take another good&zen princales.\n",
      "Yea, as found me you\n",
      "To say in brawly;\n",
      "And bidst swords desting now behead:\n",
      "Who know; his neckHa3' LKKABEEN:\n",
      "Speak news?\n",
      "\n",
      "ABXETER:\n",
      "And at say we ballow not.\n",
      "\n",
      "Nurse:\n",
      "From elppForing Richmorrow: feastly you?\n",
      "Let's portune saunt.\n",
      "\n",
      "Provost:\n",
      "With keep his way, that should you fortune! which goodness.\n",
      "\n",
      "Eist mercy years.\n",
      "\n",
      "QUEEN:\n",
      "Be? whom whch the poison, JyLeavezes.\n",
      "\n",
      "MENENIUS:\n",
      "Why shall you,\n",
      "As he this:\n",
      "Stand:\n",
      "if is told do canst Declebores knew lely had, may gracious slain for thronfold full we terred, queen that!\n",
      "\n",
      "CThis litted word, this pulets of my to two hell\n",
      "wouldst naAlies, of my and there little, and Warwick now my srowHE-'tis a fearful shome best is well:\n",
      "To-morrow, how what you dearly to blin'd upon than your voices, he's terous; likew: part. What, and, not his captvet. Do make me more.\n",
      "\n",
      "MENENIUS:\n",
      "You so the duke;\n",
      "And theresFFLOBefore that thought\n",
      "And like Qurged\n",
      "EAMdLULIF:\n",
      "Yes there\n",
      "For that!\n",
      "\n",
      "BUCKINGHAM:\n",
      "To what Oxfult the Becamnn'd mortal is more's death;\n",
      "If Fifth our heart\n",
      "As proclaw, that fear'd thy warsW\n",
      "To thence; it whom cratch.\n",
      "Supposte; that is stronger gries are your hand carry that.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "For, good my sweet contrain.\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Desty; and, harm, as must,\n",
      "I'll lady his gloriate:\n",
      "And of all me?\n",
      "Why, whom the tauntf all them.\n",
      "\n",
      "COMINIUS:\n",
      "That's Jown:E has stutle\n",
      "gefound makes no more mine awer our with his rotve as a shall be to do\n",
      "hall hath Geding from of my honourth\n",
      "Your killy.\n",
      "\n",
      "ROMEO:\n",
      "Let for thineJUSBILIg!\n",
      "Richard thou Rich that name is send\n",
      "By his the elmitty spremiony:\n",
      "Came, Aubly fair had bear;\n",
      "They sore fearful was and spicians,\n",
      "Amback out not now Borse as trouble his faird; and where are Boy:\n",
      "Travel's friendly wedigni<unk>eB&Qanacvious chall\n",
      "onged your and in heat them of gove gressJO, hi' there?\n",
      "O rit Command, he is autther good time remented buuy ctigolanus and bring; thou suppering heart a deceived not poor stwisted\n",
      "Excupties to myself,\n",
      "AwayO:\n",
      "They Nore all thy hand friends event mirth stiris the heart:\n",
      "I'll not us I say he speeded pButter asters, when I'll speak\n",
      "Dispair seen jone discordies, it with would make milding gively together,\n",
      "That co' 'Belon move.\n",
      "\n",
      "Second Servingman: if I will shel: beseech a ballaw'd.\n",
      "O Plack\n",
      "atimes rumaids, and now!\n",
      "\n",
      "CORIOLANUS:\n",
      "To you approant frown that yourzeness little art he dault, foot are\n",
      "upon the king, and yet you made you well have haspesset die: I have been extire it night stoncius!\n",
      "Do, let us!\n",
      "\n",
      "CORIOLANUS:\n",
      "Bembering night\n",
      "Bapt:\n",
      "In whost,\n",
      "And my guards on, common either natients I may buMrice to Be thou art what king, and prisone cough my follow ho breathe pen'd cheeber.\n",
      "\n",
      "CORIOLANUS:\n",
      "Now, wish'd min, my shall earth to maying:\n",
      "The may late on homan\n",
      "A setDeirr leapNerown aglace follon, is enemion, Gremio, the morpaison.\n",
      "Setter Mine opprobaAngerrous of remeent our is from heart.\n",
      "\n",
      "LEONTES:\n",
      "Ay, not set'stistreme; but those arse twitoZYou have but by could,\n",
      "And by his living\n",
      "And bam I learn to thee with your may,\n",
      "That we are too trumis.\n",
      "\n",
      "ROMEO:\n",
      "TNoth within black me,\n",
      "That patract; the test call this sake.\n",
      "\n",
      "GLOUCESTER:\n",
      "Say, I shall then he amore and well in a flifve be life pitread\n",
      "Of call dihorse! What the te\n",
      "3.676461935043335\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_sequence_char_rolling(\n",
    "    model, stoi, itos, prompt,\n",
    "    max_new_tokens=100,\n",
    "    block_size=1024,\n",
    "    temperature=1.0,\n",
    "    space_fallback=' ',\n",
    "    strict_window=False,          # if True, periodically re-prime caches on the last block\n",
    "    reprime_every=None            # if strict_window, how often to re-prime (int). Default: block_size\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolling-block generator that:\n",
    "      - keeps the ENTIRE generated text (no trimming of output),\n",
    "      - maintains a rolling block window internally,\n",
    "      - optionally re-primes feature caches on the last `block_size` tokens to strictly\n",
    "        mimic block-window semantics seen during training.\n",
    "\n",
    "    If strict_window=False (default): fastest path; caches stream forever.\n",
    "    If strict_window=True: we periodically reinitialize the per-layer states using the\n",
    "      most recent `block_size` tokens. This ensures exact 'sliding window' behavior.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    B = 1\n",
    "\n",
    "    # ---- encode prompt (fallback to space if empty) ----\n",
    "    space_id = stoi.get(space_fallback, 0)\n",
    "    prompt_ids = encode_chars(prompt, stoi)\n",
    "    if len(prompt_ids) == 0:\n",
    "        prompt_ids = [space_id]\n",
    "\n",
    "    # ---- left-pad ONCE to match your training forward's left-pad-to-block ----\n",
    "    pad_len = max(0, block_size - len(prompt_ids))\n",
    "    pad_ids = [space_id] * pad_len\n",
    "    priming_ids = pad_ids + prompt_ids  # padding only used for priming; not returned\n",
    "\n",
    "    # ---- per-block feature caches (one state per block) ----\n",
    "    feat_states = [\n",
    "        CausalPyramidState(\n",
    "            num_scales=model.config.n_scales,\n",
    "            C=model.config.n_embd,\n",
    "            device=device,\n",
    "            batch_size=B,\n",
    "            tau=1e-6\n",
    "        ) for _ in model.transformer.h\n",
    "    ]\n",
    "\n",
    "    # helper: (re-)prime caches with a sequence of token ids (left-pad to block if shorter)\n",
    "    def _reprime_with_ids(tok_ids):\n",
    "        # optionally left-pad the window up to block_size (only needed if strict semantics desired)\n",
    "        if len(tok_ids) < block_size:\n",
    "            tok_ids = [space_id] * (block_size - len(tok_ids)) + tok_ids\n",
    "        ids_t = torch.tensor([tok_ids], dtype=torch.long, device=device)  # (1, T)\n",
    "        x_last = None\n",
    "        # fresh states\n",
    "        new_states = [\n",
    "            CausalPyramidState(\n",
    "                num_scales=model.config.n_scales,\n",
    "                C=model.config.n_embd,\n",
    "                device=device,\n",
    "                batch_size=B,\n",
    "                tau=1e-6\n",
    "            ) for _ in model.transformer.h\n",
    "        ]\n",
    "        for t in range(ids_t.size(1)):\n",
    "            x_last = model.transformer.wte(ids_t[:, t])  # (1,C)\n",
    "            hidden = torch.zeros_like(x_last)\n",
    "            i = 0\n",
    "            for blk, st in zip(model.transformer.h, new_states):\n",
    "                x_r = blk.step(x_last, st)\n",
    "                x_last = x_last + model.transformer.c[i](x_r , x_last, hidden) \n",
    "                hidden = hidden + x_r.clone()\n",
    "                i = i + 1\n",
    "        return new_states, x_last\n",
    "\n",
    "    # ---- initial priming with left-padded prompt ----\n",
    "    ids = torch.tensor([priming_ids], dtype=torch.long, device=device)\n",
    "    x_t = None\n",
    "    for t in range(ids.size(1)):\n",
    "        x_t = model.transformer.wte(ids[:, t])  # (1,C)\n",
    "        hidden = torch.zeros_like(x_t)\n",
    "        i = 0\n",
    "        for blk, st in zip(model.transformer.h, feat_states):\n",
    "            x_r = blk.step(x_t, st)\n",
    "            x_t = x_t + model.transformer.c[i](x_r , x_t, hidden) \n",
    "            hidden = hidden + x_r.clone()\n",
    "            i = i + 1\n",
    "    # ---- FULL output accumulator (never trimmed) ----\n",
    "    out_full = list(prompt_ids)  # store ints\n",
    "\n",
    "    # ---- rolling window buffer of last block_size tokens (prompt + generated) ----\n",
    "    window = deque(prompt_ids, maxlen=block_size)\n",
    "\n",
    "    # strict-window settings\n",
    "    if reprime_every is None:\n",
    "        reprime_every = block_size\n",
    "    steps_since_reprime = 0\n",
    "\n",
    "    # ---- incremental rollout ----\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.lm_head(x_t)  # (1,V)\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / float(temperature)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "        next_id = int(next_token.item())\n",
    "\n",
    "        # record full output\n",
    "        out_full.append(next_id)\n",
    "\n",
    "        # advance rolling window\n",
    "        window.append(next_id)\n",
    "                \n",
    "        # step one token\n",
    "        x_t = model.transformer.wte(next_token.squeeze(-1))  # (1,C)\n",
    "        hidden = torch.zeros_like(x_t)\n",
    "        i = 0\n",
    "        for blk, st in zip(model.transformer.h, feat_states):\n",
    "            x_r = blk.step(x_t, st)\n",
    "            x_t = x_t + model.transformer.c[i](x_r , x_t, hidden) \n",
    "            hidden = hidden + x_r.clone()\n",
    "            i = i + 1\n",
    "\n",
    "        # optionally re-prime to strict sliding-window semantics\n",
    "        if strict_window:\n",
    "            steps_since_reprime += 1\n",
    "            if steps_since_reprime >= reprime_every and len(window) == block_size:\n",
    "                feat_states, x_t = _reprime_with_ids(list(window))\n",
    "                steps_since_reprime = 0\n",
    "\n",
    "    # decode full continuation (prompt + all generated)\n",
    "    return decode_chars(out_full, itos)\n",
    "    \n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "import time\n",
    "then = time.time()\n",
    "prompt = \"ROMEO: ROMEO: ROMEO: Juliet! My Juliet! come forth to me! \"\n",
    "generated = decode_sequence_char_rolling(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=4096,\n",
    "    block_size=2048,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(generated)\n",
    "print(time.time()-then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.arange(128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "UzqWUbNRlmiD"
   },
   "outputs": [],
   "source": [
    "file_path = 'simple_model_tiny.pth'\n",
    "\n",
    "# 3. Save the model's state_dict\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
