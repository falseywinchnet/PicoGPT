{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copyright joshuah.rainstar@gmail.com\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "class ZLSGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # smooth gate\n",
    "        x = 4.0 * x\n",
    "        sp = F.softplus(x)\n",
    "        sa = torch.sigmoid(0.5 * x)\n",
    "        ba = sa * (1.0 - sa)\n",
    "        z = sp - 2.772588722239781 * ba  # 4 * ln(2)\n",
    "        return 1.0 - torch.exp(-(z + 1e-8))\n",
    "\n",
    "def _norm(v, eps: float = 1e-12):\n",
    "    return torch.linalg.vector_norm(v, dim=-1, keepdim=True).clamp_min(eps)\n",
    "\n",
    "\n",
    "def _unit(v, eps: float = 1e-12):\n",
    "    return v / _norm(v, eps)\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def phase_transport_between(\n",
    "    curr: torch.Tensor,\n",
    "    prev: torch.Tensor,\n",
    "    tau: float = 1e-6,          # semantic threshold (unchanged)\n",
    "    eps: float = 1e-12          # numeric epsilon (NEW: decoupled from tau)\n",
    ") -> torch.Tensor:\n",
    "    assert curr.shape == prev.shape and curr.dim() == 3\n",
    "    B, T, C = curr.shape\n",
    "\n",
    "    # Units (reuse norms) — clamp with eps (NOT tau)\n",
    "    nu = torch.linalg.vector_norm(curr, dim=-1, keepdim=True).clamp_min(eps)   # (B,T,1)\n",
    "    nv = torch.linalg.vector_norm(prev, dim=-1, keepdim=True).clamp_min(eps)   # (B,T,1)\n",
    "    u = curr / nu\n",
    "    v = prev / nv\n",
    "\n",
    "    w = curr - prev\n",
    "    c = (u * v).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "\n",
    "    # Masks (semantic thresholds use tau)\n",
    "    near_pos = (c >  1.0 - tau)                                                # (B,T,1)\n",
    "    near_neg = (c < -1.0 + tau)                                                # (B,T,1)\n",
    "    small_u  = (nu < tau)                                                      # (B,T,1)\n",
    "    small_v  = (nv < tau)                                                      # (B,T,1)\n",
    "    trivial  = near_pos | small_u | small_v                                    # (B,T,1)\n",
    "\n",
    "    # General branch\n",
    "    denom = (1.0 + c).clamp_min(eps)                                           # (B,T,1)\n",
    "    a = (v * w).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "    b = (u * w).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "    Kw  = u * a - v * b                                                        # (B,T,C)\n",
    "    K2w = u * (a * c - b) + v * (b * c - a)                                    # (B,T,C)\n",
    "    y_gen = w - Kw + (K2w / denom)                                             # (B,T,C)\n",
    "\n",
    "    # Antipodal candidate\n",
    "    if C == 1:\n",
    "        y_neg = -w\n",
    "    else:\n",
    "        # Keep this normalization stable with eps as well\n",
    "        idx = torch.argmin(v.abs().reshape(-1, C), dim=1, keepdim=True)        # (B*T,1)\n",
    "        s = v.reshape(-1, C).gather(1, idx)                                    # (B*T,1)\n",
    "        p = -s * v.reshape(-1, C)\n",
    "        onehot = F.one_hot(idx.squeeze(-1), num_classes=C).to(s.dtype)\n",
    "        p = p + onehot\n",
    "        n = torch.linalg.vector_norm(p, dim=1, keepdim=True).clamp_min(eps)\n",
    "        p = (p / n).view(B, T, C)\n",
    "        proj_v = (v * w).sum(dim=-1, keepdim=True) * v                         # (B,T,C)\n",
    "        proj_p = (p * w).sum(dim=-1, keepdim=True) * p                         # (B,T,C)\n",
    "        y_neg = w - 2.0 * proj_v - 2.0 * proj_p\n",
    "\n",
    "    # Fuse selections\n",
    "    y = torch.where(trivial, w, y_gen)\n",
    "    y = torch.where(near_neg, y_neg, y)\n",
    "    return y\n",
    "\n",
    "# ===========================================================\n",
    "# Multi-scale features (vectorized pyramid)\n",
    "# ===========================================================\n",
    "class CausalCentroidPyramid(nn.Module):\n",
    "    \"\"\"\n",
    "    Child-driven centroid pyramid.\n",
    "    Level s (s=0..K-1) covers window 2^(s+1):\n",
    "      - child stream z_0 := x\n",
    "      - y_s(t) = PT(z_s(t), z_s(t - 2^s))             # distance between bracketing child markers\n",
    "      - z_{s+1}(t) = z_s(t) - 0.5 * y_s(t)            # centerpoint of the window (right-anchored)\n",
    "    Unsupported prefix t < 2^s is zeroed.\n",
    "    Returns feats: (B,T,K,C) with feats[:,:,s,:] = y_s.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, tau: float = 1e-6):\n",
    "        super().__init__()\n",
    "        assert num_scales >= 1\n",
    "        self.K = num_scales\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor, mask_early: bool = True) -> torch.Tensor:\n",
    "        assert x.dim() == 3\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        feats = []\n",
    "        z = x.clone()  # child stream for current level\n",
    "\n",
    "        for s in range(self.K):\n",
    "            d = 1 << s  # child lag (1,2,4,...)\n",
    "\n",
    "            left = torch.zeros_like(z)\n",
    "            if T > d:\n",
    "                left[:, d:, :] = z[:, :-d, :].contiguous()\n",
    "\n",
    "            y = phase_transport_between(z, left, tau=self.tau)  # (B,T,C)\n",
    "\n",
    "            if mask_early:\n",
    "                y[:, :d, :].zero_()\n",
    "\n",
    "            feats.append(y)\n",
    "\n",
    "            # next-level child markers (centerpoints from the two halves)\n",
    "            z = z - 0.5 * y\n",
    "            if mask_early:\n",
    "                z[:, :d, :].zero_()\n",
    "\n",
    "        return torch.stack(feats, dim=2)  # (B,T,K,C)\n",
    "\n",
    "\n",
    "        # ----- STREAMING STATE FOR INFERENCE -----\n",
    "class CausalPyramidState:\n",
    "    \"\"\"\n",
    "    Keeps ring buffers of child marker streams z_s for s=0..K-1 with lengths 2^s.\n",
    "    At step t:\n",
    "      - z_0(t) = x_t\n",
    "      - for s: y_s(t) = PT(z_s(t), z_s(t - 2^s)); z_{s+1}(t) = z_s(t) - 0.5*y_s(t)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, C: int, device, batch_size: int = 1, tau: float = 1e-6):\n",
    "        self.K = num_scales\n",
    "        self.C = C\n",
    "        self.B = batch_size\n",
    "        self.device = device\n",
    "        self.tau = float(tau)\n",
    "        self.t = 0\n",
    "\n",
    "        # ring buffers for child streams z_s\n",
    "        self.buffers = [torch.zeros(batch_size, (1 << s), C, device=device) for s in range(self.K)]\n",
    "        self.ptrs = [0 for _ in range(self.K)]\n",
    "\n",
    "    def _read(self, level: int, r: int):\n",
    "        if self.t < r:\n",
    "            return torch.zeros(self.B, self.C, device=self.device)\n",
    "        L = self.buffers[level].size(1)\n",
    "        idx = (self.ptrs[level] - r) % L\n",
    "        return self.buffers[level][:, idx, :]\n",
    "\n",
    "    def _push(self, level: int, value: torch.Tensor):\n",
    "        L = self.buffers[level].size(1)\n",
    "        self.buffers[level][:, self.ptrs[level], :] = value\n",
    "        self.ptrs[level] = (self.ptrs[level] + 1) % L\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor) -> torch.Tensor:\n",
    "        B, C = x_t.shape\n",
    "        assert B == self.B and C == self.C\n",
    "\n",
    "        feats = []\n",
    "        z_t = x_t.clone()  # z_0(t)\n",
    "\n",
    "        for s in range(self.K):\n",
    "            d = 1 << s\n",
    "            left = self._read(level=s, r=d)           # z_s(t-d)\n",
    "            y_t = phase_transport_between(z_t[:, None, :], left[:, None, :], tau=self.tau).squeeze(1)\n",
    "            if self.t < d:\n",
    "                y_t.zero_()\n",
    "            feats.append(y_t)\n",
    "\n",
    "            # push current child stream for this level\n",
    "            self._push(level=s, value=z_t)\n",
    "\n",
    "            # next level child marker z_{s+1}(t)\n",
    "            z_t = z_t - 0.5 * y_t\n",
    "            if self.t < d:\n",
    "                z_t.zero_()\n",
    "\n",
    "        self.t += 1\n",
    "        return torch.stack(feats, dim=1)  # (B,K,C)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SemanticClusterFeaturesCausal(nn.Module):\n",
    "    \"\"\"\n",
    "    Unified wrapper:\n",
    "      - forward(x): vectorized for training\n",
    "      - step(x_t, state): single-step for inference with cache\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, tau: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.pyramid = CausalCentroidPyramid(num_scales=num_scales, tau=tau)\n",
    "        self.K = num_scales\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pyramid(x)  # (B,T,K,C)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor, state: CausalPyramidState) -> torch.Tensor:\n",
    "        return state.step(x_t)  # (B,K,C)\n",
    "\n",
    "        \n",
    "        \n",
    "class Cell(nn.Module):\n",
    "    def __init__(self, dim_in: int, hidden: int,dim_out:int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, hidden, bias=True) \n",
    "        self.fc2 = nn.Linear(hidden, dim_out, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, x):\n",
    "      \n",
    "        return self.fc2(self.act(self.fc1(x))) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GPTSemanticBlock(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, features):\n",
    "        super().__init__()\n",
    "        C = config.n_embd\n",
    "        self.C = C\n",
    "        self.K = config.n_scales\n",
    "        self.L = self.K+1\n",
    "        self.features = features\n",
    "        width = self.L*self.C\n",
    "\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.ln = nn.LayerNorm(width)\n",
    "        self.mlp = Cell(width, width*2,width)\n",
    "        # NEW: highest-scale low-pass bank (e.g., M=3)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        out = self.drop(self.ln(self.mlp(x)))\n",
    "        return out\n",
    "\n",
    "    # single-step incremental\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor) -> torch.Tensor:\n",
    "        B, C = x_t.shape\n",
    "        return self.drop(self.ln(self.mlp(x_t)))\n",
    "\n",
    "\n",
    "\n",
    "def _is_prime(n: int) -> bool:\n",
    "    if n < 2: return False\n",
    "    if n % 2 == 0: return n == 2\n",
    "    r = int(n**0.5)\n",
    "    for f in range(3, r+1, 2):\n",
    "        if n % f == 0: return False\n",
    "    return True\n",
    "\n",
    "def _factorize(n: int):\n",
    "    f, cnt = [], {}\n",
    "    d = 2\n",
    "    while d * d <= n:\n",
    "        while n % d == 0:\n",
    "            cnt[d] = cnt.get(d, 0) + 1\n",
    "            n //= d\n",
    "        d += 1 if d == 2 else 2\n",
    "    if n > 1: cnt[n] = cnt.get(n, 0) + 1\n",
    "    return list(cnt.keys())\n",
    "\n",
    "def _primitive_root(p: int) -> int:\n",
    "    # p must be prime\n",
    "    phi = p - 1\n",
    "    factors = _factorize(phi)\n",
    "    for g in range(2, p):\n",
    "        ok = True\n",
    "        for q in factors:\n",
    "            if pow(g, phi // q, p) == 1:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            return g\n",
    "    raise RuntimeError(\"no primitive root found\")\n",
    "\n",
    "def _welch_costas_perm(V: int, device=None):\n",
    "    \"\"\"\n",
    "    Welch Costas permutation σ on {0..V-1}, where V = p-1 for prime p.\n",
    "    σ[i] = g^(i+1) mod p, mapped to 0..V-1 by subtracting 1.\n",
    "    \"\"\"\n",
    "    p = V + 1\n",
    "    if not _is_prime(p):\n",
    "        return None\n",
    "    g = _primitive_root(p)\n",
    "    sigma = torch.empty(V, dtype=torch.long, device=device)\n",
    "    for i in range(V):\n",
    "        sigma[i] = pow(g, i + 1, p) - 1\n",
    "    return sigma  # permutation of 0..V-1\n",
    "\n",
    "def _coprime_mul_perm(V: int, device=None):\n",
    "    \"\"\"\n",
    "    Fallback: σ[i] = (a*i + b) % V with gcd(a, V)=1 and a not ≡ ±1 mod V.\n",
    "    Not Costas, but non-monotone and well-distributed.\n",
    "    \"\"\"\n",
    "    # pick a\n",
    "    a = None\n",
    "    for cand in range(2, V):\n",
    "        if math.gcd(cand, V) == 1 and cand % V not in (1, V-1):\n",
    "            a = cand\n",
    "            break\n",
    "    if a is None:\n",
    "        a = 1  # degenerate small V\n",
    "    b = V // 3\n",
    "    i = torch.arange(V, device=device)\n",
    "    return ((a * i + b) % V).long()\n",
    "\n",
    "def _perm_inverse(sigma: torch.Tensor) -> torch.Tensor:\n",
    "    inv = torch.empty_like(sigma)\n",
    "    inv[sigma] = torch.arange(sigma.numel(), device=sigma.device)\n",
    "    return inv\n",
    "\n",
    "\n",
    "class FlatRollEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding matrix W in R^{V x D} built as:\n",
    "      W = roll_rows(x) + S\n",
    "    where x ∈ R^D has |FFT(x)|^2 flat (k=1..D-1) and DC=0, roll_rows(x)[r] = roll(x, r % D),\n",
    "    and S adds a single spike per row at column (r + M) % D, with M = argmax(x) and scale N = 1/(x[M]+eps).\n",
    "\n",
    "    Works for any vocab_size V and embedding dim D.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, scale: str = \"box\", seed: int = 0,\n",
    "                 freeze: bool = True, dtype=None, device=None):\n",
    "        super().__init__()\n",
    "        V = int(config.vocab_size)\n",
    "        D = int(config.n_embd)\n",
    "        dtype = dtype or torch.float32\n",
    "        eps = 1e-12\n",
    "\n",
    "        # base vector x ∈ R^D with flat spectrum, DC=0\n",
    "        x = self._make_base(D, scale=scale, seed=seed, dtype=dtype, device=device)  # [D]\n",
    "\n",
    "        # circulant-like rows: row r is x rolled by (r % D)\n",
    "        # (vectorized loop for clarity; can be optimized further if needed)\n",
    "        shifts = torch.arange(V, device=device)\n",
    "        rows = [torch.roll(x, shifts=int(s.item() % D), dims=0) for s in shifts]\n",
    "        W = torch.stack(rows, dim=0).to(dtype)\n",
    "\n",
    "        # align a single positive extremum via a \"tower\" S\n",
    "        M = int(torch.argmax(x))          # index of max in x\n",
    "        pm = x[M].item()\n",
    "        N = 1.0 / (pm + eps)              # safe reciprocal\n",
    "\n",
    "        # S[r, (r + M) % D] = N\n",
    "        r_idx = torch.arange(V, device=device)\n",
    "        c_idx = (r_idx + M) % D\n",
    "        S = torch.zeros((V, D), dtype=dtype, device=device)\n",
    "        S[r_idx, c_idx] = N\n",
    "\n",
    "        mixed = W + S\n",
    "        self.embed = nn.Embedding.from_pretrained(mixed, freeze=freeze)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_base(D: int, scale: str = \"box\", seed: int = 0,\n",
    "                   dtype=torch.float32, device=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Build x ∈ R^D where |FFT(x)| is flat for k=1..D-1 and DC=0.\n",
    "\n",
    "        scale:\n",
    "          - \"unit\": ||x||_2 = 1\n",
    "          - \"box\":  max|x_i| = 1\n",
    "        \"\"\"\n",
    "        # Build on CPU, then move to device at the end.\n",
    "        # Use complex64 for float/bfloat/half; complex128 otherwise.\n",
    "        if dtype in (torch.float16, torch.bfloat16, torch.float32):\n",
    "            complex_dtype = torch.complex64\n",
    "            work_float = torch.float32\n",
    "        else:\n",
    "            complex_dtype = torch.complex128\n",
    "            work_float = torch.float64\n",
    "\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        X = torch.zeros(D, dtype=complex_dtype)\n",
    "\n",
    "        # DC bin = 0\n",
    "        X[0] = torch.tensor(0, dtype=complex_dtype)\n",
    "\n",
    "        if D % 2 == 0:\n",
    "            # bins 1..D/2-1 are complex-conjugate pairs; Nyquist bin must be real\n",
    "            for k in range(1, D // 2):\n",
    "                phi = torch.rand((), generator=g, dtype=work_float) * (2 * math.pi)\n",
    "                val = torch.cos(phi).to(work_float) + 1j * torch.sin(phi).to(work_float)\n",
    "                val = val.to(complex_dtype)\n",
    "                X[k] = val\n",
    "                X[D - k] = torch.conj(val)\n",
    "            # Nyquist bin (real, ±1)\n",
    "            X[D // 2] = (1.0 if torch.rand((), generator=g) < 0.5 else -1.0)\n",
    "        else:\n",
    "            for k in range(1, (D - 1) // 2 + 1):\n",
    "                phi = torch.rand((), generator=g, dtype=work_float) * (2 * math.pi)\n",
    "                val = torch.cos(phi).to(work_float) + 1j * torch.sin(phi).to(work_float)\n",
    "                val = val.to(complex_dtype)\n",
    "                X[k] = val\n",
    "                X[D - k] = torch.conj(val)\n",
    "\n",
    "        x = torch.fft.ifft(X).real  # real length-D base vector (float32/64)\n",
    "        x = x.to(work_float)\n",
    "\n",
    "        if scale == \"unit\":\n",
    "            x = x / (x.norm() + 1e-12)\n",
    "        elif scale == \"box\":\n",
    "            x = x / (x.abs().max() + 1e-12)\n",
    "        else:\n",
    "            raise ValueError(\"scale must be 'unit' or 'box'\")\n",
    "\n",
    "        x = x.to(dtype)\n",
    "        if device is not None:\n",
    "            x = x.to(device)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        # (batch, seq_len, D)\n",
    "        return self.embed(input_ids)\n",
    "\n",
    "\n",
    "        \n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 66 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 6\n",
    "    n_head:int = 6\n",
    "    n_embd: int = 66 #tied to vocab\n",
    "    n_scales:int = 3#more than 3 has no real meaning\n",
    "  \n",
    "    dropout: float = 0.1\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.n_embd = config.n_embd\n",
    "        self.C = self.n_embd\n",
    "        self.K = config.n_scales\n",
    "        self.width = (self.K+1)*self.C\n",
    "        self.features = SemanticClusterFeaturesCausal(num_scales=config.n_scales, tau=1e-6)\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = FlatRollEmbed(config),\n",
    "            h = nn.ModuleList([GPTSemanticBlock(config,self.features) for _ in range(config.n_layer)]),\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear( self.width , self.config.vocab_size, bias=False)\n",
    "        \n",
    "    # ---------- forward ----------\n",
    "    def forward(self, idx, targets=None, eprint=False):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        x = self.transformer.wte(idx) \n",
    "\n",
    "        \n",
    "        feats = self.features(x)                  # (B,T,K,C), K = 1 + (K-1) deltas\n",
    "        # build low-pass bank on μ_top(x) only, append as extra K channels\n",
    "        feats = feats.reshape(b,t,((self.K)*self.C))\n",
    "        x = torch.cat([x,feats], dim=-1)   \n",
    "\n",
    "    \n",
    "        for i, block in enumerate(self.transformer.h):\n",
    "                x= x + block(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_greedy(model: nn.Module, idx: torch.LongTensor, max_new_tokens: int, block_size: int):\n",
    "        \"\"\"\n",
    "        model: your GPT with:\n",
    "           - transformer.wte (embedding)\n",
    "           - transformer.h : list[GPTSemanticBlock]\n",
    "           - lm_head\n",
    "        idx: (B, T0) prompt token ids\n",
    "        \"\"\"\n",
    "        device = next(model.parameters()).device\n",
    "        B = idx.size(0)\n",
    "        # per-block feature caches\n",
    "        feat_states = [CausalPyramidState(model.config.n_scales, model.config.n_embd, device, batch_size=B)\n",
    "                       for _ in model.transformer.h]\n",
    "  \n",
    "    \n",
    "        # 1) prime caches with the prompt (causal, one step at a time)\n",
    "        for t in range(idx.size(1)):\n",
    "            x_t = x_all[:, t, :]\n",
    "            feats_t = model.features.step(x_t, feat_states)     # (B, K, C)\n",
    "            fused = feats_t.reshape(B,((self.K)*self.C))\n",
    "            x_t = torch.cat([x_t,fused], dim=-1)     # (B,T,K+M,C)\n",
    "\n",
    "\n",
    "\n",
    "            for i, blk in enumerate(model.transformer.h):\n",
    "                x_t =x_t +  blk.step(x_t)      # per-block step\n",
    "\n",
    "        # 2) roll out new tokens\n",
    "        out = [idx]\n",
    "        cur = idx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # last token embedding\n",
    "            last_idx = cur[:, -1]                      # (B,)\n",
    "            x_t = model.transformer.wte(last_idx)      # (B,C)\n",
    "            feats_t = model.features.step(x_t, feat_states)     # (B, K, C)\n",
    "            fused = feats_t.reshape(B,((model.K)*model.C))\n",
    "            x_t = torch.cat([x_t,fused], dim=-1)     # (B,T,K+M,C)\n",
    "            \n",
    "            for i, blk in enumerate(model.transformer.h):\n",
    "                x_t = x_t  + blk.step(x_t)                # (B,C)\n",
    "  \n",
    "\n",
    "            logits = model.lm_head(x_t)                # (B,V)\n",
    "            next_idx = torch.argmax(logits, dim=-1, keepdim=True)  # greedy; swap to sampling if you like\n",
    "            out.append(next_idx)\n",
    "            cur = torch.cat([cur, next_idx], dim=1)\n",
    "            # keep only last block_size tokens in cur (typical AR convenience)\n",
    "            if cur.size(1) > block_size:\n",
    "                cur = cur[:, -block_size:]\n",
    "        return torch.cat(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFGVJvlN_yfW",
    "outputId": "f11f6493-0761-458d-9be0-4ebc604e53e1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading aochildes.txt...\n",
      "📥 Downloading cbt.txt...\n",
      "📥 Downloading children_stories.txt...\n",
      "📥 Downloading gutenberg.txt...\n",
      "📥 Downloading qed.txt...\n",
      "📥 Downloading simple_wikipedia.txt...\n",
      "📥 Downloading switchboard.txt...\n",
      "📥 Downloading wikipedia.txt...\n",
      "📥 Downloading shakespeare.txt...\n",
      "✅ Done. Files saved to ./babylm_10m_cleaned\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"📥 Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"❌ Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"✅ Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0fFuL2a_sAF",
    "outputId": "79c1170f-c818-4568-cf7b-939e02bc33e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Char tokenizer finalized.\n",
      "🧾 Train tokens: 1016242 | Val tokens: 99152\n",
      "🔤 Vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,#\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"✅ Char tokenizer finalized.\")\n",
    "print(f\"🧾 Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"🔤 Vocab size: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g42l_Fa8_v9z",
    "outputId": "7bbfe691-e965-43cd-e53e-a4438ff8d7ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 128\n",
    "batch_size = 8\n",
    "\n",
    "# === Load tokenizer metadata ===\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "\n",
    "# === Load mmap edata (char-level tokens, uint16) ===\n",
    "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
    "\n",
    "# === Efficient GPU Batch Sampler ===\n",
    "class GPUBatchDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size, batch_size, device, jitter=63, p_aligned=0.5, pad_len=0):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.pad_len = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len  # X length\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = self.total // self.sample_len\n",
    "        self.jitter = int(jitter)          # small random offset added to aligned start\n",
    "        self.p_aligned = float(p_aligned)  # mix aligned and jittered\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = np.empty((self.batch_size, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # choose a base aligned block\n",
    "            base_block = np.random.randint(0, self.n_blocks)\n",
    "            start = base_block * self.sample_len\n",
    "\n",
    "            # with probability, add a small jitter (keeps cache-friendly contiguous reads)\n",
    "            if np.random.rand() > self.p_aligned:\n",
    "                j = np.random.randint(0, self.jitter + 1)\n",
    "                start = min(start + j, self.total)  # stay in range\n",
    "\n",
    "            X[i] = self.data[start : start + self.sample_len]\n",
    "            # targets correspond to the final block_size visible steps\n",
    "            Y[i] = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + self.block_size]\n",
    "\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
    "            torch.from_numpy(Y).to(self.device, non_blocking=True)\n",
    "        )\n",
    "\n",
    "\n",
    "config = GPTConfig(\n",
    "    vocab_size=len(stoi),\n",
    "    n_layer=4,\n",
    "    n_embd=vocab_size,\n",
    "    block_size=block_size,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "train_dataset = GPUBatchDataset(train_ids, block_size, batch_size, device, pad_len=0)\n",
    "# === DataLoader ===\n",
    "train_loader  = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "model = GPT(config)\n",
    "model = torch.compile(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9042234420776367\n",
      "2.4511091709136963\n",
      "2.3599181175231934\n",
      "2.400890350341797\n",
      "2.346912145614624\n",
      "2.176090955734253\n",
      "2.1745569705963135\n",
      "2.138833522796631\n",
      "2.215945243835449\n",
      "2.105602979660034\n",
      "2.139035940170288\n",
      "2.106557846069336\n",
      "1.9816009998321533\n",
      "2.0175578594207764\n",
      "1.9958734512329102\n",
      "2.0062508583068848\n",
      "2.1174323558807373\n",
      "1.9690488576889038\n",
      "2.043478488922119\n",
      "2.013014793395996\n",
      "2.00122332572937\n",
      "1.9801832437515259\n",
      "1.7857928276062012\n",
      "1.8777629137039185\n",
      "2.028447389602661\n",
      "1.9042567014694214\n",
      "1.8829174041748047\n",
      "1.9186255931854248\n",
      "1.9097460508346558\n",
      "1.9640905857086182\n",
      "1.876897931098938\n",
      "1.8561121225357056\n",
      "1.8458044528961182\n",
      "1.8553303480148315\n",
      "1.900394320487976\n",
      "1.8680081367492676\n",
      "1.8514200448989868\n",
      "1.798840045928955\n",
      "1.8339545726776123\n",
      "1.7878179550170898\n",
      "1.8095662593841553\n",
      "1.830919861793518\n",
      "1.893642783164978\n",
      "1.7647511959075928\n",
      "1.8892362117767334\n",
      "1.7420527935028076\n",
      "1.8204798698425293\n",
      "1.8589063882827759\n",
      "1.8335845470428467\n",
      "1.792112112045288\n",
      "1.798677682876587\n",
      "1.7119901180267334\n",
      "1.7721309661865234\n",
      "1.741382122039795\n",
      "1.7741994857788086\n",
      "1.7468552589416504\n",
      "1.7965073585510254\n",
      "1.7940922975540161\n",
      "1.840985894203186\n",
      "1.8613680601119995\n",
      "1.9168565273284912\n",
      "1.6844696998596191\n",
      "1.7259116172790527\n",
      "1.7596524953842163\n",
      "1.7335011959075928\n",
      "1.7369394302368164\n",
      "1.6952707767486572\n",
      "1.774288296699524\n",
      "1.831559419631958\n",
      "1.7056694030761719\n",
      "1.7496024370193481\n",
      "1.5501763820648193\n",
      "1.7687568664550781\n",
      "1.7556090354919434\n",
      "1.7010712623596191\n",
      "1.7443467378616333\n",
      "1.6450620889663696\n",
      "1.6284680366516113\n",
      "1.6989848613739014\n",
      "1.7569797039031982\n",
      "1.559088945388794\n",
      "1.6475801467895508\n",
      "1.7786856889724731\n",
      "1.6155012845993042\n",
      "1.6590330600738525\n",
      "1.7172045707702637\n",
      "1.7052005529403687\n",
      "1.6078672409057617\n",
      "1.6590449810028076\n",
      "1.6408801078796387\n",
      "1.6777548789978027\n",
      "1.707423448562622\n",
      "1.6508409976959229\n",
      "1.6415201425552368\n",
      "1.627339243888855\n",
      "1.7117197513580322\n",
      "1.7042183876037598\n",
      "1.603400468826294\n",
      "1.6443358659744263\n",
      "1.726410150527954\n",
      "1.697479486465454\n",
      "1.6118323802947998\n",
      "1.638547658920288\n",
      "1.5782328844070435\n",
      "1.5509841442108154\n",
      "1.675354242324829\n",
      "1.5543038845062256\n",
      "1.6151955127716064\n",
      "1.508073329925537\n",
      "1.528083086013794\n",
      "1.6376090049743652\n",
      "1.6063833236694336\n",
      "1.6770949363708496\n",
      "1.6355865001678467\n",
      "1.629577398300171\n",
      "1.5406818389892578\n",
      "1.5453755855560303\n",
      "1.5594855546951294\n",
      "1.5379691123962402\n",
      "1.7491599321365356\n",
      "1.6474475860595703\n",
      "1.6960645914077759\n",
      "1.6565468311309814\n",
      "1.7267318964004517\n",
      "1.5303250551223755\n",
      "1.5628823041915894\n",
      "1.672132968902588\n",
      "1.6218936443328857\n",
      "1.5309317111968994\n",
      "1.6999945640563965\n",
      "1.6646981239318848\n",
      "1.5309298038482666\n",
      "1.543138027191162\n",
      "1.6885745525360107\n",
      "1.5208408832550049\n",
      "1.6399345397949219\n",
      "1.662381649017334\n",
      "1.6891707181930542\n",
      "1.5366917848587036\n",
      "1.578012228012085\n",
      "1.527404546737671\n",
      "1.6573060750961304\n",
      "1.6214842796325684\n",
      "1.5788981914520264\n",
      "1.5944721698760986\n",
      "1.61268949508667\n",
      "1.4387996196746826\n",
      "1.6770225763320923\n",
      "1.499563455581665\n",
      "1.6343133449554443\n",
      "1.5171113014221191\n",
      "1.6261115074157715\n",
      "1.6414964199066162\n",
      "1.4395477771759033\n",
      "1.4422032833099365\n",
      "1.5674347877502441\n",
      "1.6115179061889648\n",
      "1.535407304763794\n",
      "1.5652430057525635\n",
      "1.4283623695373535\n",
      "1.536293387413025\n",
      "1.6099259853363037\n",
      "1.5471028089523315\n",
      "1.5418410301208496\n",
      "1.6049528121948242\n",
      "1.6284657716751099\n",
      "1.7025177478790283\n",
      "1.6178843975067139\n",
      "1.5890334844589233\n",
      "1.615213394165039\n",
      "1.4808588027954102\n",
      "1.5119210481643677\n",
      "1.4390796422958374\n",
      "1.593032717704773\n",
      "1.6421911716461182\n",
      "1.5394561290740967\n",
      "1.6521960496902466\n",
      "1.6373003721237183\n",
      "1.5875624418258667\n",
      "1.5244715213775635\n",
      "1.6099740266799927\n",
      "1.5739970207214355\n",
      "1.7104802131652832\n",
      "1.5724461078643799\n",
      "1.668550968170166\n",
      "1.459853172302246\n",
      "1.6299797296524048\n",
      "1.4959173202514648\n",
      "1.719927191734314\n",
      "1.4760699272155762\n",
      "1.473598837852478\n",
      "1.6181035041809082\n",
      "1.5397565364837646\n",
      "1.6002185344696045\n",
      "1.586575984954834\n",
      "1.6024820804595947\n",
      "1.546036958694458\n",
      "1.5105875730514526\n",
      "1.5758249759674072\n",
      "1.5339674949645996\n",
      "1.562010407447815\n",
      "1.4604785442352295\n",
      "1.5955896377563477\n",
      "1.5589001178741455\n",
      "1.431057095527649\n",
      "1.6031992435455322\n",
      "1.5996371507644653\n",
      "1.5435599088668823\n",
      "1.5255773067474365\n",
      "1.5351269245147705\n",
      "1.5908582210540771\n",
      "1.4574699401855469\n",
      "1.4706472158432007\n",
      "1.564929485321045\n",
      "1.7497954368591309\n",
      "1.510006070137024\n",
      "1.4197826385498047\n",
      "1.6105759143829346\n",
      "1.564765214920044\n",
      "1.6320996284484863\n",
      "1.5984807014465332\n",
      "1.5809783935546875\n",
      "1.5896105766296387\n",
      "1.47145414352417\n",
      "1.4960020780563354\n",
      "1.587929606437683\n",
      "1.6395959854125977\n",
      "1.538568377494812\n",
      "1.3818292617797852\n",
      "1.5224063396453857\n",
      "1.588036060333252\n",
      "1.5923871994018555\n",
      "1.4603749513626099\n",
      "1.600623607635498\n",
      "1.5516564846038818\n",
      "1.5352391004562378\n",
      "1.4541139602661133\n",
      "1.5224006175994873\n",
      "1.5822651386260986\n",
      "1.4609646797180176\n",
      "1.5578477382659912\n",
      "1.530996561050415\n",
      "1.4418747425079346\n",
      "1.5640449523925781\n",
      "1.4616377353668213\n",
      "1.540879249572754\n",
      "1.5570167303085327\n",
      "1.4158694744110107\n",
      "1.515123724937439\n",
      "1.6145614385604858\n",
      "1.5218632221221924\n",
      "1.4608176946640015\n",
      "1.5262216329574585\n",
      "1.5717369318008423\n",
      "1.428411841392517\n",
      "1.633508324623108\n",
      "1.4225834608078003\n",
      "1.6103489398956299\n",
      "1.5167909860610962\n",
      "1.5268223285675049\n",
      "1.5361052751541138\n",
      "1.5672764778137207\n",
      "1.423816442489624\n",
      "1.551295280456543\n",
      "1.5563472509384155\n",
      "1.4770851135253906\n",
      "1.529325246810913\n",
      "1.5196737051010132\n",
      "1.541496992111206\n",
      "1.5230367183685303\n",
      "1.478750228881836\n",
      "1.5136981010437012\n",
      "1.5429795980453491\n",
      "1.6120047569274902\n",
      "1.5685393810272217\n",
      "1.538210391998291\n",
      "1.6026558876037598\n",
      "1.4418619871139526\n",
      "1.423350214958191\n",
      "1.5496516227722168\n",
      "1.6116693019866943\n",
      "1.5725409984588623\n",
      "1.5158030986785889\n",
      "1.528573751449585\n",
      "1.456020712852478\n",
      "1.4418840408325195\n",
      "1.5384225845336914\n",
      "1.5106353759765625\n",
      "1.5172425508499146\n",
      "1.458500862121582\n",
      "1.54243803024292\n",
      "1.4271166324615479\n",
      "1.4648371934890747\n",
      "1.4655258655548096\n",
      "1.513814091682434\n",
      "1.6013630628585815\n",
      "1.5865912437438965\n",
      "1.636345386505127\n",
      "1.5214886665344238\n",
      "1.4369595050811768\n",
      "1.4924155473709106\n",
      "1.5193250179290771\n",
      "1.464921474456787\n",
      "1.3613612651824951\n",
      "1.4779365062713623\n",
      "1.4692630767822266\n",
      "1.5573487281799316\n",
      "1.5092113018035889\n",
      "1.4765535593032837\n",
      "1.4300323724746704\n",
      "1.5630226135253906\n",
      "1.547346830368042\n",
      "1.5395444631576538\n",
      "1.5667961835861206\n",
      "1.5075289011001587\n",
      "1.56215238571167\n",
      "1.5322867631912231\n",
      "1.4885058403015137\n",
      "1.4678550958633423\n",
      "1.4651836156845093\n",
      "1.5990774631500244\n",
      "1.4270350933074951\n",
      "1.5610365867614746\n",
      "1.4808764457702637\n",
      "1.4646495580673218\n",
      "1.4971247911453247\n",
      "1.4872360229492188\n",
      "1.6218523979187012\n",
      "1.4990160465240479\n",
      "1.424872636795044\n",
      "1.3951094150543213\n",
      "1.540604591369629\n",
      "1.5432102680206299\n",
      "1.483048677444458\n",
      "1.4770581722259521\n",
      "1.3833569288253784\n",
      "1.4281589984893799\n",
      "1.3755193948745728\n",
      "1.4764820337295532\n",
      "1.4871904850006104\n",
      "1.4385029077529907\n",
      "1.503525972366333\n",
      "1.5601919889450073\n",
      "1.6093522310256958\n",
      "1.5751533508300781\n",
      "1.5841457843780518\n",
      "1.4141044616699219\n",
      "1.5719034671783447\n",
      "1.414217472076416\n",
      "1.5319522619247437\n",
      "1.386116623878479\n",
      "1.4632956981658936\n",
      "1.5076695680618286\n",
      "1.3511683940887451\n",
      "1.5498839616775513\n",
      "1.5069730281829834\n",
      "1.493821620941162\n",
      "1.4557642936706543\n",
      "1.374065637588501\n",
      "1.4391926527023315\n",
      "1.5398497581481934\n",
      "1.4080311059951782\n",
      "1.5020049810409546\n",
      "1.4981367588043213\n",
      "1.5114670991897583\n",
      "1.3989673852920532\n",
      "1.5294896364212036\n",
      "1.4142796993255615\n",
      "1.4940359592437744\n",
      "1.5288193225860596\n",
      "1.503319501876831\n",
      "1.5668182373046875\n",
      "1.432044506072998\n",
      "1.4298436641693115\n",
      "1.521230936050415\n",
      "1.5417375564575195\n",
      "1.6776323318481445\n",
      "1.5043103694915771\n",
      "1.5500192642211914\n",
      "1.408730149269104\n",
      "1.455180048942566\n",
      "1.5387308597564697\n",
      "1.5348894596099854\n",
      "1.5241283178329468\n",
      "1.4717237949371338\n",
      "1.4259889125823975\n",
      "1.5373163223266602\n",
      "1.6443629264831543\n",
      "1.393405556678772\n",
      "1.529312252998352\n",
      "1.381902813911438\n",
      "1.4526703357696533\n",
      "1.3720656633377075\n",
      "1.4272842407226562\n",
      "1.6377018690109253\n",
      "1.5135836601257324\n",
      "1.4882073402404785\n",
      "1.5304312705993652\n",
      "1.497167944908142\n",
      "1.5422074794769287\n",
      "1.4564132690429688\n",
      "1.4077835083007812\n",
      "1.5020092725753784\n",
      "1.4884077310562134\n",
      "1.6320061683654785\n",
      "1.4059686660766602\n",
      "1.506027340888977\n",
      "1.3762387037277222\n",
      "1.5007481575012207\n",
      "1.428478717803955\n",
      "1.5184335708618164\n",
      "1.3431605100631714\n",
      "1.4351234436035156\n",
      "1.4634219408035278\n",
      "1.5705276727676392\n",
      "1.5563290119171143\n",
      "1.4693803787231445\n",
      "1.5033032894134521\n",
      "1.4658197164535522\n",
      "1.560186743736267\n",
      "1.4480388164520264\n",
      "1.5245007276535034\n",
      "1.4810543060302734\n",
      "1.4652581214904785\n",
      "1.4838752746582031\n",
      "1.621726393699646\n",
      "1.483577013015747\n",
      "1.4374327659606934\n",
      "1.3995717763900757\n",
      "1.4539083242416382\n",
      "1.397418737411499\n",
      "1.4775241613388062\n",
      "1.5393955707550049\n",
      "1.4338326454162598\n",
      "1.4814488887786865\n",
      "1.476196527481079\n",
      "1.484979510307312\n",
      "1.5136849880218506\n",
      "1.5266780853271484\n",
      "1.4320262670516968\n",
      "1.4363930225372314\n",
      "1.5449771881103516\n",
      "1.4395347833633423\n",
      "1.3501570224761963\n",
      "1.4164705276489258\n",
      "1.4496885538101196\n",
      "1.5343079566955566\n",
      "1.5234730243682861\n",
      "1.3987585306167603\n",
      "1.4331655502319336\n",
      "1.517506718635559\n",
      "1.5350728034973145\n",
      "1.473663568496704\n",
      "1.5944039821624756\n",
      "1.4734567403793335\n",
      "1.4041767120361328\n",
      "1.4442203044891357\n",
      "1.4458988904953003\n",
      "1.4733959436416626\n",
      "1.5183255672454834\n",
      "1.572371244430542\n",
      "1.4456244707107544\n",
      "1.4404187202453613\n",
      "1.5414972305297852\n",
      "1.485973596572876\n",
      "1.4086614847183228\n",
      "1.5969210863113403\n",
      "1.522189736366272\n",
      "1.424376130104065\n",
      "1.4106183052062988\n",
      "1.4796767234802246\n",
      "1.526093602180481\n",
      "1.3822404146194458\n",
      "1.3322868347167969\n",
      "1.5138880014419556\n",
      "1.3745636940002441\n",
      "1.4753940105438232\n",
      "1.388957142829895\n",
      "1.5743353366851807\n",
      "1.4813796281814575\n",
      "1.4482333660125732\n",
      "1.4966402053833008\n",
      "1.506772756576538\n",
      "1.4444977045059204\n",
      "1.5539450645446777\n",
      "1.4310548305511475\n",
      "1.560840368270874\n",
      "1.420686960220337\n",
      "1.4741380214691162\n",
      "1.4444968700408936\n",
      "1.4607816934585571\n",
      "1.5135736465454102\n",
      "1.4631308317184448\n",
      "1.4934470653533936\n",
      "1.4859778881072998\n",
      "1.3937077522277832\n",
      "1.5148751735687256\n",
      "1.4217400550842285\n",
      "1.4243923425674438\n",
      "1.488420009613037\n",
      "1.5276609659194946\n",
      "1.3947620391845703\n",
      "1.5021443367004395\n",
      "1.458540916442871\n",
      "1.4150739908218384\n",
      "1.465837001800537\n",
      "1.4527944326400757\n",
      "1.4328292608261108\n",
      "1.4321813583374023\n",
      "1.4782276153564453\n",
      "1.3428912162780762\n",
      "1.3852648735046387\n",
      "1.5159649848937988\n",
      "1.4146407842636108\n",
      "1.4377814531326294\n",
      "1.4732110500335693\n",
      "1.436319351196289\n",
      "1.5127443075180054\n",
      "1.3946170806884766\n",
      "1.4077093601226807\n",
      "1.4824838638305664\n",
      "1.5269577503204346\n",
      "1.451068639755249\n",
      "1.4758855104446411\n",
      "1.410501480102539\n",
      "1.5559861660003662\n",
      "1.4720556735992432\n",
      "1.4913854598999023\n",
      "1.3043544292449951\n",
      "1.4605560302734375\n",
      "1.5112292766571045\n",
      "1.3993728160858154\n",
      "1.5101183652877808\n",
      "1.4127366542816162\n",
      "1.511562466621399\n",
      "1.4811069965362549\n",
      "1.3658503293991089\n",
      "1.467036485671997\n",
      "1.4900429248809814\n",
      "1.3411312103271484\n",
      "1.5688226222991943\n",
      "1.430609941482544\n",
      "1.3584797382354736\n",
      "1.431369662284851\n",
      "1.5669811964035034\n",
      "1.4444310665130615\n",
      "1.519174337387085\n",
      "1.4288339614868164\n",
      "1.3513455390930176\n",
      "1.3457125425338745\n",
      "1.417251467704773\n",
      "1.583559274673462\n",
      "1.4630600214004517\n",
      "1.3655892610549927\n",
      "1.5572770833969116\n",
      "1.493822693824768\n",
      "1.385422945022583\n",
      "1.4627528190612793\n",
      "1.447277307510376\n",
      "1.4237825870513916\n",
      "1.5551323890686035\n",
      "1.3094913959503174\n",
      "1.462264060974121\n",
      "1.422088384628296\n",
      "1.4068397283554077\n",
      "1.4928107261657715\n",
      "1.4957109689712524\n",
      "1.3569927215576172\n",
      "1.4945502281188965\n",
      "1.5068416595458984\n",
      "1.4789268970489502\n",
      "1.5295671224594116\n",
      "1.4947702884674072\n",
      "1.3646821975708008\n",
      "1.3741810321807861\n",
      "1.4819307327270508\n",
      "1.4676536321640015\n",
      "1.4603471755981445\n",
      "1.5652644634246826\n",
      "1.4522966146469116\n",
      "1.3783822059631348\n",
      "1.330214500427246\n",
      "1.325498342514038\n",
      "1.443495273590088\n",
      "1.4845207929611206\n",
      "1.4585533142089844\n",
      "1.5104303359985352\n",
      "1.4759451150894165\n",
      "1.5030659437179565\n",
      "1.3856879472732544\n",
      "1.4610042572021484\n",
      "1.4294871091842651\n",
      "1.5203852653503418\n",
      "1.4864990711212158\n",
      "1.4939440488815308\n",
      "1.3412867784500122\n",
      "1.3410003185272217\n",
      "1.5188568830490112\n",
      "1.4335694313049316\n",
      "1.4023900032043457\n",
      "1.401233196258545\n",
      "1.41691255569458\n",
      "1.383972406387329\n",
      "1.3782691955566406\n",
      "1.4201300144195557\n",
      "1.365491271018982\n",
      "1.3857446908950806\n",
      "1.4750685691833496\n",
      "1.333369255065918\n",
      "1.419819712638855\n",
      "1.507824182510376\n",
      "1.4446090459823608\n",
      "1.469936728477478\n",
      "1.4601714611053467\n",
      "1.5355838537216187\n",
      "1.43906569480896\n",
      "1.484778881072998\n",
      "1.559563398361206\n",
      "1.4937483072280884\n",
      "1.3929789066314697\n",
      "1.4405932426452637\n",
      "1.3732961416244507\n",
      "1.267791986465454\n",
      "1.439401626586914\n",
      "1.308362364768982\n",
      "1.4088053703308105\n",
      "1.314771056175232\n",
      "1.4179736375808716\n",
      "1.300767183303833\n",
      "1.4092738628387451\n",
      "1.3033771514892578\n",
      "1.4105570316314697\n",
      "1.382631778717041\n",
      "1.335727572441101\n",
      "1.3652515411376953\n",
      "1.495732069015503\n",
      "1.4607908725738525\n",
      "1.3565120697021484\n",
      "1.4749579429626465\n",
      "1.4187126159667969\n",
      "1.5753846168518066\n",
      "1.3608531951904297\n",
      "1.3753039836883545\n",
      "1.4022600650787354\n",
      "1.3676211833953857\n",
      "1.3627593517303467\n",
      "1.4821162223815918\n",
      "1.3948825597763062\n",
      "1.4462296962738037\n",
      "1.5067538022994995\n",
      "1.3787868022918701\n",
      "1.3787777423858643\n",
      "1.4419093132019043\n",
      "1.4272910356521606\n",
      "1.442046046257019\n",
      "1.2890424728393555\n",
      "1.411944031715393\n",
      "1.479872465133667\n",
      "1.3792312145233154\n",
      "1.4834704399108887\n",
      "1.352168321609497\n",
      "1.372537612915039\n",
      "1.3504667282104492\n",
      "1.3442782163619995\n",
      "1.3801429271697998\n",
      "1.4783687591552734\n",
      "1.3089492321014404\n",
      "1.3982188701629639\n",
      "1.3947181701660156\n",
      "1.3818920850753784\n",
      "1.4403802156448364\n",
      "1.3202300071716309\n",
      "1.4234896898269653\n",
      "1.4020967483520508\n",
      "1.5426640510559082\n",
      "1.4540927410125732\n",
      "1.5397101640701294\n",
      "1.4319024085998535\n",
      "1.3451334238052368\n",
      "1.6538900136947632\n",
      "1.3907358646392822\n",
      "1.4216824769973755\n",
      "1.353003978729248\n",
      "1.3850584030151367\n",
      "1.5146459341049194\n",
      "1.4403724670410156\n",
      "1.3477931022644043\n",
      "1.385673999786377\n",
      "1.5364456176757812\n",
      "1.4186298847198486\n",
      "1.3315284252166748\n",
      "1.4490504264831543\n",
      "1.4634721279144287\n",
      "1.4404516220092773\n",
      "1.4284344911575317\n",
      "1.3508517742156982\n",
      "1.4576144218444824\n",
      "1.3980993032455444\n",
      "1.427893877029419\n",
      "1.4012932777404785\n",
      "1.5122230052947998\n",
      "1.4126720428466797\n",
      "1.4805066585540771\n",
      "1.385054588317871\n",
      "1.38567054271698\n",
      "1.5342133045196533\n",
      "1.4460880756378174\n",
      "1.406808614730835\n",
      "1.3918251991271973\n",
      "1.371511697769165\n",
      "1.4869334697723389\n",
      "1.403075933456421\n",
      "1.4340332746505737\n",
      "1.4285355806350708\n",
      "1.5641844272613525\n",
      "1.4199960231781006\n",
      "1.3767671585083008\n",
      "1.3860745429992676\n",
      "1.3840086460113525\n",
      "1.5547242164611816\n",
      "1.4093092679977417\n",
      "1.3260231018066406\n",
      "1.3916561603546143\n",
      "1.4839348793029785\n",
      "1.4694900512695312\n",
      "1.4744727611541748\n",
      "1.3677492141723633\n",
      "1.394044280052185\n",
      "1.4778269529342651\n",
      "1.4763147830963135\n",
      "1.42264723777771\n",
      "1.3848479986190796\n",
      "1.4321539402008057\n",
      "1.3017243146896362\n",
      "1.5530208349227905\n",
      "1.3473612070083618\n",
      "1.4492108821868896\n",
      "1.402501106262207\n",
      "1.4000111818313599\n",
      "1.3969159126281738\n",
      "1.4936964511871338\n",
      "1.564692735671997\n",
      "1.416344404220581\n",
      "1.51554274559021\n",
      "1.4776866436004639\n",
      "1.3730310201644897\n",
      "1.3898189067840576\n",
      "1.3130115270614624\n",
      "1.435726284980774\n",
      "1.4699277877807617\n",
      "1.4469006061553955\n",
      "1.5001790523529053\n",
      "1.4083213806152344\n",
      "1.286430835723877\n",
      "1.4046361446380615\n",
      "1.3645908832550049\n",
      "1.481050968170166\n",
      "1.3684576749801636\n",
      "1.4597041606903076\n",
      "1.4880239963531494\n",
      "1.376940131187439\n",
      "1.3015880584716797\n",
      "1.5281468629837036\n",
      "1.4076546430587769\n",
      "1.3958643674850464\n",
      "1.3348331451416016\n",
      "1.3532733917236328\n",
      "1.4193528890609741\n",
      "1.3075470924377441\n",
      "1.3493120670318604\n",
      "1.4074044227600098\n",
      "1.2517712116241455\n",
      "1.4330105781555176\n",
      "1.4424529075622559\n",
      "1.3884429931640625\n",
      "1.4110617637634277\n",
      "1.4444043636322021\n",
      "1.3398207426071167\n",
      "1.5056904554367065\n",
      "1.4544482231140137\n",
      "1.4173877239227295\n",
      "1.4190397262573242\n",
      "1.4725779294967651\n",
      "1.2826087474822998\n",
      "1.3404297828674316\n",
      "1.468010425567627\n",
      "1.4579955339431763\n",
      "1.3976411819458008\n",
      "1.4226605892181396\n",
      "1.3433189392089844\n",
      "1.4174554347991943\n",
      "1.400496244430542\n",
      "1.4956918954849243\n",
      "1.4127156734466553\n",
      "1.4280813932418823\n",
      "1.3706707954406738\n",
      "1.3567743301391602\n",
      "1.431739091873169\n",
      "1.3684464693069458\n",
      "1.471719741821289\n",
      "1.3931299448013306\n",
      "1.4847815036773682\n",
      "1.3670539855957031\n",
      "1.520140528678894\n",
      "1.4482446908950806\n",
      "1.4486511945724487\n",
      "1.451082706451416\n",
      "1.4600509405136108\n",
      "1.3796559572219849\n",
      "1.3853744268417358\n",
      "1.343658685684204\n",
      "1.4788697957992554\n",
      "1.346813440322876\n",
      "1.3834033012390137\n",
      "1.4277474880218506\n",
      "1.3244541883468628\n",
      "1.3113842010498047\n",
      "1.3439650535583496\n",
      "1.4820117950439453\n",
      "1.4510397911071777\n",
      "1.2538142204284668\n",
      "1.3742434978485107\n",
      "1.3617584705352783\n",
      "1.473552942276001\n",
      "1.468984842300415\n",
      "1.289064645767212\n",
      "1.4677931070327759\n",
      "1.4789808988571167\n",
      "1.3526339530944824\n",
      "1.4194289445877075\n",
      "1.398477554321289\n",
      "1.3916186094284058\n",
      "1.2953343391418457\n",
      "1.349257469177246\n",
      "1.4148025512695312\n",
      "1.3503122329711914\n",
      "1.4999749660491943\n",
      "1.4364523887634277\n",
      "1.3552539348602295\n",
      "1.4661500453948975\n",
      "1.477881669998169\n",
      "1.296026587486267\n",
      "1.457592248916626\n",
      "1.4571467638015747\n",
      "1.4952938556671143\n",
      "1.3439242839813232\n",
      "1.3207519054412842\n",
      "1.4053535461425781\n",
      "1.3773233890533447\n",
      "1.3735642433166504\n",
      "1.350380539894104\n",
      "1.4107093811035156\n",
      "1.3393102884292603\n",
      "1.4513322114944458\n",
      "1.434809684753418\n",
      "1.4636660814285278\n",
      "1.4216792583465576\n",
      "1.3704684972763062\n",
      "1.4396274089813232\n",
      "1.440513014793396\n",
      "1.4094159603118896\n",
      "1.284820318222046\n",
      "1.440921425819397\n",
      "1.291300892829895\n",
      "1.3231076002120972\n",
      "1.2869973182678223\n",
      "1.2711368799209595\n",
      "1.4123444557189941\n",
      "1.389971137046814\n",
      "1.3162919282913208\n",
      "1.4324474334716797\n",
      "1.3659197092056274\n",
      "1.395557165145874\n",
      "1.3961381912231445\n",
      "1.3473789691925049\n",
      "1.479209303855896\n",
      "1.409764051437378\n",
      "1.369465947151184\n",
      "1.3761321306228638\n",
      "1.5346547365188599\n",
      "1.4278751611709595\n",
      "1.4757413864135742\n",
      "1.2547435760498047\n",
      "1.4627325534820557\n",
      "1.333701729774475\n",
      "1.378661036491394\n",
      "1.378899335861206\n",
      "1.4280178546905518\n",
      "1.3499507904052734\n",
      "1.2779585123062134\n",
      "1.4317457675933838\n",
      "1.314900517463684\n",
      "1.2885380983352661\n",
      "1.3974974155426025\n",
      "1.331392526626587\n",
      "1.2588427066802979\n",
      "1.4334917068481445\n",
      "1.3504114151000977\n",
      "1.381134271621704\n",
      "1.4525094032287598\n",
      "1.4174200296401978\n",
      "1.246720552444458\n",
      "1.4034316539764404\n",
      "1.3327116966247559\n",
      "1.368239402770996\n",
      "1.45603346824646\n",
      "1.472947597503662\n",
      "1.403069019317627\n",
      "1.304213523864746\n",
      "1.3847997188568115\n",
      "1.281646966934204\n",
      "1.3221943378448486\n",
      "1.4828388690948486\n",
      "1.4611289501190186\n",
      "1.412438154220581\n",
      "1.4391467571258545\n",
      "1.4582022428512573\n",
      "1.3652524948120117\n",
      "1.350526213645935\n",
      "1.4013599157333374\n",
      "1.3663851022720337\n",
      "1.4989200830459595\n",
      "1.324359655380249\n",
      "1.3456974029541016\n",
      "1.4806954860687256\n",
      "1.3777878284454346\n",
      "1.2892687320709229\n",
      "1.3027572631835938\n",
      "1.3237075805664062\n",
      "1.420087218284607\n",
      "1.4307774305343628\n",
      "1.4328255653381348\n",
      "1.2872931957244873\n",
      "1.4185234308242798\n",
      "1.4097727537155151\n",
      "1.2483149766921997\n",
      "1.3886735439300537\n",
      "1.473771572113037\n",
      "1.2700326442718506\n",
      "1.4248809814453125\n",
      "1.3896691799163818\n",
      "1.2953587770462036\n",
      "1.3280808925628662\n",
      "1.2246508598327637\n",
      "1.3031737804412842\n",
      "1.3492457866668701\n",
      "1.3536663055419922\n",
      "1.3618885278701782\n",
      "1.3727107048034668\n",
      "1.4191187620162964\n",
      "1.332780361175537\n",
      "1.4449468851089478\n",
      "1.3765373229980469\n",
      "1.3312902450561523\n",
      "1.439875841140747\n",
      "1.4056744575500488\n",
      "1.3692198991775513\n",
      "1.3526114225387573\n",
      "1.4809578657150269\n",
      "1.3227932453155518\n",
      "1.3766647577285767\n",
      "1.393363118171692\n",
      "1.4711676836013794\n",
      "1.2653807401657104\n",
      "1.352781891822815\n",
      "1.351682186126709\n",
      "1.377077579498291\n",
      "1.3876503705978394\n",
      "1.4863426685333252\n",
      "1.4367235898971558\n",
      "1.3962136507034302\n",
      "1.3602896928787231\n",
      "1.4213237762451172\n",
      "1.3876924514770508\n",
      "1.381793737411499\n",
      "1.3366599082946777\n",
      "1.3010045289993286\n",
      "1.4696977138519287\n",
      "1.378626823425293\n",
      "1.3631163835525513\n",
      "1.4162826538085938\n",
      "1.2684297561645508\n",
      "1.303771734237671\n",
      "1.4462027549743652\n",
      "1.4025559425354004\n",
      "1.3731663227081299\n",
      "1.3749282360076904\n",
      "1.4230811595916748\n",
      "1.2846839427947998\n",
      "1.2871506214141846\n",
      "1.5271512269973755\n",
      "1.393319845199585\n",
      "1.4644207954406738\n",
      "1.3254064321517944\n",
      "1.2950836420059204\n",
      "1.3344663381576538\n",
      "1.2999969720840454\n",
      "1.4136488437652588\n",
      "1.378635287284851\n",
      "1.4038901329040527\n",
      "1.3119230270385742\n",
      "1.3969836235046387\n",
      "1.4408130645751953\n",
      "1.403206706047058\n",
      "1.2820065021514893\n",
      "1.3156211376190186\n",
      "1.369444489479065\n",
      "1.385314702987671\n",
      "1.3623969554901123\n",
      "1.3851945400238037\n",
      "1.3920482397079468\n",
      "1.3696695566177368\n",
      "1.2704952955245972\n",
      "1.310499668121338\n",
      "1.425399661064148\n",
      "1.3491517305374146\n",
      "1.3436305522918701\n",
      "1.251662015914917\n",
      "1.2876110076904297\n",
      "1.3879464864730835\n",
      "1.3923691511154175\n",
      "1.420466423034668\n",
      "1.3936189413070679\n",
      "1.3543180227279663\n",
      "1.3672795295715332\n",
      "1.362789273262024\n",
      "1.5093510150909424\n",
      "1.3440804481506348\n",
      "1.3739221096038818\n",
      "1.3229904174804688\n",
      "1.4214329719543457\n",
      "1.3983302116394043\n",
      "1.3158280849456787\n",
      "1.3135292530059814\n",
      "1.4308278560638428\n",
      "1.3714241981506348\n",
      "1.4158477783203125\n",
      "1.4157187938690186\n",
      "1.3054206371307373\n",
      "1.4386030435562134\n",
      "1.359673261642456\n",
      "1.3547157049179077\n",
      "1.3462347984313965\n",
      "1.3505734205245972\n",
      "1.3666805028915405\n",
      "1.3979942798614502\n",
      "1.438098430633545\n",
      "1.3516290187835693\n",
      "1.3436949253082275\n",
      "1.4059817790985107\n",
      "1.367883324623108\n",
      "1.4014010429382324\n",
      "1.2944674491882324\n",
      "1.44744873046875\n",
      "1.3416252136230469\n",
      "1.3602590560913086\n",
      "1.3604289293289185\n",
      "1.2455756664276123\n",
      "1.325420618057251\n",
      "1.2318775653839111\n",
      "1.271430253982544\n",
      "1.3857753276824951\n",
      "1.358422875404358\n",
      "1.449454426765442\n",
      "1.3408173322677612\n",
      "1.3307266235351562\n",
      "1.3550045490264893\n",
      "1.3862764835357666\n",
      "1.5422824621200562\n",
      "1.399470567703247\n",
      "1.3995294570922852\n",
      "1.3411939144134521\n",
      "1.3243708610534668\n",
      "1.3090862035751343\n",
      "1.3907076120376587\n",
      "1.3936775922775269\n",
      "1.290776252746582\n",
      "1.374880075454712\n",
      "1.377934455871582\n",
      "1.4302160739898682\n",
      "1.3833532333374023\n",
      "1.3269672393798828\n",
      "1.2346923351287842\n",
      "1.3424670696258545\n",
      "1.3468767404556274\n",
      "1.3808209896087646\n",
      "1.393415927886963\n",
      "1.3545770645141602\n",
      "1.3127614259719849\n",
      "1.3381845951080322\n",
      "1.3838738203048706\n",
      "1.3643888235092163\n",
      "1.3815900087356567\n",
      "1.3824609518051147\n",
      "1.2832362651824951\n",
      "1.3612010478973389\n",
      "1.280884027481079\n",
      "1.3880021572113037\n",
      "1.2447998523712158\n",
      "1.3233890533447266\n",
      "1.3992798328399658\n",
      "1.3358739614486694\n",
      "1.4065788984298706\n",
      "1.2335907220840454\n",
      "1.2845327854156494\n",
      "1.2779862880706787\n",
      "1.3120360374450684\n",
      "1.2568105459213257\n",
      "1.3406929969787598\n",
      "1.323044776916504\n",
      "1.3438844680786133\n",
      "1.3438966274261475\n",
      "1.4122700691223145\n",
      "1.4213566780090332\n",
      "1.3182406425476074\n",
      "1.3588011264801025\n",
      "1.337883710861206\n",
      "1.3295124769210815\n",
      "1.3974452018737793\n",
      "1.4548182487487793\n",
      "1.4154819250106812\n",
      "1.3746256828308105\n",
      "1.32208251953125\n",
      "1.3057080507278442\n",
      "1.400445818901062\n",
      "1.282741665840149\n",
      "1.2947331666946411\n",
      "1.4510886669158936\n",
      "1.328428030014038\n",
      "1.4124867916107178\n",
      "1.4500503540039062\n",
      "1.4199650287628174\n",
      "1.4527727365493774\n",
      "1.222282886505127\n",
      "1.3856614828109741\n",
      "1.3883578777313232\n",
      "1.2072824239730835\n",
      "1.4581878185272217\n",
      "1.377450704574585\n",
      "1.4785406589508057\n",
      "1.3522074222564697\n",
      "1.2312593460083008\n",
      "1.330985426902771\n",
      "1.3560926914215088\n",
      "1.4121036529541016\n",
      "1.364050030708313\n",
      "1.3595846891403198\n",
      "1.484187364578247\n",
      "1.3410735130310059\n",
      "1.4215917587280273\n",
      "1.315505862236023\n",
      "1.3637001514434814\n",
      "1.4075617790222168\n",
      "1.3113608360290527\n",
      "1.2913365364074707\n",
      "1.3819915056228638\n",
      "1.3684165477752686\n",
      "1.382704734802246\n",
      "1.3500661849975586\n",
      "1.329481840133667\n",
      "1.3377232551574707\n",
      "1.2725874185562134\n",
      "1.3347394466400146\n",
      "1.3380212783813477\n",
      "1.429254174232483\n",
      "1.2714143991470337\n",
      "1.2498427629470825\n",
      "1.212050437927246\n",
      "1.2762202024459839\n",
      "1.4114007949829102\n",
      "1.3622881174087524\n",
      "1.3449219465255737\n",
      "1.2359731197357178\n",
      "1.2397819757461548\n",
      "1.2625869512557983\n",
      "1.312927484512329\n",
      "1.500436544418335\n",
      "1.3494828939437866\n",
      "1.3372712135314941\n",
      "1.4106543064117432\n",
      "1.4119277000427246\n",
      "1.3980084657669067\n",
      "1.5085961818695068\n",
      "1.4213358163833618\n",
      "1.3481006622314453\n",
      "1.2619829177856445\n",
      "1.304160475730896\n",
      "1.4585591554641724\n",
      "1.3310871124267578\n",
      "1.3321199417114258\n",
      "1.4967849254608154\n",
      "1.2627670764923096\n",
      "1.360837697982788\n",
      "1.4074063301086426\n",
      "1.2801575660705566\n",
      "1.3437628746032715\n",
      "1.3732619285583496\n",
      "1.3803414106369019\n",
      "1.2826555967330933\n",
      "1.3499687910079956\n",
      "1.2875767946243286\n",
      "1.3098912239074707\n",
      "1.233938455581665\n",
      "1.4104284048080444\n",
      "1.3383264541625977\n",
      "1.2385722398757935\n",
      "1.2865724563598633\n",
      "1.2653512954711914\n",
      "1.359134316444397\n",
      "1.3442249298095703\n",
      "1.152397632598877\n",
      "1.3636963367462158\n",
      "1.37519109249115\n",
      "1.3496192693710327\n",
      "1.3723745346069336\n",
      "1.3729424476623535\n",
      "1.2680716514587402\n",
      "1.345916986465454\n",
      "1.435872197151184\n",
      "1.273985743522644\n",
      "1.361232042312622\n",
      "1.3718373775482178\n",
      "1.2989062070846558\n",
      "1.427328109741211\n",
      "1.3762030601501465\n",
      "1.3916940689086914\n",
      "1.347217321395874\n",
      "1.3356953859329224\n",
      "1.2994999885559082\n",
      "1.258131980895996\n",
      "1.3916759490966797\n",
      "1.3588459491729736\n",
      "1.3591948747634888\n",
      "1.318273663520813\n",
      "1.2746915817260742\n",
      "1.4120980501174927\n",
      "1.4152883291244507\n",
      "1.383509635925293\n",
      "1.3054202795028687\n",
      "1.4161841869354248\n",
      "1.412447452545166\n",
      "1.2686116695404053\n",
      "1.3731476068496704\n",
      "1.2630739212036133\n",
      "1.517383098602295\n",
      "1.3464691638946533\n",
      "1.2218666076660156\n",
      "1.3013620376586914\n",
      "1.3177534341812134\n",
      "1.3037641048431396\n",
      "1.3780795335769653\n",
      "1.3529167175292969\n",
      "1.350904107093811\n",
      "1.3892829418182373\n",
      "1.2914090156555176\n",
      "1.2096199989318848\n",
      "1.3272827863693237\n",
      "1.4185291528701782\n",
      "1.4182766675949097\n",
      "1.4017388820648193\n",
      "1.298022747039795\n",
      "1.4029781818389893\n",
      "1.4236600399017334\n",
      "1.505985975265503\n",
      "1.3874881267547607\n",
      "1.3114748001098633\n",
      "1.3164438009262085\n",
      "1.3032759428024292\n",
      "1.3591958284378052\n",
      "1.2476850748062134\n",
      "1.3678908348083496\n",
      "1.355137586593628\n",
      "1.3566480875015259\n",
      "1.4260785579681396\n",
      "1.344923496246338\n",
      "1.4458866119384766\n",
      "1.417029619216919\n",
      "1.336336612701416\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m logits, loss = model(xb, yb)\n\u001b[32m     12\u001b[39m loss = loss\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     15\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:311\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    310\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2259\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   2257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   2258\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2245\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimpl_fn\u001b[39m(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     out = \u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _backward_epilogue_functional(\n\u001b[32m   2247\u001b[39m         CompiledFunction.metadata,\n\u001b[32m   2248\u001b[39m         CompiledFunction.maybe_subclass_metadata,\n\u001b[32m   2249\u001b[39m         out,\n\u001b[32m   2250\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2376\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2360\u001b[39m     torch._functorch.config.donated_buffer\n\u001b[32m   2361\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[32m   2362\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata.bw_donated_idxs != []\n\u001b[32m   2363\u001b[39m ):\n\u001b[32m   2364\u001b[39m     torch._check(\n\u001b[32m   2365\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2366\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m   2373\u001b[39m         ),\n\u001b[32m   2374\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2376\u001b[39m out = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/tl/ctlx4h2s3ulruvqww2r6y6eqtmduwazll3auxs7j4b52elwbx6rd.py:1349\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m primals_23\n\u001b[32m   1348\u001b[39m buf12 = reinterpret_tensor(buf9, (\u001b[32m8\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m528\u001b[39m), (\u001b[32m67584\u001b[39m, \u001b[32m528\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m0\u001b[39m); \u001b[38;5;28;01mdel\u001b[39;00m buf9  \u001b[38;5;66;03m# reuse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m \u001b[43mcpp_fused_gelu_gelu_backward_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddmm_6\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m addmm_6\n\u001b[32m   1351\u001b[39m buf14 = empty_strided_cpu((\u001b[32m528\u001b[39m, \u001b[32m264\u001b[39m), (\u001b[32m264\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    it = 0\n",
    "    for xb, yb in train_loader:\n",
    "          xb, yb = xb[0], yb[0]  # unwrap batch dimension\n",
    "          optimizer.zero_grad()\n",
    "          it = it + 1\n",
    "          logits, loss = model(xb, yb)\n",
    "          loss = loss\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "          optimizer.step()\n",
    "          total_loss += loss.item()\n",
    "          losses.append(loss.item())\n",
    "          if it % 10 == 0: print(loss.item()) \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5869231224060059\n",
      "1.5327808856964111\n",
      "1.5128729343414307\n",
      "1.5264091491699219\n",
      "1.5360016822814941\n",
      "1.5110175609588623\n",
      "1.4978361129760742\n",
      "1.4761021137237549\n",
      "1.4639396667480469\n",
      "1.4139299392700195\n",
      "1.4155921936035156\n",
      "1.4781222343444824\n",
      "1.434190273284912\n",
      "1.3913090229034424\n",
      "1.3877649307250977\n",
      "1.4638370275497437\n",
      "1.4361367225646973\n",
      "1.3830711841583252\n",
      "1.4057782888412476\n",
      "1.4554815292358398\n",
      "1.4043946266174316\n",
      "1.4072433710098267\n",
      "1.3971418142318726\n",
      "1.4467896223068237\n",
      "1.3549902439117432\n",
      "1.369402289390564\n",
      "1.327133297920227\n",
      "1.3223388195037842\n",
      "1.376537561416626\n",
      "1.3253755569458008\n",
      "1.4278180599212646\n",
      "1.3397783041000366\n",
      "1.3271937370300293\n",
      "1.3419313430786133\n",
      "1.3649258613586426\n",
      "1.3753366470336914\n",
      "1.3410391807556152\n",
      "1.3182134628295898\n",
      "1.3611254692077637\n",
      "1.3536961078643799\n",
      "1.3214341402053833\n",
      "1.376505732536316\n",
      "1.3474347591400146\n",
      "1.3484331369400024\n",
      "1.3247432708740234\n",
      "1.347876787185669\n",
      "1.3651046752929688\n",
      "1.3076739311218262\n",
      "1.3503267765045166\n",
      "1.2889927625656128\n",
      "1.2910735607147217\n",
      "1.3577179908752441\n",
      "1.317771315574646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m logits, loss = model(xb, yb)\n\u001b[32m     12\u001b[39m loss = loss\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     15\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:311\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    310\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2259\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   2257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   2258\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2245\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimpl_fn\u001b[39m(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     out = \u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _backward_epilogue_functional(\n\u001b[32m   2247\u001b[39m         CompiledFunction.metadata,\n\u001b[32m   2248\u001b[39m         CompiledFunction.maybe_subclass_metadata,\n\u001b[32m   2249\u001b[39m         out,\n\u001b[32m   2250\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2376\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2360\u001b[39m     torch._functorch.config.donated_buffer\n\u001b[32m   2361\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[32m   2362\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata.bw_donated_idxs != []\n\u001b[32m   2363\u001b[39m ):\n\u001b[32m   2364\u001b[39m     torch._check(\n\u001b[32m   2365\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2366\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m   2373\u001b[39m         ),\n\u001b[32m   2374\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2376\u001b[39m out = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/j2/cj25cuu65dlvwog3k2iky3gc6dmvhopiorkzucdlhvwxudbvusal.py:1813\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m   1811\u001b[39m buf40 = empty_strided_cpu((\u001b[32m3\u001b[39m, \u001b[32m16384\u001b[39m, \u001b[32m132\u001b[39m), (\u001b[32m2162688\u001b[39m, \u001b[32m132\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n\u001b[32m   1812\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [], Original ATen: [aten.bmm]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1813\u001b[39m \u001b[43mextern_kernels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf38\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m264\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m264\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m792\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_17\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuf40\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m primals_17\n\u001b[32m   1815\u001b[39m buf41 = reinterpret_tensor(buf6, (\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m66\u001b[39m), (\u001b[32m135168\u001b[39m, \u001b[32m66\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m0\u001b[39m); \u001b[38;5;28;01mdel\u001b[39;00m buf6  \u001b[38;5;66;03m# reuse\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of the streets of \n",
      "1.4085562229156494\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_sequence_char_rolling(\n",
    "    model, stoi, itos, prompt,\n",
    "    max_new_tokens=100,\n",
    "    block_size=1024,\n",
    "    temperature=1.0,\n",
    "    space_fallback=' ',\n",
    "    strict_window=False,          # if True, periodically re-prime caches on the last block\n",
    "    reprime_every=None            # if strict_window, how often to re-prime (int). Default: block_size\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolling-block generator that:\n",
    "      - keeps the ENTIRE generated text (no trimming of output),\n",
    "      - maintains a rolling block window internally,\n",
    "      - optionally re-primes feature caches on the last `block_size` tokens to strictly\n",
    "        mimic block-window semantics seen during training.\n",
    "\n",
    "    If strict_window=False (default): fastest path; caches stream forever.\n",
    "    If strict_window=True: we periodically reinitialize the per-layer states using the\n",
    "      most recent `block_size` tokens. This ensures exact 'sliding window' behavior.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    B = 1\n",
    "\n",
    "    # ---- encode prompt (fallback to space if empty) ----\n",
    "    space_id = stoi.get(space_fallback, 0)\n",
    "    prompt_ids = encode_chars(prompt, stoi)\n",
    "    if len(prompt_ids) == 0:\n",
    "        prompt_ids = [space_id]\n",
    "\n",
    "    # ---- left-pad ONCE to match your training forward's left-pad-to-block ----\n",
    "    pad_len = max(0, block_size - len(prompt_ids))\n",
    "    pad_ids = [space_id] * pad_len\n",
    "    priming_ids = prompt_ids\n",
    "\n",
    "    # ---- per-block feature caches (one state per block) ----\n",
    "    feat_states = CausalPyramidState(\n",
    "            num_scales=model.config.n_scales,\n",
    "            C=model.config.n_embd,\n",
    "            device=device,\n",
    "            batch_size=B,\n",
    "            tau=1e-6\n",
    "        ) \n",
    "\n",
    "    # helper: (re-)prime caches with a sequence of token ids (left-pad to block if shorter)\n",
    "    def _reprime_with_ids(tok_ids):\n",
    "        # optionally left-pad the window up to block_size (only needed if strict semantics desired)\n",
    "    \n",
    "        ids_t = torch.tensor([tok_ids], dtype=torch.long, device=device)  # (1, T)\n",
    "        x_last = None\n",
    "        # fresh states\n",
    "        new_states = CausalPyramidState(\n",
    "                num_scales=model.config.n_scales,\n",
    "                C=model.config.n_embd,\n",
    "                device=device,\n",
    "                batch_size=B,\n",
    "                tau=1e-6\n",
    "            ) \n",
    "\n",
    "        for t in range(ids_t.size(1)):\n",
    "            x_last = model.transformer.wte(ids_t[:, t])  # (1,C)\n",
    "            feats_t = model.features.step(x_last, new_states)     # (B, K, C)\n",
    "            fused = feats_t.reshape(B,((model.K)*model.C))\n",
    "            x_last = torch.cat([x_last,fused], dim=-1)     # (B,T,K+M,C)\n",
    "            for i, blk in enumerate(model.transformer.h):\n",
    "                x_last = x_last + blk.step(x_last)\n",
    "   \n",
    "        return new_states, x_last\n",
    "\n",
    "    # ---- initial priming with left-padded prompt ----\n",
    "    ids = torch.tensor([priming_ids], dtype=torch.long, device=device)\n",
    "    x_t = None\n",
    "    for t in range(ids.size(1)):\n",
    "        x_t = model.transformer.wte(ids[:, t])  # (1,C)\n",
    "        feats_t = model.features.step(x_t, feat_states)     # (B, K, C)\n",
    "        fused = feats_t.reshape(B,((model.K)*model.C))\n",
    "        x_t = torch.cat([x_t,fused], dim=-1)     # (B,T,K+M,C)\n",
    "        for i,blk in enumerate(model.transformer.h):\n",
    "            x_t =x_t +  blk.step(x_t)\n",
    "\n",
    "    # ---- FULL output accumulator (never trimmed) ----\n",
    "    out_full = list(prompt_ids)  # store ints\n",
    "\n",
    "    # ---- rolling window buffer of last block_size tokens (prompt + generated) ----\n",
    "    window = deque(prompt_ids, maxlen=block_size)\n",
    "\n",
    "    # strict-window settings\n",
    "    if reprime_every is None:\n",
    "        reprime_every = block_size\n",
    "    steps_since_reprime = 0\n",
    "\n",
    "    # ---- incremental rollout ----\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.lm_head(x_t)  # (1,V)\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / float(temperature)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "        next_id = int(next_token.item())\n",
    "\n",
    "        # record full output\n",
    "        out_full.append(next_id)\n",
    "\n",
    "        # advance rolling window\n",
    "        window.append(next_id)\n",
    "                \n",
    "        # step one token\n",
    "        x_t = model.transformer.wte(next_token.squeeze(-1))  # (1,C)\n",
    "        feats_t = model.features.step(x_t, feat_states)     # (B, K, C)\n",
    "        fused = feats_t.reshape(B,((model.K)*model.C))\n",
    "        x_t = torch.cat([x_t,fused], dim=-1)     # (B,T,K+M,C)\n",
    "        for i, blk in enumerate(model.transformer.h):\n",
    "            x_t =x_t+ blk.step(x_t)\n",
    "\n",
    "\n",
    "        # optionally re-prime to strict sliding-window semantics\n",
    "\n",
    "    # decode full continuation (prompt + all generated)\n",
    "    return decode_chars(out_full, itos)\n",
    "    \n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "import time\n",
    "then = time.time()\n",
    "prompt = \"of\"\n",
    "generated = decode_sequence_char_rolling(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=4096,\n",
    "    block_size=2048,\n",
    "    temperature=0.0001\n",
    ")\n",
    "\n",
    "print(generated)\n",
    "print(time.time()-then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.arange(128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "UzqWUbNRlmiD"
   },
   "outputs": [],
   "source": [
    "file_path = 'simple_model_tiny.pth'\n",
    "\n",
    "# 3. Save the model's state_dict\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "def fenchel_decode(logits, tau=1.0, iters=3):\n",
    "    \"\"\"\n",
    "    Fenchel-decoded logits -> smoothed probability distribution.\n",
    "    \"\"\"\n",
    "    energy = -logits\n",
    "    p = torch.full_like(energy, 1.0 / energy.size(-1))\n",
    "    for _ in range(iters):\n",
    "        p = torch.softmax((-energy / tau) + p.log(), dim=-1)\n",
    "    return p\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "def decode_sequence_char(\n",
    "    model, stoi, itos, prompt, max_new_tokens=100, block_size=256,\n",
    "    use_fenchel=False, tau=1.0, fenchel_iters=3, temperature=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Character-level decoding from a prompt using the model's logits.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    start_ids = encode_chars(prompt, stoi)\n",
    "    idx = torch.tensor([start_ids], dtype=torch.long).to(device)\n",
    "    cc = model.transformer.wte(idx)\n",
    "    feats = model.features(cc)                  # (B,T,K,C), K = 1 + (K-1) deltas\n",
    "        # build low-pass bank on μ_top(x) only, append as extra K channels\n",
    "    feats = feats.reshape(1,-1,((model.K)*model.C))\n",
    "    cc = torch.cat([cc,feats], dim=-1)   \n",
    "    plt.figure(figsize=(40, 26)) \n",
    "\n",
    "    plt.imshow(cc[0].cpu().detach().numpy().T)\n",
    "    plt.show()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = idx[:, -block_size:]\n",
    "        logits, _ = model(context)\n",
    "        last_logits = logits[:, -1, :]\n",
    "\n",
    "        if use_fenchel:\n",
    "            probs = fenchel_decode(last_logits, tau=tau, iters=fenchel_iters)\n",
    "        else:\n",
    "            probs = torch.softmax(last_logits / temperature, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    return decode_chars(idx[0].tolist(), itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAf/CAYAAAAA8V73AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAA6BdJREFUeJzs/Qm85WlV3/v/fnve++wzV51Tcw/VMzIoyJCAgJBAdy6GiCM4JYbmcsVcJfmbcHGKN4FwNYoD4kUBQQEl3iuKIsbgFfQGFUgYbJqmp+rqmqvOvOfp93/9TtMtfVPru04/Rx66qj7v16te2jz1/PbzG/Zee3ev9aw0y7IsAQDgK6zwlX4BAAByBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFKXkEjSZTJJTp04l09PTSZqmX+3lAMAVLcuyZGtrKzlw4EBSKIjfMdlXyS/90i9lV111VVatVrOnP/3p2V/91V/teO6DDz6Yb8fDH/7whz/8SR4/f/LPZuWr8gvnt3/7t5PXvva1ya/8yq8kz3jGM5I3v/nNyYte9KLkrrvuSpaWltz5+S+b3NH/5ceTYrV20b9TP5+f/8VNKvr47QPitY/puYWxc+x99i+y6oa95txwyp5b7OvXra1M5PjWEftbSXGgj12/YB97MK1/gY7qenzq1CRozbnp4/bc9gE9NxPLKozk1KTY1/cxScOvx0S8YwtD52X1I5CUO/a6h8661LHHVf26WVGPjxr2uubu0ddaXU/vnNR9yvXn7dcudcLvY9F5L3vjqbgk/Vk9t7ZqT97737fMsdG4n/z5Z37ukc9my1cl4Pzsz/5s8spXvjL5p//0n27/cx54/vAP/zB5xzvekfybf/Nv3PkP/2u0PNhYAadYsS9c6gScwsUP+aXj7i7gFKsiaIg15yZqrvMZVyrrT5tiVQQc541XrNjHLlb05Eyck7dutebdzpUBx/mALHr74abh1yNVAaewu4BTHIkvad661LF3GXCyWhb8nsnE8+edkxdwimpd4/D7WNQv646re1F07oW6nqXicMefzY+bpIHBYJB86lOfSl74whf+7SIKhe1//vjHP37ROf1+P9nc3HzUHwDApSV6wLlw4UIyHo+T5eXlR/3v+T+fOXPmonPe+MY3JrOzs4/8OXz4cKTVAgCuqLTo173udcnGxsYjfx588MGv9pIAAI9R9P+Gs2fPnqRYLCZnz5591P+e//O+ffsuOqdarW7/AQBcuqIHnEqlkjz1qU9NPvKRjyQvfelLH6mryf/5Na95zWM61mg6SybWf7gT//Fq4vxXt8ajY+GjdB/9bwL/B+WWHq+ITLRJ2fkP6CKLaDDr/EdK5z9iqnX1Fp3/8L9s/1Aub2XB57T92vNp0Jq9ud7r9ufCswkrm86xZ8PXtZ18ag2V0l1lz/XF9fLu41j9x/mynJpU1/Sx6+fssY549ryEBO+cvGzEyoY9Nq46yQxF+3oNZvTretmIoyl7zPvv/sOmva61W+wMtPGgnCT/LXF9VbLU8pTo7/3e702e9rSnJU9/+tO306Lb7fYjWWsAgMvPVyXgfPu3f3ty/vz55Md//Me3EwWe8pSnJB/+8If/h0QCAMDl46u2tU3+r88e679CAwBcui6JLDUAwKWPgAMAiIKAAwCI4pJsT/Cw5nF7b7OsYKclfve/+CN53N98863BKYnldvgGnIWul0ppj9XEZqVeymuuIlJEvVRKlco7ru4uVXcwI9KinbRWtXGol7rcPGGPd/Y7GzM6afcq/X03G392lvTccsvZ50rc595CGrzpY21VTk1GDT2uNv/00sjHtTT4nLznviyeIbWBZm4wHfbs5YbN8NKMkvN8bR3+yv4G4RcOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACCKS7oOJx0lSWqEzM4BO8f+fW+y62xygz323FFdr6m+4jSOF2nwW0cKwfn1Xk2Kt0V8uZWF1xY17OuVGXVSO63TUetWa/bqLLzXVbUhhV1s8e7N99ZVW7Wfr+n79dzBnBxOhuKcq+vJLq61nls/PwluE6BaPez2nCbOs1sTbRUmJf+zK7QdyFC0H8g1T2bB7RxUDZBqgTIe6DU/jF84AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKC7ptOjuUpoUjTTS21/+IXPeO992mz7uPjs18Af+kW5t8Gvvvi04JTZz7kYmvh54qafelucyXdLJeCwM7LHiwNk+vpKGtwnYRYqntzW9SgXvLOm5qZMZr47tXQ91zt5W/SpF2NuOfyi20/dSjL1nwLuP6rxUq4fdnpPXZqInWn6MRKmA187BK0MoDNOvWDsHlZJdv2A/2OOB89B/Cb9wAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABRXNJ1ONX1LClWssfcgmCypI979P1b5tj7/ka3Nig5W8CrGg2vhYCqZ/DqEXqL3rHtsXQcXkvj1buoegSvLkCt2Z3b16/bFzUWqrYj1zitj711JA2+HlkxDdouPzec0fdi6pT9cI6ctgm9vaImZaLnljrh7R68VhG7Oae+1yZArKvUS4JrZQZOywVVj5crt8T1cmqiVA1QqSfmDvVxH3n9Hf0tAAB2iYADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4pKuw+ku2v1wyi17XmVT54yv3TId1AMjN67J4WQ0Zb925vTfmJTs126c0edUdL5atA7bY1MnwuuDCutO/YZTp9M4lwWtOdd8MAnuaVMWtSH187r3R3dPIbh3jHc9JlX7eqSj8Dobry9NbUU/X7XzWXD9T3fJeXa7aXDvmN2ck3e9tg7bx+7O6GPXz4XXFpWdzy/F69MzrttjvQX7A2rcdz68voRfOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgu6bTof/qt/zmpNS9+Cm9/523mvI6z/ffMMdUGQK+pbHc22FY/Z49lzt0YV+yxrpPmWxjrY0/fnwVtw54bzIS/rkoR9toEqDV7W+a7ryvaTGQi1XYnafeqbYK3LrWVvzpubjTltQnIgrbTzxX79tiwoedO36/HC6MsqNWD1+7BO6feor7PzRPic8J5L2cFe27f+Xyqruljt/enwa0gVEp2Kt7Lqbj/jzr+zv4aAAC7Q8ABAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEcUnX4fzOz/2DpFi5eD+ASs3Ocx/M6uMOxHbqXo3FcFrn0KeTsFqG7WMvhtfwFHp6XNUkZM7XElU7UuyF18p418Sro1A1BUO7A8VDrzsMq1fxaoe8rfy966FqIbwasep6Fn7sil5XkmRB13In93EwG163tJtz8u6zqnvynvu+eEZ2e73UZ4FqJeK1xyiMxJp2+NOFXzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoLum06N5cmhSr6WNOfW6c1cetrtu5yxvX6hjtpTbX1uy0xM1rCuHptHt0iqe3RfzUKXHshfC01XI7CT6nXGdfGrTm3LpIW22c0XNLfdGuoeG0gnDSWlXLBu96VFpZ8LUe1fX42Hgv7SRFeFxX92myq+ul2l94LRlK7fBzyop6XeV2Ft6iYq+4Xg/q69Wf058ThUFY2rNbDtAKe81H/b2d/TUAAHaHgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjikq7DybdjT40t2SflLLhWpiS2FlfHzdXP6WP3xLbkVVGj49VvDGec+o2NNHhdXu6+qmnxtlL37oWqC1Br9uaqa5nrLBeCaw68mqdSJ/x6qHMe18JbG3htKLpL+tj1c1nQtcyVRD1LbijahXh1S8NmGnxOZVF3khuLuqaJU8NTEM+Qd70qTpuJzn57rHFKn3N1YxK0LtoTAAAeVwg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgu6bToylaWFI1t5Es9O/1vMK2P21soBrc2SDOdsjiYToNTiOsX7GNPH9NzS1295Xl7n/3do7qehKdri23rH6Kvl9raXq3Zmzuccra1F1vXjyvh6di5SVnND0/zrW44c522CsVBFpSa7KUYV9fDz8lrX+Clt+/mnFSauLcu75yGooxh6oR+XS+de/qYaDWyqI89rtknPRHRYuLch4fxCwcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEMUlXYfTWU6TYvXiOelTp+1c9NYRfdypE3aeu1X3s5P6DW+r/9tf/iE5911vvS14i3dvy/O81YNl6DwlhZE9VuzqdY2dOp38Hoes2Ttnrw1AQdRvqNqOXH1Fj4+MZ3Yn10Odc99p11DeCn82vZYMk0p4WwTvPqr6NNXqYbfn5N1ndexUl70lqm4uc0rXMuf9mIr3o9fOQd1HVSI2dq7Vw/iFAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4pKuw8lKWTIpZ4+578zR92/K4554wYw5NinpJHn1urnmCTtf/X1vulXOLRrnmuuKepVcxemVUj9nj41Vbr5TOzJY0HPr5/S6VA2HWnNu2EyCazBGonfMYCZ8rnc9vevR3LDHenvS4LoRr+9R54BeV3lT9XkKf09sv/Z+VYv1lTsn772u+h41TnvHTkwjrz+Q8+yqz4LGGb2uTNVMiWWlOyvD4RcOACAOAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgiks6Lbp+Pk2KlYvn6rWO2Hl6a7dMy+OWt+yx1tVO2upxJzV1UYw7qYWjhj1WW3UmO8NqXdV1Z7JoQZA5W+Z3l/R4TWz1L6+l0yrCe1217b1Ktc2lkyw4bdpdV9ceG4rnYyfrHk6Hbaef2xLvi2JvF++JvF3IqUlQq4ftY+9Ng8/JKweobtjr6u7R3+XHNXusKO5xbjjjfQaFtXrYHjc+T3O11UlQO49H/b0d/S0AAHaJgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjikq7DKfazpJhdPP/76Pu3gutwxnV7TB03d+yb9LGbD9pj/VldU5CJu9V1tqav6I4MSbmdBefuq23vva3UJ17rA1FnodacG9XSoGuZK4q6goFzn6qr+tjqmnjXQ53zRNRQeHU2udp5+9ipXYLh1rRMSvo+FUb62J3lQnCtjGr34J3TcFGPj2v2uiqbWXA9Veb8DPDqhzqilqsq2lt4ry3vQ39nv134hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjikk6LHjTTpGikzarU58GMTh+diKvipVRP358Eb5decdoA9Iv23H/+PR+Sc9/5ttv0sUUbAbVVv5diPKrpcypvOqnghbA15wr9JDhdW6V6N86Etx/wtpf3roc6Z7WN/0628i+M7bHOPj23fla0kXBaLhSHcjgpb2XB13o356TSxL0Udu/ZHJeTYFOn9Lpuf7n9WfCbP3urnJuKc8rE50/i3MOH8QsHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABDFJV2H01vOkoJR51G/YM+rrut6hXSkc+gVbyv/wjC8LmBStfPv3/cmnV9fnNK5+0nBfu1i19lefkrVrOiX3bo6C2+rINbstRjoLel1NU6lQbUdu91e3rsetQtp0Pbx23NXsuBnt7qm53aXxZb463Kq25IhHYfXU+3mnNz7rNbl1KV8//fatTLv+uXbgq+191nQPRhe96Y+f8Y95/PlS/iFAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4pKuw7nmd7eSktFM4+wzZ8x5U2d0Hc5Q1JV4vXTq5/Wx1xft+ZNyFly/0XP6bwybSbBRTR971LTXXT+XBJ9TruP0UlGGolfK9DF9rXuL9lhhlATX/+TS0W6uhz1WWw2vs8lNyvb16s/qufULWXBvmEkxCa6LU72FcuX2V+accuOKWFcjCa6VGR7Qc736IfVZoHpXbR97NQnqizV26qEexi8cAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFJd0WvT6jdNJsVJ7zC0I1o/qPMzmCZHanIanJOaMLO5t1Q09Nx3Zrz0py6nuFvGjqfB0WpXK622lXj+bBa9brTlXaoelJudq58VW7HV9Tl66rdr23rse6vlR2/jnJiKN12tDcfur/0jOfdt77S31G2f0OfVFqYC3Zb6XRp5Mws/pXW+9LXhd6j55nxPq2dtJG5NiP7ydw3A67Pka77B6gV84AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoLuk6HKXUs3PZS109t7Nsx+GZ47rYob1Px/CpU3aNz8ip71A1LVVna/r+Qhpce9Q6qM9J1bR4W6l7dTrB9VL5ug/Z6x5O69eVtSOiXiVXzfSxC6Keyrse6px78/o+lTrOvRCtINR2+rnb//WHzLH3/vStwe+J3EDcK6+eStWseOdUdNqFpBV7rL6SBX9OFJx6qttfbl/r3G/8vH1erav0safvs8cqLfucRkN9vg/jFw4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACCKNMuyneWzPY5sbm4ms7OzyRNe9QazPcHQ2bpeKYtt7b3jqi3Lc0W1PbhzJ8pt+y90nHTaqZP64IM5e35/Qc8tb9lzJ7oTRFK/4KSejsLWnKus28fOdlEQ0HNSzJ0OFvKaeNdj2AxPER419Li6JuVNPbcm0t9H9ST4nLbnX/wtvqMUYtW2wzsn93qJ93qpp+eWt8R9dioFvLKOzavtA0yd1s9XYRh2H8aDXvI3b399srGxkczMzNjHl68OAMDfEQIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgiku6PUF1fZIUKxff2jwrFILrFbr77Fz12gWdJO9txz9s2mNb1+h1jTZEvYuzlbpXr6C2rq849QpZwZ773f/ij+Tct733NjneFPVD3nb76pxbYtv6XHUjCa7hqZ/S6/qu1/5R8PWoOW0olOGM0zZhEFbPkuvNp1+RZy9X7KVBrR5yfVGr5Z2Td5+rq/ZYOsmC2ybUxHFzPVEP430WeJ9942pYvdS4v7M2I/zCAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEARHFJp0UX+1lSMtIP1ZboBSc1cPp+e6y7pOdmhTQ4BfTo+7fk3Be/6y/MsXe9VafTjqtOGwCRuuqlUnYO2Of8vjfdKufe/q8/JMd/4xduDVqzl+J5+8v16/7mz9qvW5hy0ouddalr4l6Pn7fndveGpz3nhjP2M9J8QM8diJTrcks/e5nTwmIwa49Nivqcx43wc+o7xx5Oh6VMe6nLRaf9wEikoOcaZ+3x7rI+drmVBKWRTy5enfI/4BcOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACCKNMuy8P3Ov0o2NzeT2dnZ5IYffkNSrF58r+7yVlj+fK6ykQXX2RSH+nKOavb8qnhdj6o5eWhcrzsTw6Mpfex//t127cg73+Zst++0c1D1HWrNuVQcurKpX7d10D74qKnnNo8nwc+Adz36s/bc1KmF6BzQx54+Zo+19zu1H2fsY/cW0+DaD692TbV6yP3mm28NPqeJU7s2fZ89Nmqkwc/mwPl8qq04NXViOHN+Yqiau6Ko4xoPesl//63XJxsbG8nMzIz59/iFAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4pLuh1PqJknRqD0Y18PqbLxaiNYhnV8/ajo9SfpJsJ7og+H1FPHOeSjWXXVqQ1R/l8lS+Dl5vVTUmnMlMdd73awUVq+yk5on1VfEW5fi1Ut561Y1GKp/S66zT9QHOeVlXm2I6i/k9VtKi+Hn5F0vtS7vnEqitihzngF1rb26JtUnzKvlyj9vLeP+zp5bfuEAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiuKTTovMUv8zZlj8kTVOlpk6cK6a28H5oPAvait9Np13KdpWqW12353f3hKdhem0AvHPuLqVBa/bmFoZyalIYhKUP54qTLPjY3vUot7Pgbf699gXdZfu1G2f13M6yWlca/J7wWm+oVg/bc0UasHdO6Sh8Xd45DabFs+l8hgyd9hjdffbYD/wj3c7hbb91W9CzV3DO95G/t6O/BQDALhFwAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAUVzSdTh53YFV95KKWgi15X2uvb8QvM1/b1HXBQwb9vjMA7pQoiDmHvyzLTn3+K0zcry3YB/79pd/SM59+zvt3P3OrL4ejTNZcG2IWrP3daqyol+3dZUYTJ3WBs6yVD2Vez322gdvngxvIeC1ofBqeBpn7WN39ut1TUrOujbCruVuz0k9e17dU39WH3s4mwXXiDVO63Ut/+WmOfa+v9HtHAr77bG+qE+kPQEA4HGFgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4pJOix7X8rzHx56y2FnWcValWnqtDYpOSmOpLcZ6+uAb19rrXrtlWs71tjxX1+t9b9KplJWaaLngpId6qbrFfhK8Hf+wGf6608fsc8oK4W0RcpNyFryu2krYdvleirDXhqLU1ceubNnHnj6m544rerwo3hfec63SyL1z8q6XalNRdXLjj/yxXcZw4Wv1ezlJ9LrUZ4FqvZIbztjHrl2w52bO597D+IUDAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjikq7DmZSSJC099hYEk7LORa+fV1ua6xz4ctvZ9v6QHePXm8Xg+g2Pyq/3ttz3to9XtTaNs/plO8vO1vXV8DYBw2lxbGdrelVj0TmgX9erDamupcHXYzhtzy239Lq8LfNVG4q3/KGuxUqSNLieZbiojzyYSYPbE7zq28PPabShr6dqb+DV662JWhn17HltAnL1vvrs08dWNVOdJXtsrD+6HsEvHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFJd0HU5lM0uKlewx97wZzDm1H6LuJCukoeUI22qixmc0pSeX2/b4YCYJ7u+yTdQXeXn/qj5I9bPJNc56/XBULU22i94x+nW7y2H1Krm3fkDXd9RWd3E9uuG1WLe/+o/kuOp7dPTzdv+W3L3fZteVTCpp8HvCq3vyatN2c04vftdfyPG3/dZt5lj9VPh96i479VQL+tjFQSo/MxVVAyR7fdEPBwDweELAAQBEQcABAERBwAEAREHAAQBEQcABAESRZlkWnr/3VbK5uZnMzs4mT/xn/z4pVmqPuQVByWkhkJXC02nTsT52qWuPdZx0yNqqfeySky47dloyqO3UxxU5Vaa9enMrW+Fpmt4W8Fkado+3xwth9zA3Vi0V8hR20WLAux4FkX7a3q/vcfOkPnZPpL/XnBYD8yLF+PxT7ZTpnaTOq/dFua3nTsS2+d45edT1bpx1WjI07bmFoZ6791M6nXv9pqY5tnGt/o1RFK01mifsfgzjQS/5b7/9o8nGxkYyM2PXaPALBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQxSXdnqDcstsTrN9oz2uc0scdztg58sOmzpGfOhFeo1HeCq/fqK3ZOfK51iH93aIqahK8+qCp0/bc1hE51e3nMKmErdlrq1AQ9Qa5Ud0eK7f0XG8L+O4+Ne61qMiCa4tUnU1uUk6Crd1i19p09+jXrV9walZmsqBWD7nRrNMvZBfXS61rvK6PXV2336+bVxWCr3Wu3JkEt3NonLXHVMuXcX9nv134hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjikk6L7iwVkmL14jFz+n47/a/ccbbyr9vpkNP3J8HbjrttAJxt7bvL9uRJWezDvoMUYtXOwUulVOnaR9+/Kefe+206xXPm3rA1e+e8eVROTX7gH/2ROfb2d94m53adNHJ1TbzrUTwenurtpT2rdO6BKBXw5t7+8g/Jue96q76e08eyoGdvt+fkXa/pY+FtTEq9SfD1et+bbpXj60eLQWnPue6esPYo6WBnrR74hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiCLNsmxnCdSPI5ubm8ns7GzyhFe9ISlWahf9O+ko/LR6i6INwIpTw+Pk3+ctFczXdbZxP/QRu37jzN+bkXMHurwjaZzJguqScq0j9tyFv9GvO/953ZPh+Itmgtac6+yz133kjzeDt4AfXfyR+1vOjvilbvj1uPBke11b1+rXbT6QBF+v2ft0+4v+XCG4XcPYqXdJxUuv36znllrh5zRs6BtZGNpjmfNVvife66rdx07qh9Rrl7r62N/1Q3b92W/8vF3/Mx70ks+98/XJxsZGMjNjv2f5hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiOKS7ofT3ZMlxdrF88oX7rDzzbcO6zhbFPn1NaevTG9BDifdpTS4Z42sDanr1/V6bPzqb9o9SeoX9LqOvn8raM07GVfrVmv25r7vQd1TpDdv36fBrJyapGM9Pv1gFnw91POj+sbk2vu9/i/2/FLPqWtbt2tauov6/TZqJME9pLx+S/f/4+ngc9q41vmcEP2HMqcWq9wOe/Zy9RVdP9Q6aK97UkqDe+2MZ+25Y6/47Ev4hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjikk6Lnn4gSYqVx55aOHG2Q59+cBKcsthbSINDvEr/9JT1rvYy3TGXXWWPDab1OalUXm8rdW/rerVutWZvrkc9Iyp9OPfqb7e3eM+96623BV+PTLxj05Gc6q67cda+V+tHi3Lu3L3j4PebOqdc7Vx4Gvluzsm/XmHp614Zg5dW76Vzq8+C1tV6bjreWXrzY23H8DB+4QAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAorik63DG1bxw4eJjw6adT948oXPRO8uF4G3Hq+t6fNi0x8otva7+fEFfC6FX1gsviK3Wu/v0uuoX7LGq2LY+15/T33kmxbA1ezVTBafWQdXDlHrhW7znqpNJ8PWQ92k5Da4byRX79jlnBX1sVdPitd3oF/Wxu3vSoFYPuz2ncttbV/izWRa1MoWRPievfqgwCmslkjv5vBlzrNwW6xrurIiQXzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoLum06Op6lpSMLcT7IiW2t6jTHUsdO8WvP6fnDp0rOpzOgtKxc41zdjpt66Cemzrp3MOZLDiV8uwz7VTKqTM6LTpx0qbb+wpBa/a2Wq+v6NfduNZ+3anT+nW9FhbTJyfB16OzX6T7H5dTk9S5FaoNhddioNS1xzr79PWYVJ12Dy/9o6BWD7lxJQk+p9qqXtftL7fX9dYP6NT4wkh8Pk2luyq9UK0PvHYOqhygu9de17i/s7YG/MIBAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERxSdfh5DUxo0r6mFsQjGv6uKoFgdfaQOX952orSXCLgVLPfu3KRrarGozqRnjuvmpB4G2lPnfvOLguQK05l6WqxYC+XhOjvsurV9l+XedrnLom3vWYPma/dmcpDa6V8dpQNMXr5obTYdcyN30sCW73oFo95FaelAafk3ef1bqmvXYhC+HXa1TX6/I+CxRVMyU/u5x2DA/jFw4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACCKSzotejiVJpNq+phTnydO6nJRpI+OGn7LBCUTWZzdw3rucKoY/LreFvHNB8NTKVWKsZeK66VNq1Rxb83tw/Y59+f16zbO2mOtI/p1Z+6Rw8m4lgZfj/o5+7Vvf/mH5Ny3/KHeMl+1oTj1XLsFRa4sOliU2/rZS0fh7R5kq4ftc2oFn1N3Obz9hUrn91KXveuVOCUOohogGcw4rSJESvZQpImP+8mO8AsHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABDFJV2HMyknSVq++FhV5LkP0/Dt5cttpz2Bsy15X9UU3J8FbwFfbjnb7R/S6+ou2+sq26UM29r7de2I4tXpDJtZ0Jq9moLqup5b7GdB9Sq5B27T9R3F/i5aCIhzVtvl545+Xq9btaGYeJ8U9SS4JsW7j+koCa5bGjbCz8m7zyefNxN8TtU1+5oUxLO3k1qa6pksrEjnoVc3R4pd0fJlsLM6Pn7hAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiuKTrcJons6RYuXj+d2e/6N1g1O48rLaaBdXR5EqdJDj/3qvhGYlah85yIbi/S64wVP05nB4/hUJQDYVXW5RrnE2D1pwrt+y5wyn9upNSGlSv4tX/5GoX0uDroZ4f1TdmJ+sO7d+SG8zarz0QfVS8c9qWhfUW2u05eddL1hc5yyqLujnvvezVD6n5M8fHcu5I1AAVxNRMH/Zvj7GzvwYAwO4QcAAAURBwAABREHAAAFEQcAAAURBwAABRXNJp0b2FNClWL55/WBju7riWspP2rLbwznX22ceeu2ci52aFNPirg9puf3tdS/ax6yuT4HTtgpMWXdY7wCdFkfqs1pxrnBPnPNFzW1fbc+sXwlO5c+N6+PXo7kmD2h7shNr23ktBVynGW9fq1x1tOO1CRAcC75zTSfg5edR7OXXShIdT9tzuPr2u6fv0sTOR0u+1c1Cfm6o8YrzD7HR+4QAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAorik63DKnSwpjrLHXKMx84CuK+nP2nG4P6fXNHK2S7/95R8yx979i7cG128MZp06ifXwdf3au2+Tc1XtiLfdfiLOydt+Xq059463i3U7dQNH32+f1Nlnzuyq5knVYHjXQ53zb/z8rcF1NrnqunhfOCUrw2YadC1zL37XX8jxt/7ftwa1eshVN8PPqT9fCG5D8epv/yM5911vvS34ep1/qn5TdZay4OvVPmTPLR63506ynRXi8AsHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQRZpl2e726P4q2NzcTGZnZ5Mbf+gNSbFae8zzva3DJ2UxV2dUJ+OqHp9+IHy7fbVNe11txZ9vS36zXtfC39hj/bk0+HqVxJbmucGsPnZ/MQtac271a+yx6op+3anT9uumE6/Vg/4eV+pkwddDrWvzaj139j798JZ64thH9Lb2jbP2sbPiDveuD0jn9t5vMw+Mw8/pnFM+MWff58qmfkY2rrPHZu+RU5N0rI89rojWB85nzKGPbJpjF55sp2OPB73ks+9+fbKxsZHMzNhlA/zCAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEcUm3JygMk6RghMz6eTuHfjCtc9ELA3us5Gw93zqoY3hvPg2qZ8lV17LgegRvy/PTf9/OnR8/9lKnR6ROlVdlQ/+FQx/ZClpz7uj77ZqCc0/z+iaE1avkuvv0+My94ddDPz/Zrta9ftSuSym35dSks2w/9/UL4ee0/dqibkm2etjlOXnXayQ+C7xzmr4/vPxRXWuvzku1Esmt3TIdVMMz7tOeAADwOELAAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEARHFJ1+EMG0kyMepPMpGrXt7SOfCFcXgO/HBGHzsd2/nqzRNO3n/DHquuZ8H59V4NkJe7r/rDlFt6XcNmGrxur25JzfXqg1QPlvb+YnD9T279xmbw9VDn3DgbXpOSK3Xtse6SvmD1c2nQtdzJfVR1c17d0kT2hnGezamiWwdoyZyv8mNRNzcq6etVFHWCuZqo12sdcuplWvZQJqJF5vQYexi/cAAAURBwAABREHAAAFEQcAAAURBwAABREHAAAFFc0mnRxWGSFAuPfYvucdVJDRQ5s+q4uYrOiE0ycezeol6Xarngb/Gu11XZFFvAO+0cyq3wNPLGWfucclkxDVqzx0vXVs9AZvXE2GEKendPGnw9ErFsL9U7K+j7OKrbY9P362NvXWO/+NwXnTKEhl7X5nVhrR52e05Dp4NF+5B9XjP36LlVkc7ttZlYv0Ffr3QkWlg4n/gqhV21bcmcVO1HjrGzvwYAwO4QcAAAURBwAABREHAAAFEQcAAAURBwAABREHAAAFFc0nU4U6cnSbFy8bqFocjtzyr6uJOyykV32g+M9LHbh+1jT53Wx1Y1Ld526Gpr8VxvwR4r9sNrWtS13EmdjtqKXa05V1u1x8Zi2/rczPFxUG1HrrqeBJ+T2/6iGVYPtZM2AKoNhdpO36tpKfX0c71xbSG43YNq9ZCbiDou75y8+3z0/fYFe+C2GTk3E6fcuKBrsabv18+uqufz2jlUxHu5s1/U9zj3+GH8wgEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERxSadFd5YKSbFaeMxbaRed1GY1d+Rspd53UnXV1uN9p8VASbQYGDhbqVfP6FTLzrL92vVz+tgqlXcwp691/Zw+5+r6JGjND80V13pOf9daP1oMSh/2tnjPdZey4OvRF89Pd59eV/OYPvZYpAE3T+j7qNpjrC/Z13In2/Grdg+q1cO2NPycvHYOal3eOQ2b4nrN6uvlpTar9HevhYVKYZ8Wz8/4q9We4Cd/8ieTNE0f9eemm256ZLzX6yU/8AM/kCwuLibNZjN52ctelpw9e/bvehkAgCvhX6k94QlPSE6fPv3In7/4i794ZOyHf/iHkw9+8IPJf/pP/yn56Ec/mpw6dSr55m/+5q/EMgAAl/u/UiuVSsm+ff/j7/uNjY3k7W9/e/Le9743+cZv/Mbt/+2d73xncvPNNyd/+Zd/mTzzmc/8SiwHAHC5/sK5++67kwMHDiTXXntt8opXvCI5fvz49v/+qU99KhkOh8kLX/jCR/5u/q/bjhw5knz84x83j9fv95PNzc1H/QEAXOEB5xnPeEby67/+68mHP/zh5K1vfWty//33J895znOSra2t5MyZM0mlUknm5uYeNWd5eXl7zPLGN74xmZ2dfeTP4cOH/66XDQC41P6V2q233vrI//+kJz1pOwBdddVVyfvf//6kXnd2wzO87nWvS1772tc+8s/5LxyCDgBcWr7idTj5r5kbbrghueeee7b/u85gMEjW1x+9nW6epXax/+bzsGq1mszMzDzqDwDg0vIVr8NptVrJvffem3z3d3938tSnPjUpl8vJRz7yke106Nxdd921/d94nvWsZz3mY5e6WVIcZ495+/negs6vTydhtTC56ppOdC+30uDt41X9xuzdeq63RXzjrP3dI82y4O321bb1ua1r9LGnzmRBa86VevaNXBfXMle7IGoOnB/qjbO65qnUToOvR+OsPXf5L/V/2zz1XP1FbVIKq7PJ1cRzP3FaQZTF9XhIFvTs5YZT4efk3eekFXafvLq5cjsJvtbe/HSSBdef1c9lwW1ZvmIB51/9q3+VvOQlL9n+12h5yvNP/MRPJMViMfnO7/zO7f/+8v3f//3b/3psYWFh+5fKD/7gD24HGzLUAODy9ncecE6cOLEdXFZWVpK9e/cmz372s7dTnvP/P/dzP/dzSaFQ2P6Fk2efvehFL0p++Zd/+e96GQCAyz3g/NZv/ZYcr9VqyVve8pbtPwCAKwebdwIAoiDgAACiIOAAAKIg4AAAorik++Gk44f+PNaeN4X18J42Xi8drz9HQ+Sy92f1XFXT0lvUc9tV3WOjtmqvq9RxcvdvtMcap+RUt05H1QWoNXtzp+937uNSEsyreRqIXije9chKWVB/Fq/Oxuuzovq3eEZOPYt3H1V/IVWb5j1/3jll+i0j11Xs63U1N+yx9gG9Lrd+qJYG9/gpde2xrug/Ne7v7PngFw4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACCKSzotersFQfXi6XgDsRN7wUilfmRcbHnutTa4/eUfkuPvePtt5lhxqNc1rtpj5bZOw0wn3nbpadCYl2JcdlKqvRRPlabprUvNVdcyV976ymzxnps5Ngm+HqOGPV53UnFV2nMuFcNe6wyVYqyu5U7uY+PcJKjVQ25SCT8nr9VIpWWPd5b0d/lUtE3wrpeXzl0V93lsfF4+cuypsOsxdspFHsYvHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFJd0HU5WeOjPxVTX7XlFZ/v4rWvtscYpncf+vjfdKse7X2O/9uJn9boKok5n6yr93SFzdg/vL9qv3TzubIcualq8vP+aU+uweXUatOZcdcWeO3NWz20dEnUlrSS4/sertfGux3nxbBYH+loXhvrYZVFXUuokwddr7LQn6C7rdc0cz4JaPeQqW+HnNGqEt6Ho7tPnNHt3WNuDndQP9cXzNWzqddXPpEGtV2hPAAB4XCHgAACiIOAAAKIg4AAAoiDgAACiIOAAAKK4pNOii8MsKRp7qg9mRVprWx93+pg91lvUcyclnR549P2b5tip54qeCttbsWdBKdPb4309fugj9p7ox75pWs5duMNe19bh3X2nUVu1qzXnzj59JrgNwKQUlj6c68/pY6d5W41AR99vn/P6TU092dlBvrNs36vU7hCwrXnCPnj7gPee0PfxxAtmglo95FoHw8+pfn4S3IZCvc9zJ79xJriEQV3rXE+kL0/fr4/dXwhrvdJrjZLX/3Ti4hcOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACCKS7oOJ6+lKBrb3zfOZEHb6ecKA3usdt6pwXDqO9ZusWtaSh197KHYit07p5Lz1UKty8vdVzUt3lbq6pxyRpmVu2ZvW/wsdba138iC6lW82qHcYDb8eqhzLnd03cjmEbtuJFcUz32x79R+LKZB13In91G1e/DqqXZzTt59Vuvyzqkg6uaK3fBrnRuKtgpFp3WGqj9TrVfGg16SJH+mD84vHABALAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBSXdFp0dSNJipWLjxXG9rxhXacV9uftMZVunatsOnvAC6lYs5diXByGpzt6vJRrlcrrbaXeOuSkRTtbyIeec+Z81VLp2CrV1kvH9q6Jdz1Ct8vfSVuO7pK9rnJLr6u6lgVfa8+oHt7qYTgTfk6ljl7XcMoeq64nwanLJed1y6JNSW4irklN3CePSkEf93f23PILBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQxSVdhzNsJsnEqBEpjMK3/1Zbh3s1KZ1lnY9eW7XHsjS8fiPN9DkNnG3vBzP2eP38JLglg7eVuleno2od1Jq9bfG9mpR0Ys9dv1HPLbX1utQ18a7HYDYN2i7fq7Px2lAM5pLgZ2DqpPf8hLd7UK0edntO3vWqn0mDn82i+Iwpt/Trtg/q6zV1ahLczkHVn6k6wMkO6+X4hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiOKSrsOpXciSYuXiieNj0fNmsKCPq/p35D14lElZ59BX1+3x7mIhuH4jK+r8ei+3v7IenruvakfGNTnVrdMpiZoptebccCq8Pigr2OPT9+trOa46dV6i75G3rvoF+5xbTn2GqknxasxU/5ZcWfRw6SzrdTXO6fuo5u+mjss7J+969cXnSOOsPqfqevj1UnWC3nyv1m/qtFi3uNTjwc767PALBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEMUlnRadp5eaKaYinTZz0nxv/44PmWO/+bO3yrmNs06ab89OOyw6qYWq5cLEuZNeWnSpb4+39+nvJSr1eVLR66quOevqZEFr3iaGU2c79XE1DW5RobZ497byd69HLwvaxn8n666J1x462+2rFGOvlECdk/dse2nkKs3XOyfveql1eee0eVXRHCv29ev2nZYMtVX7tftzaXBKdd4OxjLuO/nWX8IvHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFJd0Hc52nrxR59FbsPPCCwN93Pe9ya616R70to/X+ffrR4vBtTRFsS15RbQ92MmW55NyWLuGXHVD1G+kaXBNSm4wnQat2dvGPXOu9bBpn9PCHeF1I7lRIw2+Hr2FYlCd1k628ldtKKZO6cKl9gH7IekueS0E7HPKVcTzpVo97PacvDYAoe/zXGXTPqfO/kQqt/Qzko7Dn4HBrHg2ReuVbEx7AgDA4wgBBwAQBQEHABAFAQcAEAUBBwAQBQEHABDFJZ0WPSmnSVpOH3Pa66jmpIf20uB0WpXGmyt1w9IZva3+1Xb6uaKTCq5Sar0t8zv7xVb+TupyuZMEp6Z6acCqBcHEyeKcvj8s1dZLe94er4dfD/WMqOPuZN3Dpj0+mnLaAIgU43HZed3p8HYPXhp5Ip4B75xK7Sx4Xep97t1H9ezlikOdzt2bt39HjEQbiVx/IQtqvTIe0J4AAPA4QsABAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEcUnX4RS7WVI0tsUuiBz7xhl93P5sEtzaoLtP5+5XV0S+uldSIHY8L7f03N6iXlepLWqPCulXbBv3oVMXMKlmQWvOjabEtvZ9p26pG1avsr2unhyWtVze9VC1R2ob/52sW7V7qO6i/YVXz+LVD9VWsvCap1r4OXnXa+q0XQ/Tn3PezJl6v+l19Rf074RUPCNl515UNuyxrCTmDmhPAAB4HCHgAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAorik63C6e9OkaPSBGTXtvPD6OX3cyqY9t3WVnnv0/Vty/OzTZ8yxYt/Jv1+0c/eLTh6812ND1UIUh04NxpK9rpkHdO+O9gH9nWfqVHj9hrrPvb16bk30APJ6sHRn9PWa+2IWfD1KrbD+LF6dTa55Igu+1qVOFnQtd1LnpWp8vHVNHw8/J+96qXWVt8KPXXOul2jX5fKe3fJmWM3T2Klrexi/cAAAURBwAABREHAAAFEQcAAAURBwAABREHAAAFFc0mnR+Tbv1lbv08fsed1lncI3rthj08d0yuLaLdP62CIVs9yRU5OpU3aK8eZVheAt3r11jZxt2ovDsNRR75xyA/Haas25dBL+uj2RPqrSh3MT512lrom3LjW3esZpQeE8Xz2Rdp+O9VyV0q+u5U7uY7Ef1upht+fk3efBbPizWeqEXy9v3Xm5SOhnjErJPn+tPW/Soz0BAOBxhIADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4pKuw6msJ0mxevGxcVnUUTyoax02ry0Eb+89e59Okh92wo9dGITn19dXndYHopamK9oP5KZO28cuOfn5badOR9UFFJy2CYVxWI2Ody+GdocJt4VArrJhr7u7qK+H2lJ/8xo9t9hz1rW1s1qKixnV0+Bt/kdNPd7dK1ofrOhns3nSfgh6C4XgepZcfzH8evUX7LHM2em/uq7/QnXdHhs4z+6WqOe77t2r5tho3E9E6eMj+IUDAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4pJOi1a2jtp5r22RXpwrb4WnLG5cU5TjlU07lbLYDU+znBT1wrYOFYLXlTrL2rrKfu3qqnOtW1lwqwiPahMwKTlbwIu06cJAv27zjE6NT8f2OXcy/fyU2/ZY5nx99La1HzXS4DYA6poMnbTngmg/kFu4zx4b1Zx2ITcUw7fqd1p6qHKAcTW8BUp/wZlrlII8bCIeIS9FXT1+5581b69p0EuSzycufuEAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKK4bOtwFj5n57Lf+sMfk3M3xB7w//UXvl7OLTu1NKqOZ1zR+fdFsR1/xaln6c96296nwdvaF0UdRWHkXA9dduLWM8hj7+Lr1MwxuxCnP+fVPOmTKvbFNXFqnoqi3qV1UK9rOK0PXtmwxyaV8DqcuXsn4dcjbxMg2gh4bSYqoqaub5eVbOvs0+ON0/a6yk6rh9GUaDWy6bzuBX3So5o4dlvPbS8Xgz6fxsnO3qf8wgEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERx2aZF98QW37/39ufKuaW2ndLYvlqn/zWP63XNHLfzR9eu17mnW3vt7wfNB3eXfqxSU5sn9b72wyl7Xb35NDgde3u8YY/VVrPglg1eSmx7X+ErssX7blPQVVrr3s/o+9Q6WAxOA1ap714K+urN+nvtzH36PjZPj8yxSVk/P2s3lILPqbKuj11fse9FbU3fi/GqfU26i/p6rd6kx8ste2zYKAR/DpTE2Hjg5Kd/Cb9wAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABRXLZ1OD/6P7/HHOs5hRQ/9cFvNcc++K0/I+e+Z/0Zcvx33/8cc2xc82pp7HHRUWFb46zOkx807ZqDlSfox6QwtMdKTruGgl1isW3qlD1/4jy9pYE9t7qp1zVs2Ndj4NR+DJv62OVWGnw92vtFzZPYxt+racpNyqptgj7nkqgfqp/V12Nc0+vauKYc3P5Cvq7TcqE/59R5VcPvRUm8L6ob+nVTXeKTZOp94VyusaoRE+/zrEB7AgDA4wgBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEMVlW4fz5h//zqCak9y1d3bNsX/5ju+Vc88/c48cr0zZY6Ohk8suh7Pg/i7bhxbTK05dQFayF9bbo88pdepO1El7fWeU0SQNrtHw6kamj+lxNX8wo+dW1+x7UXf6A3nPfW01Ce7v0tpv34zWVXJqUuqkwXVeQ/Hs5aZO29ek3NG1aZ09+j3T2xO2Zq+sqSVqrXL9BTmclOyPr6TqPCMVUZ9WX7HfrKOh+0bexi8cAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFJdtWvT6UTuW1lZ0amDrkJ23OrxB9wHoLus0zd6i/drjGSe1ULQnmP58ZVftCTavtq9X6qQQ18+JVEonPbS3qI89EmnkY91lIknFKRcHeu5QvO70cWfbeuddJdO5ncz49Zvt1+6u6e+P81+YBLdk6Drb7U+J56vU0ye1edRJuxdj1TXn/TZvj49q+pxmjuv3Y1awH8DW1fqciqKdQ3VNTk0aItU7VxOp84Npfb06++zxTdEmYtwbJ8kfJy5+4QAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAorhs63Be/u1/ao69+w+eL+eOq3YcLnXD8+tzi5+zxwpDvd/+uGrnyHf26dddv0F/t2icsc+r4NSsqNz9UVNfL6/mIBGlIzVnq/WsINbV0C+rtpffuiq8LslTcEqx9vw3+7XHNf26G9foZ6DUscfK7fDaI/Xc5qorcjiZOWa3Rugs6ffMcNoeK7f1626IupNcsWdfk6VPOs9mao+19zu1aaJeKtctpsHtL1R9WnnLHiv09XEf+Xs7+2sAAOwOAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQxWWbFv2Hb3yeOVY6qtMK+wv2WPmEft3UzuB86NizadC24rlx1R4bzDrp2n19zplIpVSvm6ts2mNTp/TcYVOPD+aS4G3vVeqpd05ZIex8d5K2qtLMi1197N6eNOh8d5JyXW5lYS0V8vvUFOsqhLeRyHWWi8HHVq1I3HWNnfdjXZQpOKngo0b4e7m2mga/p1InY38iMsGH0/bkiUgR/3L8wgEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEARHHZ1uGs3yhiqZP3P/dF+y909+oY3dur89Fn77bHRk7u/nDKHm849S7dZa9Oxx6rbOq5W4fta5JO9DlNndU3Iyvaxy72nS3zRe1I0anhmb3fLqialPTczpLTBkDULFQ39DmVOvb1KnV0EVh3SW+3359Jg+vLVH1HeUXf4+njem/7zn67aKq9z3k/Loi6twtO6wznuR9X7LFKS59zWzwjhbHznjmtj711xD72QNTSbBPDVVH/M3bq/B7GLxwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBSXbR3OqG4nlJe3dM54bd0uOuju0TG62NHHnpSy4L4gzVP2X9i4Rq/L6/+ianx6i+lXrIZn4jyBpW4WtOZtYrjU0VM7ot7K63czqutjq/nltr5e/Vl7XZtHSsG9TjyjKT2urmfJ6fFz7qn6gqm+NYWhPnZt1b6e3WWnnkr0iPKe+36vGPzcj5xr3V0oBB87dWp8VA2i+gwZD+iHAwB4HCHgAACiIOAAAKIg4AAAoiDgAACiIOAAAKK4bNOiZ+4NTzs8/+RycBpm/bxOD1SpvP1FPbfQt78fNE/quXP36Jzr/px97KygUymrIvVUpbTmBnUnRX11ErTmbeKUaxv6evTEsYuDZFfpx2or/0FTX49xTYw7mamqLcJO2i4onf2i5cKak17cDb+elS2n7UYvCTauea0i0uDU+N5e+/krDJzrNRWeru2lqG865RUW2hMAAB5XCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoLts6nMGsnRc+nNb59Wob7tl79OvW1nR9R3vJ3rb8P3znb8i5L51qmWNf//pXy7ndRf3dortkX69MtFTIpZk9t3bBqS0ayeFk67B9vYbNJNhgTm8fXxb1HUXRMsHbEt9rudCb1/epJ2rIRg1vXWnwc6/afXitNbxaLO9rr36/htcOeedU2UiD64OGU/rYjVOF4Fq+zn45LK/3lGhxkhtX06C2B7QnAAA8rhBwAABREHAAAFEQcAAAURBwAABREHAAAFFctmnR/Xk7Te/qD3bk3HHdvixnnll1Xlen29ZEyuPPvfblcu7PibHhNU7KayUJbl9QGOqUx/6M/b2ldUivy9suvS7SqqfO6HWJbG13K36V8trZ56TiOu0cVNpqZUMfev6LY3Ns0NTfH7eu1seWrTecUx5Xs+D0Yjd1Xmx9P3JS40tte2w4ree2btB9KCqn7Ydk+pg+dn11HPR+2gmVRu6l3dfWVdq0KJ1wyhsexi8cAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUl20dTibKYU68UOzxniTJzH12Lvpwxtmm3U6v31bs2bns63P6dgwb9lipF17PkuuLdg5FUQeRq7TsYw/m9NzOfr2uSdmeX7a7NbhG9fCt+iub+thu3dJ8Gnw9hlPFoGc+V+wnwS0Gauf19RrO2OO9RafOZpAGP7vN05Pg9hbeOc3eUw6+z97ztXqTuI/Oz4BRMwuupyr19NzBtHq/2XMz55l/ZG07+lsAAOwSAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABDFZVuHc/27VsyxdEv3wzn7osPm2NW/r+e2D9Xl+PpRO8+9shWefz+Y03PHVafWQfTpUfUZua3D9veWwaxTt+QceyRKpqrrem7ocb1aBlWjsy11egCJR2g4ow9dVDUWTi+d3oIeV9ezcU7fqOxc+LOXZOE1YpvX6OKjijinqbOT4NfNtQ7Zz31tRZ9T86Q9PnE+lb33TG9R1Hkt67nldliNzri/s98u/MIBAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEcdmmRZ9/5qI5Vhzo/ND+gp3+t/I1DZ1evKpzFtXW9l46ZOOUva5/85r3ybl39fbL8f/03ucFt1wYi0zwwaKeXOjp7zzlTbHtvbhPnnSkx4ciBb22ol+36GwB37rKHhtX9Vy17X1lXc9d/LyT2lywj91e1vcpFS/tbbffPqjX3ThtjxUG+tiJuFWdvXphE92dIGmeyIJSk71zHtf09aif0et+wXf+tTl2pqfz7u97243m2Ei8zwsD2hMAAB5HCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoLts6nM6ynQe/9zOj4K3pvZqCkbMVu5rf26Nz2Qf77YX9+7d/p5w7dVLXYJTssqWk2NfrGvftcy5v6kfM245ftWSon/W2vbeHenv01HRsH7u66lwPp31B87g91l0qBNcHqfqxh+hjV1r2scttp4XAnP3ak0p4fVmuqGo8MuecxbB3TuVOFvxeLgz0uv7ly37PHHtS9UE595W/8oNy/KNvf7pYmJyatG+yz3nUEC0VutThAAAeRwg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgu27Tog3/WNsc6B2pybqlnpxC3l4tybs3ZIr4wtMeHM3pu5Yy9X3plQ8/dvNZJt52y5xec9gSjKft6Vc/r1529R6drtw/Y89sHdpaKedF1rXmpuElw+nFZpBdvH7sbfj1aBwvB2+mn+tDJoGmfV3evc71EKUGxp1+3sz8LvlfeeyYR5zyc8tLI9Xj9gv3aAyfd/5d/+aXBZQjptD52YWzP7+zR51Rq2eNzX7DnjQdpopO5v7S2HfwdAAB2jYADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4rKtw+nvsfeI7807cVakqk+f1K0N+jO6Tmdctw8+9WB43v+oLqcmJbssaVuxa7/2YE7XBTROFoJaPeR6i/pe1M/br52l+nqlYtk1cS29LfGHDf26nX16fDRtH3vqhL4epU4S3AbAa61REI/2S7/lL+TcF8zcYY790K+8Ss6dPqbXNZwOa/Xg1UR947/6f+XcNyx/Vo4/5T/8L0HXMjdqJKaSU7eUOnVxrcPhNVGZqOXqiRoe1aLky/ELBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEMVlmxZd7Nq5g7V1ncK3eZWd2txvOWnPFWfr+k07TbO24WzVv68YvjW9s4u7Wle5FZ5uW3XOqbOsv/OsX2+PNR/Mgte1cZ2cmjRO25PrK84+/2f1cHLOHho4W8+X2+L5Oaevx1Ck5D/02vb4h9/29+Xc9X9m5/n+5Ct/U879xR/6djneXQz/mFJp9945/b8nninHKwtZUAmDdy+89/JzXvEpOf4Ns3YfgZ94zyvk3IJoyzF/l53rPRo6eeAPH39HfwsAgF0i4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKJIsyxzKjQefzY3N5PZ2dnklle9ISlWaxf9O+N62Nbg7lb+zi7cw6nwrcWL/SR4a/phU8/12gSoY7eucupdSvZ4YaAvWHVNj0+dsmte+jP6+1I6sddV2dLnNK7a6+otOO0HnGdgXMuCr0exK47rtKiYOOUsg1lVi6XX1Twu5nacWqwlXdvW22OPjevOx5cYLnWcN7NTbjVqZsHHrq7ZY2Xn2Sz1suDatrHdtWVb88QkqK5tPOgl/+39P5psbGwkMzMz5t/jFw4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIIrLth9O40wW1Pcj19trj2VO6v7svTpHPh3b46tfow/e/Zqeva5VnWA/dVJ/t6i07HXN3SWnJunEXvetP/wxOfcbmnbvjtwPv+VVwT1HVA1Gb0FfD1XXlDnvmsq6Hv+H3/uX5tiLZz8r5/6LXwu/Ht29+vlKJ+E1Kd0l+9iDvq6zKQz0uuuif1DBacMybNrrGjbC62xyJVGbVFvRc3uLafB9qq04nxPLov5sQ8/NimHvmXF/Z79d+IUDAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4rJNi56I9L7+gp7bOG2nFY4rOq2wfUCPVzbsset+c1XOvfNf2Nt+f9/zdPrx77/lucHXq+ikrXaW7O8tv/d2/brnv3tajv+7V/+6Ofa/v+F7k1D/+J9/VI7f1Vo2x77wWzfJud71+quf+npzrP16nd7+iu/8iDn2vt94gZw7crbyH1fFdvttp21CLzDdOk/zXdfrah20n6/eHj131LT7gdTO6nTtxil9zv1Fe2zifE4khbAWJttTneerIlpcqLRn71rL9gQ7jCT8wgEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEARJFmWebs8f74s7m5mczOzia3vOoNSbFau+jfmVTCWwwURB58qa0vl7ddusqDL/b13MY5++CDGZ1g35/VJz2upkFb9ed+/J+9xxzrTcpy7i+98VvleGFkX+/+fPj3pcqGvo8Dcb02r9OFJbWzel3VNVHv0pVTZU3LYEbf485+59kd2POLA70u9Z4qiRqdnTxfjVP2up//A3arh1x/YheI/Onv2PVQufZ1+qQb99kfMjWnVcRE1Ol418Or01GtWVQtjfd8lTv24GjYSz7xez+WbGxsJDMzdr0gv3AAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABRXLbtCVSaZmGo506fsPMOB9OFXbUnUCmNKp0x15u3U5/bB/S6yi0vJdYer6zLqcmbf/w7zbFB09nifUmPV9fDt2lPxfBYpM3neqKFReOkvtaVTb2uobgmQ92tQT67o7qeWzuvr3XjnJ32unm1PufuIfFgO2m85a1C8H1UrR5yk6LYqv8Gva7KGZ3SP3O/fb0G0/paF8Szm46cFihH9AUdzBWCUt9z45pI2W/Znz/jvtP34OHX39HfAgBglwg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKC7bOpzRlD1W2dRz+7N2HO7POb0NnGYPpU54HUVv0V5X+7DeMj8r6YVNHbPz6J0OA0lv0b4mtRX9ukVn6/rWIfvY5a3wr1PDqfB6l1I72VXNkxrvLRSC21s0TzrX2qlb6u4R9RtOLU1lxZ47nNGvW9nQ76msYM9v7XPacsynwec0e08SfC9U7VBuVLfXNa4mklenMzpiv6n+4DlvkXPvGi6ZYz/2q99jT3RqGx/GLxwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBSXbR1O47SdCF9b10nyfdHLQvWz2Un+ffug/Reax3V+/fzdI3Ps0B9vyLmdq2fk+Mnn2t89/uM/eZec+9nuEXPs3X/wfDnXq6Upi5qX3pJzsYXKmr7WE9EvZ+uornnqOf1dhlP2uqurcmpSsB+BpLvH6cEydM5Z1FvVz+prrda9dbV+3dYNAzme3V0Juh65cmsXfYucnjbDKXt84nyydg/bCy/N6OtROK4L9o7+rD3/X77he+XcM8/dY46Nlu15Y6c88WH8wgEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERx2aZFTyp2nl5nr87h6y2GpVnuZLv9+nn7tafO6pzr3ry9H/r6Cxf1uvo6BbR2zl7XG/6t2JbcSRUvHdXXuntIn3OxXQha80MLs4d6i871WLEnTz+gX7e9Xy9rstdOq+429NzaGfsZaJzR5+Sl9PdEWvXWVfqc6+ft156/U69rck85uD2B91y3D9rPz5ZoXZCrrsnhZOpUFtxi4MiH7Hz/4gVd4rDyDYfkeOto0xzrT+vfGK2rwu7DRIx9OX7hAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiuGzrcIo9Oy+83Ha2Whdp8NV1vTW9Z+uQXUex8kR7LDeYs1+7cULXFIzrTu3RHvvYWVF/Lxmr3dKdy1VZ0cfuHxqaY92CfnwzcejJnLOv/aq9JX5vwdnm36nBmHrAvs/DZhb8DBSG+lpO9OOV/Ow/fbt9bOdGvvqD32+OzX1hh3vXGzr77fnDKT33//zm/9McG6sHJD+nj3+XXteJmjlW7DnPSMluF9I8rU9q5LyXRzV7PHOegamT9tyRfbrJuL+ze8wvHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBSXbVr0qGGn6W1ep+emImO2su6k4jpph5UNO+116oSTri1SiFMn/djbxr15wh7rz+pj9xdECvqWTpecOq6P3Txub11fEqnvnlFVb4nf22OP/dj//B459xOta+X4H/zes8yxpnM90ol9Pctt/RA879/8Vzn+4Y0nmmN3f6veEn/fU+17ceqFui/C9F36XhS79lhftHrI/dBbX2WOHfm/T8m5k3+t3+v/8Vt+wxz7iV/SLT0a5+11d5b0h8jK1zktPTqF4BT16pq4nrPi82eQ7Ai/cAAAURBwAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAUaRZloUXM3yVbG5uJrOzs8ktr3pDUqzWHnPdSbGvj18c2HOHU9624/rY5bY9NphxWgyI7cFnjuncfK9OZ1K2X3usyyTkNRk510u1kcjV1rLgc1IyZzd1tcV7yalp8toAePc59Hp0F/X3x3Si113qBC8r2fvRk/ag8xGz9qyDcryzVAh6T+SKvbCauNzUWd3CYuNq+82eZuHrGjvtLZb/YkX/hXP2+MnvulFO7e6zFz59vz1vPOgln3vH65ONjY1kZsZuvcAvHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFJdtP5zhtF3r0Nmn51ZED5eOyFPfnrvu1J3I+iB97KwoamUqaXBdSW4wJ2ppnFqHgihXGE7rc/LGe6IHkKppymXi69SoET43HTk9fk7rc+rutecP5nVxUXavvbDhlJyaTJxnRNWGVEX9T671RPtNtXlYf8z05+VwMqkkwbVYqrZo4tSX9eZ1QVXrKnusvKmv9dy99sLHznv15D/co2uiPtO0B536oFJLfcbY88ZeYduX8AsHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQxWWbFl3ZtPP/Fu7U2453F+3LUm45L+xsAa+261dph7nW1XYLgs3r9Nzp+/R3i1LHXndvQR87E9mjV39Q73nfPqRzrs891V5X/bxel0oB7c/pqR/47v9ojv1fm18n5773/d8oxw/+uZ1/3N2jc3XPPc0+qdqKk5rqpBCrtGiVJp7rz9p/4eu+57Ny7tuP/IUcf9pPvDq4HUhWSIPPqb1f/4Up0ZGhMMyCW0UcfPkxOffb931Cjv/cz3xbUIr5Q+uyx4Yi29prYfIwfuEAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKJIsyxzNqx+tI997GPJT//0Tyef+tSnktOnTye/+7u/m7z0pS99ZDw/3E/8xE8kv/qrv5qsr68nf//v//3krW99a3L99dc/8ndWV1eTH/zBH0w++MEPJoVCIXnZy16W/PzP/3zSbIpE7y+zubmZzM7OJre86g1JsXrxOo7iwD6tVJfhJL09afh26M6W+UWRn++1EFj8fN9+3a2BnPvgP5yW42qr/7Koacpt3CDmiu3Oc9f8xgk5PjxoFwGdfbruMaDqLBbvGMq59QfWzbG1pyzKuaO6Pudy276e8391Ss7NynbhyQPfqvtueHUnqs2EV39WW7XfGBPRVmN77rpdX5YbTNsLX79On1TvoH2f5z6ni0dmHtAfFP2Z8O/rvQV7bqovR1Jb1x9C3UX72KWefi+/+AfsmqhGwf6M6bWGyb9/1n9ONjY2kpmZGfPvPeYr1m63kyc/+cnJW97ylouO/x//x/+R/MIv/ELyK7/yK8lf/dVfJVNTU8mLXvSipNf726qyV7ziFckdd9yR/Mmf/EnyB3/wB9tB7Pbbb3+sSwEAXM47Ddx6663bfy4m/3Xz5je/OfnRH/3R5B//43+8/b+9+93vTpaXl5MPfOADyXd8x3ckd955Z/LhD384+cQnPpE87WlP2/47v/iLv5jcdtttyc/8zM8kBw4c2O05AQAu9/+Gc//99ydnzpxJXvjCFz7yv+X/6usZz3hG8vGPf3z7n/P/Ozc390iwyeV/P/9Xa/kvoovp9/vb/xrty/8AAK7ggJMHm1z+i+bL5f/88Fj+f5eWlh41XiqVkoWFhUf+zv/XG9/4xu3A9fCfw4cP/10uGwAQwSWRpfa6171u+z9GPfznwQcf/GovCQDw1Qw4+/Y9lCVz9uzZR/3v+T8/PJb/33Pnzj1qfDQabWeuPfx3/r+q1ep25sOX/wEAXMHtCa655prtoPGRj3wkecpTnrL9v+X/vSX/bzOvfvVDW4w/61nP2k6XztOqn/rUp27/b3/6p3+aTCaT7f/W83dlUrJTMStdnRo4EdmSzQf1XC/tcOsqEeOdlGu1dX3ver3v+I9/9/vk+PmRHcR/7W3/SM69/l0XzLF0S7cnaD1FJ4nc8KN3mGMvaeqU6gPlNXPs/Einib/9p7/JHGtdpdN8xzX9DDRO2c9A+Yk6tXnzsP2WLdpZ8+5znesctvNx5/7G2ar/lJ0y296vn83zT9IfQ5lY98GPip4K+X//nbcnt5f1ferNi74beXr8LfZYsaefkYK4VyX9lnHTzFWrke6ynvvHv/Bsc6wgKgnGg/w+/Gd57O21JY9Rq9VK7rnnnkclCnz605/e/m8wR44cSX7oh34o+Xf/7t9t193kAejHfuzHtjPPHq7Vufnmm5MXv/jFyStf+crt1OnhcJi85jWv2c5gI0MNAC5fjzngfPKTn0ye//znP/LPr33ta7f/7/d+7/cmv/7rv578yI/8yHatTl5Xk/+Sefazn72dBl2r/W2B5nve857tIPOCF7zgkcLPvHYHAHD5eswB53nPe952vY0lTdPkp37qp7b/WPJfQ+9973sf60sDAC5hl0SWGgDg0kfAAQBEQcABAERBwAEAXHp1OI8nFbGlfmefs136ij1XtT3IbV6jY3h/wZ5fXdHrSif23MqWnJq8+ae+XY5399rrHszpY59/pr1df3FgtxfYibt/yi52+Ovrnyznqm3e5+7T7QkmB+2xUV0/A2952a/J8fWJ3VbhZ3/qO+XcrBTefiDTj1dSXrUPMJzWk7eOVINr00ZNPV67YL92Z0nX+IwrafA5bdyoC+OqF8JbDAxn7HOeiDXnaqv62FPn7BffvF7XFq2L93rjlL2ucd95uL6EXzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoLtu06N6inaZXXddpmIkY3rxax2hvi/jaebGuNb2uVAx3lvW63JRZ8ST0F3WOZ2Fgp1ru/cxIzu3POWma19rby4//dj/YixPXa/2o3qu/JFpYzNyrX/bH/u0/l+P1FfuapE4K+tjOPk6G0/r5Kbd06ur0A/ZYVtDHbh+wj10Y6NdtnJLDSW+vKHEYOm0TztipzZV1/brFrj72UHS4mIj75JVAeJ8DvQUnRf1q+8189e+35dysbJ/zua+tJ7vFLxwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBSXbR1OqSNy95d1HntV5OenesfyZDCnc+iHYit2VTu0/dpftL8flFv6dbtLafB26Yf/sz52dcXO7e8c0MUyU6cGcnxcsQsa2ofD63BqX9RT1db1E13Ck2RFfb1K/WLw9Wg511Opn3XaBNhdE5L+vNc6I6xeJdc7rM/5nc9/hzl2bqwP/u/f+gpzbOSUlXg1df/k2/7cHOtP9EfrB/7LM82xwbx+3US0KcmV2/a9Wvmahn4vb2RBz/3E+Vx8GL9wAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABRXLZ1ONVNO5+8vqqTxocNOw7Xzzs9a5wc+XHVzpEfOSUWtXWxbqfFT2Gkv1ukD9pjwynne0lq18r05p2eIlO6cUjj7CisiY+jvqJ7/Kw37WP35/XFzgq6ZqW3Rz0D+nos3jk0x1Zv0gVCW1fJ4WQwZz9fpY6eW1m3z2mwrN9vlTN63f/b62+3527q+zh6ihhr7K5/0B//wrPNsWJfH7shnoHWVfp6ZbqFVFLZsI9d2XL6Gu0vBPWfGutL9Qh+4QAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKK4bNOit47YsXTipBUWRCZuYaDz/7y0w4poI9A6pI+9+mR7bPpefVK72Yp95rhO0yx27dTUmkiXza1fp9e9+gT7Ea2fTYPTR9eeoF+3dj7suDlnZ3rZJqB9QJ/TYNpOIf7Gl/+1nHvb7Gfk+E++/vvNsfp53ULgwpPsdO7ZL+pzKnXlcNKfseenzpv5ql+92z7uE4/Iufe/VN/IYs/+jJn7gj7n2qr9DJS64c91rjCwjz0QbTdy/QV77mhKtC7o7aw/Ab9wAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABRXLZ1OAVRNlB2tg4fNexc9VLXa0+g17V12I7x5S09d+9f23PTTL9we5/+btHZb59XVnQS/1N7fNTQU4s9PV4Yhq35oXXZ44WRrkeorttzD//hipy79pQFOX7hyfZrV9fk1GQwY4999O1Pl3Pv/vAhOT4/PmmOnb5Nzx02k+Brrebm+rP2WHfZaQUxf505ljlft6/5PV17VBjZ77mTz9GFb+W2PVa7kO3qeonHPmmeHDnrKga1VhkP0uSBxMcvHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBSXbVq0Sl9uHdZzVXrysOlsH++E8MqmStXVc3uL9msPp/S6yh197Fl7F/ektq5Trke1NHg7dK9tQrllz28ed9KiE7GuOb2u7l577PwzF+XcjpOqWxRp+anzDBTtThBJYayvx/nn7Jfj9dVx0HObG9Xtc+7PyalJZUOPT50W2+I7n2AlkXafOteru8duBeGZOuW0Kdmy31Ojmv4Q6S2koY99UurqEofqhr2uYVOUZQy99+JD+IUDAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjisq3DUfUfJbE1eG7YFLUyQ50D3zjrbC0u1lXe0nPn77b36t88Ugpu15Dri9x+1SLA09c79bstGSor9jXp7dH3IhU1K/Vz+lqv32SPdTLnGTijj92fT4PbW6i6pZ4uD0qmj+nx/qz9/XMoWnZ47Rz6zrq2btQ9Kgona/agU5JSW7H/QmXdmbumb0broF3T0t2nn4Fiz547mNWv69VqqZq6lmiPkpuU7PHKpj1v7BUgfgm/cAAAURBwAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAUVy2dThKxan9mBZ9VraO6MT/1hF97Dd862+aY59qXy3n/tHbn22OtQ85/Sic+o5/9U8+YI4ddwopfv83nmOOFbv6dYfNJLjPyv/6PfaacycH8+bY//W+58q5V/++KNZyLvXqExpyXPU9GszoY//Id/2OOdYo9OXcN/3My+W4qgGaPqmLP0Z1+7vr0fesybl3/q+zcvwl/+AT5tiH//Dr5dzmg/ZJVTfHweeU6y/YD8Ivfcuvybmf6dkfFB/57mfKucULuoHQ/d9jH3tSkVOTcd0+p4G4TZMe/XAAAI8jBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFGmWZTvLZ3sc2dzcTGZnZ5NbXvWGpFitPeYUPi+tVW15njkhelLRB6+ftQ9e7Ou5nQP23No5va7e3iS4fYG33X5vMby1QWbv0r6tddROXa2f0JMzkfTfPajTfBf+m33s5mmdTtvep9c1qtnXa1KWU5NSx74Xk4rTrsFJjV+8w24T0Dqo82knJfu1p047D0HqlBrsLwW1eth+7TOT4Pfy2Lme6v2aFfTcznIa9Nzm9n56GHwvNq7RB9/7WbuOoXzKTscejfvJR+79+WRjYyOZmbFz+/mFAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4rJtT9A8YefIl5yttEdVO4+90tLFDJ29Ooar+qDhtM7dV7vPezU8V/+u3iJ+PH3xeqbcqefo7fbHVXussimnJtV1ve6Db7Vz/8ez9ppzhYFdL1Nc0T0qVp+xz57b1XU43T36bTWcsc959u4kuIanu6yv5dQJfeytw/aN7Cw5z6a4JJOiLi4qi9qi7fnlsFYPufXr7fdjSXSgyM3dp+9zYSDamBzWtVi1VXtufcWp81rWz9dgJrzWb+uQ/Qz0n2C/J8aDXpLcm7j4hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjisk2L3rrKTg2sruoUz/rKJDjteesab2XhKbFqXZ7egWk5vnnEfhSK9q7126oixbPmpD2nEz2+ecucOdabC9+Ov7qhU70HIkW9JdKHcwc/1pHjpXtOmWObz9EP0HDKTretrOnrMdSPgE7Zn/Luo2hR0dfrGsymwWnRJX2pk/o5e93tA3ruyec7xz5tv2dSndmcqPfUpFQMbj+QmzprP/jDRhqcdq/aJmTO+T6MXzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgu2zoctS1+YZQF12B426HP3aXHVX7+SJeGJP2ZQnjLhZrO7a9u2vPHol1DblKxxzt79dxy29lS/4x9wbNCMbTkKals6Zqmcsce7/b022bjaF2OT244ao6l+nIkzdP2AzSs6++PI72spL9g36vG6fAan1FTv+7CnfpNdfpZ9n0e2GVa2+rn1DnpuZUv6uu5+kT7ZpU39PWaP27fx+raUM698CTdlmP1FnvdZaddSOa8pSw7LMPhFw4AIA4CDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIIrLtg5n6pRdR9Gb1znyqu6k6NS7ePUw7f0ixutlyd4x7QP6u0NxoI+taoDGNX1O0/eH19l09ul19xbsZiijqSS4Dqfc1q/bOGM/PxOv/Md5V5U69sK6S3pdG0cLwV8fS239gBXEM1Ic6vtYPWmPN87oupL2gYocP/xf7IVV73hQzl37xmvNsWFTX4/eoh6/+g+75ljroO6ZtHqT/RDt+Rt9rfd8TjeoWruhGtTvJtc8aT/3W4fFA+bUjz2MXzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoLtu06NZBsZV/V+fwVTbs8UnZed1DTprvHjvtMJ3olMWskAZva19y0pOzVLy2sy6VUr15nV5Xoa/H5++yr1fLSQUvjMPS5nNbVxWCWkzkina27LbWDU7+u6Du8/CITpdNHtDb2tc6qhxAH7osUr1XnqhThFtX6XvRn7Pnl6+yWz3kusv2OU2JVO5c85RO5946ZK9r64h+Ngfz9muf+zr9sVxqh39sp/pSy7Yci3faD/5oqK/Vw/iFAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4rKtw1Hb8ff2eHUldo58eVPPra4527iv2vNHDX3s7j772LVzeu783bo/QW/RLjDqzYXXjczdqcc7+/R4d4/9nWjmuC4qSMdZ0HFzQ9H6oLwVXpOSm/mkXc+wcot+S6qWDNf/rL7Hw3l9zqf/nqiXUXVaee1a2x6bflAXLqUj3e+hup4FtezYHh+nwZ8Dg1lddFcU9XyNM95+/ak5kjntL6ZP6Oe+fs6uiRk19MFbB+zxSdle83jgFKd9Cb9wAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAUVy+adEiZbHu7KTd3RPeBqA/r1Mti2I7/swJ/9UV+9iVTb2wM0/XW8SXRVpr85ROedy8qhic9jx7n07xHEzb59zepy9Yloa1LsjVLthj/QU9t3DaeUjEcPOkvh6FkT3WOtqUczt79fUqdcSgc0qbh0W6rfNce8/9qG6PtUUbku2XHoaN5Xp79HipnQaXR8zdYz+A/Vl9TltOC5TuQjWolYj3+TauiDGnzcjD+IUDAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjisq3D6ewTbQCaWXCtzLCu5/70t79Ljn+qc4059ju/9Vy9rkFYfUautqLXPWza12vjWmf7eFFzsHCnXti4pr/zDMS6Jrt4etOBvh6qTqdxWh+7vV+f07hqn1NtfRK8fbyqPdtJrZZ67nsLTkuPZng7B68eRq1r+ZP6+erPFILvU8GpLZkW7TEGM2lw7VrReV2vhky1N1CtHnJ9cZ9LPXteSh0OAODxhIADAIiCgAMAiIKAAwCIgoADAIiCgAMAiOKyTYuubNpjU6f03PZBeyxNdbrjG/7t98jxzWvsGK+PnCTLf901x9avqwWnief6e+1cy/Km/l5S2bDHWvv1I1Zf02nAZbFl/voTnPzQiX3OC59Ng1NPVQp5rtzWqadj3SkieKv+rWv0687epdddE/eicU4fe/OIfZ+7y3JqMpjX93H2C3aeb31FH7soUq4XvqBTqidl5xkZ2ddk41r93I9FeUXZSbvvLup1TYr2sSeixYDXoqIg0q3H3ofXw8fY2V8DAGB3CDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoLts6HFXrkIr6jNzsvXY9QmefjtGtg3r8yIftvdrHVd0G4NzTGuZY6pSkVNf1+NW/b69r84YZObez1z7nirNl/ubhYnBNy41v1/veTyr2sVeeLPbTz7fjn02Dt/lX7Qe2j70o5oo15w790ao5lp45L+d2nnGtHt9rfxxsXqXPqX7BvibF43JqMvsxXQ8zEI9fZ6kY3PpA1Q7l6ufD66kOfqyjWwiU7Ot54vl1va6zWXAtjVeHkxXsY//gD/9f5li3NUpe8x8TF79wAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABRXLZ1OKqfiVdHkYnU/sYZ3b+lt6Bj+AVR/1FyalYSMdzbq+d6tUfnnzprjtU29Dmr6+n14RlNyeFkNJUGrXmbuBXdvXpdpbZYU2OHzT8Mxb49NpjXc88/y/4Lc/fYdVq53ryuWenP2+dVXQuvSZmImpNcd09Zjg9E/yGvrmQiDu2ek24xJfsibRzVtTQT8clbdXr8ZE5Prq2r7LGS3VLLref7hTe/zBwbD3pJkvy1Pji/cAAAsRBwAABREHAAAFEQcAAAURBwAABREHAAAFFctmnR1VU75TFzwmxbtCAo6x3xk+q6TiEu6J3YNbXuzEnV1cuSqeAjd7t9kU677myl3tbH7hwU97G4i3Nq6HUVe/a6Sq0sOF02VxNb+XutDcYVe3z9uqpOexYtF3LDaXussyynJlOnwlOq04mTnly3H/zegl6Xd5+V6rq+XukkLE3cu89eqcDUSeecCvaxy1t6bqVln1R72X5DZeI1H7W0Hf0tAAB2iYADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4rKtw+kvpEF1ELmpU1nQdue5rSOF4NqQ6ro+tlr33N1jvcX7dCG4RqM41OsqdUR7gmWdn988oe/Fns/Y411xj706icXP6dctt+3ruXajftv0F7yt/O11T53Wcwsje7w3r+/xYNY7Z7GuU3KqrIcp5jvXq9d1t8y3113s62eguiae64G+HlvXOLVaXVWr5bTlqNljtZXw9im5gjgv75z7s4Wg1/VqGx9Z287+GgAAu0PAAQBEQcABAERBwAEAREHAAQBEQcABAERx2aZFlzft9L/WIWeb9hl7bnkz3dV26LVVe35FrDnX3WvP7ezTt7LkpJ42ztk5xBMnDbMq1l1f1X0Ruov6O8+obp9z3UsfFbeqN6/v49Zh+3pWN5x7vCKHkyQT7QlEumwunaRB6em5xmknhXjDvletQ/o+TSr2WN+51oVRITi93StxKItr4pUwVJ37qN6vxb6eO3vM7lPS3aPfy+39TiuANKyVSK5x1j6n5mm7VGA01GUZD+MXDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgisu2DqfSsvPJB3M6F/3nvvXXzLF3nH2OnHvPO26U47X1cVC9QS5LxdbhzleHctepHblg9yDYOiyKLPK6AdGCwKvhKbf1+PQJ+3pV1u1ahm3iNpf6emGdPcXg7eG9cx427YWl+jYl0yfs+5QV9HPd3lcMfkbq5/XCNq+xx9KyXldtXT/4nT32w91d0scuPxB+Tl790HDKHi+39Dn154pB9XaPpRVAyGdM+6D92pvX2C886RWS5AP+6/MLBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQxWVbh7N12I6lg1mdf/+6n/7n5tjEqSmYNPW6hk07/763pNc1EXerccrp0yNqBnIb11bNsbE9tK26Zo+V+06tg1MTtXKzfb1SryAmsFeO1xOpeVxPnhW1Mrnuon0jW6IOInf2aeXgGovpY06RT2Bfme1jP2Cve/0mvbCTz9evPX2PfeypU3pd40oafE5zd/fkeHfJrk/buEY/m6l46eGUnJoM5vT1bB63P/umTuu5E/vxStZvEM/mDh8tfuEAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiuGzTosd1e2ywaG95n9sq2imNRZ0pKbfEz5W37LHGaSfluriLrw7OTv6VTTWmcx5H4lq3DuvXLQzD1+XNDU0xzzXO2WPFnr4e60fLOhV8IQlOL1XPz0R3kUh6e/Tz1VuwH7BRQx+7cdZe+ME/06m4vXmdQjxqZMFp9cNpeywde+0cxIPtvC/KbX0jU3FJvFTvitP6YCDOub1Pf1Co0oxUf2zuCL9wAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABRXLZ1OJV1e6y8qU+7IPLN+3P6dXtLOlm9v2Dn/lfXdPwf1UTef0vXFIwXndYHonSkuqaPnYoan1I7Ca6xyG1dY9ccFLv6emVieFLXtQxTDxTD63+cWqxSKwlundFftMcqzn3yanwG8+H3cTBtv/Zg2vmYcZat6l1qXX1Swxn74AWnNm2nW+5fdGpBn9RgRsx1rkd3Tym4BrGyrk9q4W9EHY6YOhru7GLxCwcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABDFZZsWvXXUTnutntdxduaYPbe3qOce/s86PbB2rmOOrd3SlHNHtbBUyNz0A0lwCnFW1OfUE6neFbGdfq5+Xo+Pq3Z6crnlbAEvhocNfR9LIt12XJVTk83r9HjtfBp8nwazaXDKa39e59vWREuG+qqzJf6Ufezaup5b6unxtevtnP2BU6ZQE89XfUW/bnu/fkbaB8W92NDrKos08+qGvo/lji696CwVg9LXc6NGWBr5eLCz3y78wgEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEARHHZ1uE0ThaCt5dfeaKoKVjRc4dTTu7+k5vB25LPHLcT4TeuLevc/P364MWePTZshtfweDUrUx1n/Kxdc9A6UHQWpo6razAm4tClvn7ZpU/q8Z6oHUn1spLqWvie+Qtf0Pvxdxftk24vh9ctDaad98R+5z4GtnrIDaftsUlJr6t2QV/r+nl7vLOkj52KUpqhqIXxatNy/Tl7/sip15u9bxJUlzTe4U8XfuEAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiuGzTolWab+ZkYdbF9vHVVZ0quXmNjuHlTXus2NfHPvNM+3ZNH9NzCwM5nPQW7XOun9PHHjZFGqeT6j3R2dxJV7SD6C3qubP32Oteu0Hfp8Zpe253SZ+UtzW9+po3qeipE/WOdTKmh039di91suCvpqoVhHe9Sk5q/Khhj81/UW/Vv3a9/WavbOoL5q17XA3/nOiLlh6qdcFOWgyUt9RrO+dUSYPSxMeDnaXr8wsHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABDFZVuHIzkp4yqH3svNb5xx8u/n06Bahu1jn7bHenvSXW21rlo2DGbS4K3pR85W65OSHi8M7WNXV5Pgbdorm0lwnUTjrL6Wo5pXfGQPFZ16hpFYl9d2o9Bz6k72hp9zZ5+41ut6XelEH7uynga3qFA1UUOnniXV3RySmqh3GdWdWq3NsM+IXLmlr9dgVj33ztyZwLnOI/8wfuEAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKK4Iutwus9qyfEDb7Cbx3z7+/9Uzn3/S54tx0+8ZF9wHYXqWePl17f3O/VDoudN16nxqYo6i8GMnJpUnLoTVZNQXQmf612vJLPn9kQtzPa61sJrVhpnkuC+RuNasqu+M5n4+unVlaj+QaouZHtdXb2uJMuCa57U9fJ6MXk9pCaid0zJqXkai146Xs1c+6A+5+aJLKx3VT4+ZY/Vz4t10Q8HAPB4QsABAERBwAEAREHAAQBEQcABAERBwAEARHFFpkXPf0jk/iVJcuybm+bYr/7YP5FzB88pBKeeelt8l7fCt1ovOinXFbHVutrm3zunYl+/7nAq/NilvpN+LFJ5U2fL/GEzPO156LRzqKot85201cqGSD92HiCVipsrjMNTiNsH7GNPP6ivl5dmrlLYJ84nWFYKT8fuLerxubvtPhPdxUJwi4GBk0bulU9MiuFlCvPinFZvss9p3N/Zbxd+4QAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAorgi63AWP3FBjq+82K7TmXvnipzbuXmfPvbX2TF+//t00crqTXVzbFLWufu1VV0LsXWoELytvaob8LZ499TEluhbR/T3pamTouXCkr5eU2K7/Y2jcqrbYmBSSYKpuhKvPsOrwaiLFhWqpUKucUbUcXl1JSO9rkzUlYx0SV1SboXXFpXa4TVk46qeWxetNbp7vXYg+r08Euvy3sut/YWg98SY9gQAgMcTAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgiisyLfqBl+6V4wffa+dp3vn/OyDn7vsLndJYP2XH+JPf0JBzayKV0kvxHDrpo6qNgGyp4KTjqjTeXKnjbF2/Jw1Kxc0NRMuG8qZel0pNbZzWc4tDvS7V7mHuHnt7+Nya2CJebZe//boL+kbWNuz5rav0XPX8ec/P2EkTH1fSoLR5ryVD6qRjl7tOG4qGfeyiUw7Qmw8vQ5iUnD4mYtlFp6VHKh6hdBw29uX4hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiOKKrMPxtvdu77P3Q5/5os6B7ywFL8vdXn4k8v6ra1nwFu/b4+JJ8I7dPpAGtQjIdZeduiVRZ7HptAmYu8ue2zqkX7d5wp47mEmD63+2jy2uyfr1+jtgdc0e6zvrql3Q96KzR9T4fFHP3by6EFQ/lpuIOhuvNkTV2Tx0bHusfl7XLbUPFMLXVQ5/r4+duqSZB/S6O0v2uscFfb28ujnzuH2nNuhL+IUDAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjiiqzDUfUsuYN/smqO3fet83LuwuedmoOSHeOLvSRYf1GfU0H0u3note119xb0sSuit0x/YXfrUjUv83fqeoTWoUJwTcrWEdUPR89NEn3O6nqq+h9vblZMg+9xbtgMP7bqTaR6GuWaJ/R97Ir6oElVTk0qG/a6to7o79t15xlRPWAGs2nwunrO9Ro5tUe9PfaxZ+9OgmvqVD1VYeC9J77093b0twAA2CUCDgAgCgIOACAKAg4AIAoCDgAgCgIOACCKKzMtuqbHzz9jPnj77v6ckw65lQWlpW4T2aPFrp7qrVulvXptEwpD+5yyVJ/TuBbeSkJtw/7QuuyxUV2vq9wK2/I+N3ZSdVVKduuwXtfsfZPg1gYzK3pdw+mwe5yblMO28d9Jmq+63l56u3pPldt6XROnpcdItKEot51WEMtpcDq2l3Jd3rLH0olzH8WzW1QlDINkR/iFAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCI4oqsw1FbqefGokZj6qQ+drEfXhfg1W8s3jEyx04/SxcNTB/Tx1a1Ej2n9UFJtFVId7ZreVC9TOOcLvDYOGp/n5q536mT2Ge/bqmTBG9bnxtOpcEtKialNKh2KNd36jeUsVMrM67bY7XzTssF5/lS10TVs3jXJHPqbDylThZcuzYRdUu1Vee5duqtVAsC1X4gV7U7syRDUXc0Huzs2eIXDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIIorMi266GylrVI8CwOd4lkWqZK51hE7fXDmPj1380gpOJ3W216+K9JLvRTPUtc+dnevTpdsnnCu10GRntwOT4kdi7TU3KQU/vx09+rxqQ37nEeNNLi1hnefskJ4OrfX3qK6ItpI7NfnNHNMpwFvXm0vfPoBPXfriD23srm7VHC1lX/prH6uC+Je9Wf1jZo6odfV2WePzd6jr1d7fyHofZ6Od1b/wC8cAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUV2QdjrcduqpXyFInB96pO6ms2WP9OWeLb5Hq7tUUFJ36oSQR295vOVv5LxWCt8z3WjKoVgC9Pfp6Nc6G1wfVxZb63SWnXUNXDicj0Z6gMApv11BuOW0AnHNWbTtah9Pg++TViHX26u+9NVHjM5gJvxfes1fs6/FxLawFhdcOxGub0J/T4wt32gVVazfqgzdOhz334z7tCQAAjyMEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBRXZB3OwMtjv8POYz/7TJ1vvvdT+thbh+0Y375KF2Fc8wE7ef/+b9HrOvjmDTnen99jjg2nvRoMO3c/K+i547JTr5CFjeVG9bC6kdxElCuU2npusacXNpgVPX6cfkr9eXW9wvsD5bqirqlxyqnFEj1vvJ5HbadfTmEoxtPwZ0DVae2kVkv1D/KUuuE1PLVVfexRvRBcE6V+gqjnp+DULO3g8AAA/N0h4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiuCLTovd+Wqcfn32qnRN7wzt0TuLd37Mgx+e+YKdilrr6dmwdsudO3adTKc98w6Icn4iXLjrb7WfFNCgtNVfZdNKAxfbzzQcnOlV3XyH4nIZq2/tJ+PXYnl6xx4qifYWXmuqliU/Kerwg0nxbR/Q5zd9lX5TNq/T3Wjdt+mAatJ1+bti053aWnRYVF8JT1BsXJsEtPSrOe0Klr+fGFXu8tuqk7Ivnvrpuzx277U8ewi8cAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUV2Qdzrii42xhZOeib92gexvM3q1fu33IPvb0MWcLeFE3UHNqBrwWA2pL/YIuW0oG4tiFoZ47cdoTFMW25929+j6qWptRU6+rumZfj75oL5ArODUJaVXVLYXX8Hjb5VdEHYXXYkDV2eRWn2DPXfycM/dmfR8X7rTntw7ouWXRSqIwdOp/Duh7od6v69cVgp+vrtMWYSyegdzcPeJ6HSoEPyOqjmuyszIcfuEAAOIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiuCLToldv0XF2SmyXvn7Ubl2Qa12v84Cv/W07ZfG+b9PHXv5ze93t/U6645bOW8zUbvzeUyLm+inVerxx1l73qOGkj6rWCF6LgTTwuPmaz+nxnugU4W1Nn3bssf5cGtza4KHXtse6e/TzNXNvFrQVf66qO37I9gbFnp6r7vNYpKfvqPXBlD2/shHewqIqUqZ3kjq/cdS+XvVz+tg9kZJdFa0N1Pvly/ELBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQxRVZh1MSW5bnxpXw7fanv1h2aoDssYX/rnPk12+w19Xfrxd2y384L8fv/MkFc+yaX9dJ9meeUQ2ukyiJuhKv1qEw0ter2LXnTuwlbxvX7LlTJ51t/pf09aqu22OTkp47mLHHZo7p4iKvHqa+MgmqhXFreJb19Vq4w7meog2F2jLfOyf1fsotf0L3e7jwNaXg53oi1u3Vl6l6mNzwefbNWPr+E3LunT93nTl21U/aBWajST/5XOLjFw4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACCKKzIt2k3V7dpphxNnu/3+vE5pLAzC55a37LGZ+/WtPP7NB+T4no/Y57x2YxLM225//SY9f8+n7bTWla/R7Rzqok3AqKZftylSn72051JXH7vYs4/dPpwGtwHYuE5/f5y9V6dNt/cVgq7H9tz99rqbx/Xczav1uhtnxL3YlwaXOJQ39dzuon6+VOpz5nyydr/Wfkiu/lW9rvM/pB+wq3/Irvu474eeIOcu/lf7Wt/3fYfNsXGvlyT/IXHxCwcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEMUVWYdTbum6gA1Rd7L4GT3X216+t9cemzqlj91btI/d9bbEXws/tqfg1CYplQ39uv05+ztRdc05uDh0/Vx4XcnMA7qeZetQIbhOp+Kck6o7qZ/V59Q6WAh+Rrzno7Ziz+0u67nNB7PgFhWZ85VZtRMZNfXcqlO31Fm263SKot4uN/8RuxDswRck0t7f1D0Z7n/FvDlWFm0kclkaVgdY6OvjPvL3ksfoYx/7WPKSl7wkOXDgQJKmafKBD3zgUePf933ft/2/f/mfF7/4xY/6O6urq8krXvGKZGZmJpmbm0u+//u/P2m1Wo91KQCAS8hjDjjtdjt58pOfnLzlLW8x/04eYE6fPv3In/e9732PGs+DzR133JH8yZ/8SfIHf/AH20Hs9ttvDzsDAMDl+a/Ubr311u0/SrVaTfbt23fRsTvvvDP58Ic/nHziE59Inva0p23/b7/4i7+Y3HbbbcnP/MzPbP9yAgBcfr4iSQN/9md/liwtLSU33nhj8upXvzpZWVl5ZOzjH//49r9GezjY5F74whcmhUIh+au/+quLHq/f7yebm5uP+gMAuMIDTv6v09797ncnH/nIR5I3velNyUc/+tHtX0Tj8UP9wc+cObMdjL5cqVRKFhYWtscu5o1vfGMyOzv7yJ/Dh+1N5AAAV0iW2nd8x3c88v8/8YlPTJ70pCclR48e3f7V84IXOOkXhte97nXJa1/72kf+Of+FQ9ABgEvLV7wO59prr0327NmT3HPPPdv/nP+3nXPnHr1v/Gg02s5cs/67T/7fhPKMti//AwC4tHzF63BOnDix/d9w9u/fv/3Pz3rWs5L19fXkU5/6VPLUpz51+3/70z/902QymSTPeMYzkhg6osYiN3enXRew8iQ9d/p+XVNQbtnzh400uKYg0S+bJLqkIBnXwvp+eHVNLae/i9ejZV30eFG1H7lUDI+r4b2HOkte8YceHkyH1zwpFae+zKulUc/X+O9tyLkHf+Khf2V+Mcd+QteNLP6eerCT5MLT5oPvo7rW5afooqfp9+l1nXrZtDl2za/pda08wX7D1S7IqcnGNcXg/kG9PWlwHWGpYx83HXgfQF86RvIY5fUyD/9ayd1///3Jpz/96e3/BpP/+bf/9t8mL3vZy7Z/rdx7773Jj/zIjyTXXXdd8qIXvWj77998883b/53nla98ZfIrv/IryXA4TF7zmtds/6s4MtQA4PL1mP+V2ic/+cnka7/2a7f/5PL/tpL//z/+4z+eFIvF5LOf/WzyTd/0TckNN9ywXdCZ/4r58z//8+1/Lfaw97znPclNN920/d908nToZz/72cnb3va2v9szAwA8rjzmXzjPe97zkiyzfz798R//sXuM/JfQe9/73sf60gCASxibdwIAoiDgAACiIOAAAKK4ItsTeCnEw6adGlhZ13M7Fy8lesTi5+300ZWv0emOUyfshU+cO9m6So8v3GGnJ6/dpL+XTJ0O35q+u0cfu9wKS3vePvZe+7Wrq+GtDbznp7yp/0ImUk+Hzpb5zRP2fdo4qq/lzP06BX3tJntdC79jpwDn2v/xrDl2zXf97dZWFzP8rZpOT37jlDk2rlbk3P68fU7T/2lWzr3vFfrZnf8ze2z9ejk16c/ZY7POfep8m05RX/qek+bYF37pOjn35tddfLeX3LHvvdocG/d3lurPLxwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBRppjZGe5zKG7DlnT9vedUbkmJV5/CH0Nva67nFrh6f6J3apbLYHrzrbDs+d49d/5NbfYJdA1RZC79efl2Jfvz6c/Z5pfqUkmLfPvbQaRGgtmLvLTitDdp6XaOGPVZd1ddjIK5H/Zye29nnnLNat1NmURDb02cFPTmdZMG1NKVe+Put6rS36DvtHMpb4pyLzjmPxdzUuU89vW7VEsRrB9I6aP8GqZ+3X3c86CWf+Y3XJxsbG7JfGb9wAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAUVyR7QlSnRmY9PbYY9PHwlM4c4WBPTau63WNq/axx052eLmlT7rUttOiq+vOOYs04c514oSTJDn4Z0M5fvc/s/Nar3/nSM49/2Q7/3jkXa9Ne2w0pa/H9IN6fPMb7J4LS/+7zvW++0fshR98s74e/XndYqAwyoKf69oFeyzTXTfclGvVokKtOTcR6cnjmn7hsVPCUBWP7sj5ZM3KItW7vbv09oroXuCVEqh7oea6x/0SfuEAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKK4IutwPOUte6y716lH8LY8F/UMMw/oZPZz32L3Prjuf35Azm399oIcP/Sv7SKgs8+clXNVTcvyn+pHbOV1ouAlSZKbX3G/OXbfr10l5xb/uz3WuaEv5x75oL1Xf+8f6cKSud/T3+Nan7C3b7/rlfoZuPlHz5hj53+pIuceeL0+9v3fbN/nuS/qOq7Na+xznv+Cft316/T1LIpSrkklDW5PUDkX3hrDqz0pinYN3udI85S+1lvX6udrRtQKbl6t56oWBO2Doqap7xRTfQm/cAAAURBwAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAUVyRdTgjp++MqqXp7Nf55tkuQnhvVk+ufqppjp35zlvk3OF/0eveeF5YLUOue3PPHDv4MV1TcPcpXeOz93r7xZt/Yl+P3Lhqj9X+UtesHH+JPbn0UTk1Ofe0LLjnyL4/1/fp2HccNMfqv6tf9/5/oo89/wX7Xl14ip578KN2c5jTz9QfM3s+p5+RzpL9vhi/cE3OPfKT9rpP/IScmhz+5jvl+PlXPdMcG03p6zV77yS4Lmn6Pn2f2/vs61Vdy4J7bqk+O2Pd9uoR/MIBAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEcUWmRZdbOjVwIjJmS3aHgG1ZKXzcm1sYhW2VnivamctuCrGXSln8hN2f4Mwz9OsufEof+9TzZoPPudS1jz2Y1mmrJXG9yp3wFhS52gWRdr9cCF7XqKFft/lgFpx+XLe7ImzbPGw/vIWhXtdgSh+7JK536ffn5Nxz4vkbfVqva/X77LRn7z6rFOLcsJkGvRdzmc6alu8L9bq5yqZ9rUd1e+7EeS8+jF84AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAorsg6nIKTM67yzb0aHrW99/Zri228J2Vnbt8e6+0Jr/3IZSV7/mDGOfaqfezuXj13XEvD64f01KQ/lwbVG3j1Cl5NU9Gp1dpNjYV6Bia7rN9Q9WeVdX29tq6xx+bu0nO7S05tiHjtiXhuvdYatfNyajKYc9a1mQTrz4pncz3ZVZ1XUXzG9J/alnMP/oxd7HfXK+2CqUlXFAl+GX7hAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAorgi06IzJ8yOGvZY3Ukvbh3Wx26cToKpVN3lvxb5skmSHPsufex9f2Tnj6b/9Jyc2/xxO12y+zqdg37g9Tqd8tjL9phjUyecVO9UpDbry5X07JdNCmMnpdppfaDaTJTa+tgq/X3+Ln2tLzxZP/h7P23PX71J51RP358Fpz2XWklwqUFhpK9XZSsJblFRGOp1qdfu7NfHrq7aY2O728e2yoY+5+mT9n0c1XQviPNPs49dOyNaKvR2Fkr4hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiOKKrMNR9Sy5sqgL6Owt7Gpr+lLXznMfTjlbrYu7NZx2bqW3lbp46faH9smpnaeLw/65ftlTL0yC2zlkzimreoasoK9144yosVjWz0CpG15XMqrrufXzYl1Lel3N4/rYa9fbtTZV0YLCa2FR3tJzkyy8nYhXS6OeAdXqYfvYs3q8thpWZ5ObPmGf1Lmv0/dx/ou63urc15bC688W7OtZXbHnjZ3jPoxfOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKK7IOpxMt/ZIqmsTc2zzah2jG+d0UUFnn6hX8PqC1O1jj+pOb4++Hk/Hk+DaEJXbX3PqN7au1utqHrfn9+fS4L5HXq+TxL4cycjpV6JqeHK9vaK/iy6xkOtWtTC51Dl2+3q76Gn+Q/pNc+IZdpOfG395oHuwfF1Tjk/Kog+Lcy9ah+x1Hf1t3Yvp9Gt0ccn02+3GWVuH9Udrf8Z+OGsr+j62l/W9mDptP39tp0/PjOhrtH6DPW/Sc4qpvoRfOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCiuyLRoj9ry3NsS31Ndy4LbJkzfb4+19+nvDjP3OunaItWyoLNHZdsEL4XY2yK+JNItB2N9vYoiDXhS0a87aojU5aevy7n1d+k03wuincPVv+e1ASgGbS2fGz1nQ47f8hp7//kH3rIo5x79haq9rr36IVh9is7XnjpWCk5Br66VgttMdM7pN3vjHvt6HX/xHjn3xl+z78X5Z8zLuV5LhnLLviZV/ehK1XWRnu6UXTyMXzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCjSLMt2tq/048jm5mYyOzub3PKqNyTFqlPocRGFoT7lsdjqv9R25ladLeLVtvdTcqqsOegs6detbOl1Z2nY9dg2CW8FUew5hy7bY6Vu+L1QrQtyqTh0Ogpf826PLbfjd25T3WmdsX6TPbb3v4mbvF0DZF/Q7h69sPm7dR3O2a+3jz17t5ya9MRrN8Q2/rm+U9ekVL22HNfYx569exLcriG3KY5dbsupyUS8X1U93rjfS+76+f8t2djYSGZmZuxj6JcHAODvBgEHABAFAQcAEAUBBwAQBQEHABAFAQcAEMUV2Z6g1NXj7YN2SuOCsx36wHntwUwanLaqtp+vX3BSXm/U69rzaXv+yiE9d+Hz9twLT9EpnEuf0CmgK0+0vxNNidfNbV5lv3Zlw0mJnRfptGf13JGTnzyYtcdmHtDXY+OofT2qq+FtJHLFnth+vqLPSbXW8FJxe7Nejro91Dyl88hbR+yTrm7pa711jc7pr6wlwWn3JXFNhlP6Wlc39fNX6tjzRw29rrkv2tfk/NfZ8yY9fS0fxi8cAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUV2QdjpeLXl1T29rrHPjOvjS4xUDXaTGQiJceV/XUqZP62P05e6yy4dSVTNtjs3fpdZ0TW8/nrvlAxxy7+7v1Sc9+vhC0bX2ue8iu7zjwX4dy7n3/RK/rhh+7wxy76989Qc69/j1b5tgXv1+36tj7X/XbvSrqSjpL+j7VRB3YhRfpHhQ3/euzcnztCUfMsanPnpJzmwftuevX6TqbulNzp+qDeov6+WqesOtWzt6qq/lues09crz17oPm2IE3VeTcB19k90hZ+Jx9PcaDNHkg8fELBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEEWaZZmT+/f4s7m5mczOzia3vOoNSbGqU0FDjMUhq6v6cnmpzVOn7PntA3runs/ZqbonXqC/Oyx9Qg4nm1fZ8wtjPTcVO8TXz2fB7RoeGrfH9vyN3pp+9WY7DXjh83ru6b9np8wuOy0V1m4sBm9NX113Wi5ca1+v2gU5NZmU9XjriH1eR/5YX69Tz7EPvvfT+nqtOtdLPUMb18upyezd9tjEabngqa7b59Xdq9+PE3HKNeczpnXY+Yw5KdKXa3pu+g9XzLF9/8p+BkbjfvKRe96cbGxsJDMz9puWXzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCiuyPYEqrYjN33czmNvHdJ57F7Nyqhuj5VFfUZu9Rb7djWP67ntbxN7z+e1Jb9sb0t+4vm6gOO6d9jby5/4pn3B27Tn5l562hyr/hd9I9svs7diP/Ie+7i54TfZW7xPf3FTzl0/uijHC0P7+eot6OdL1doUe7p+o22f0rb5O+zX3rhGPwOVdXvuqKrPqai7PSSDWXv+of9HTz7xfPs9c91vbci5539SH7v4Gwvm2GBWTk2mTmRB55ubuV/f560j9vzGWT139Cf2s3vyH9nzxv1ekuiuCdv4hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiOKKrMPx8v5V35DRlM5jb5zSOfSjKXu84Kyr2A2r7die+/tzcvzUs+2xmXv1uk7+T/uCz2ns9CS58Cd28Uj2jfrYS39i1/ic+Lar5dy5z9pjx1+i62wmT9N1Ovt/1m6Gcuxf6Pt47X+0xzePTul12WVJ26bO2dfr1N/XPWsO/rl9o888XdfwzN+la7F68/b34o1r9bFn7rOv1+nn6GKZ0cflcJLO2MfOnFY75Y49t7NPT66f089IYWTP7+5x6rxEL562mDvuycP+7dp29tcAANgdAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgiisyLbp23tnG/YCd/rf4OZ3C6bUvUOmStVV97M1r7O8HjVNyatJb1Osqb4m5e/Sxmw/a6946or/TVHQGcVIU6ZZDnQWcDGbsc05Hem46sp+RYtdJLf2jaTl+9un2/KmP6XWdeo49ljnv5uljenxLPLvN4/qc10V6cv2cft3Nq/Qzotp2lNr6vdyft9ddXddzk1Sfc0+kCdfP60N3luxzrupOIklv0XlPbdjnNf2g7p9y6hvs9PcjH+6bY6NRP7kv8fELBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQxRVZhzOu6vGCSFXv7i0EtxDI9eftsVEtvFZmKGpOcrULuuZg81p77ND/M5BzH7jV3vf+uvduyLn3fduMHG+cFoNpeA1PoZQF1y3N3a1rGVZv1lv5L3zBnr9xjZ6rztmtL3NqxJoP2mMDfZuSTCx75Lzf1Jb4ucpL7UKe+Zevyrl3/YLdhuLgL+hirHNPa8rx7jPsAqFDP62fkft/RNS7/JJ+BlqH9AVtHbY/o4ZN/flVO28/I2s32O/z8WCSJB9NXPzCAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEARHFlpkXX0+DtvcdVJxc3CU8BVdvp58qtLLj9QMVZdmUzLB0yN33MnnvyBbNy7sIduiXD2k322PwXnK3p5+zvU4WBnjucssdHzvOTOV/jevP2X6it6HV1l+zXnujbJNPqc+viWu/5tL5PK0+w1zV7r37dsbPu/u8vmWNrL94r51a/YF/r9Rv0Oa3frMf3/W7DHDv1XP0QNEQbijPPkFOTgtNao7xlP0ND59mdlMLaRBR05cTf/r2d/TUAAHaHgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjiiqzDSXSpQ5KK9PtJOdlVG4DhdBpcvxHaUmEndTrTx+2Tbh10agrO2nNHjUJwrUyu2N9F/YZoBVE/q+dW1+3rtXVEX8u5e5zaohvtc569Vz8/VVHH1fkHLTn3yE/L4eS+b7G34y+M9Lqu/R27DcXd361rsea+oK/nYNYer63pddVFy4aN6/Wzt+dTTkuPa9KgWj6vdYZ65nPDpldLY4+P9a2QBuKzazzYWX0iv3AAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABREHAAAFFckXU4qudDLnVqWnaTI58V7bHqus7dbx+wjz13t679OPf1cjipn0uD1pzrLdrfWyqb+py8/kKFoT2WOan/pU7Ycb3xPZ/RhRIPvlAXCO3/S7uhyann6O+AzWOihuf3puTcjevlcFI/a1/QYV2v675vmQs6bm4wG97fZeMava76OXtu6jwDMw/o+3zh6+z7PHVKn/P6jfZYdSUNruHJlTv2OffnvJ5b9lg6yYLGvhy/cAAAURBwAABREHAAAFEQcAAAURBwAABREHAAAFFckWnRBTsrdduokQanVNdFCqeXlui1PqjYO8AnvQUnnfZBfexRPQtO1x7MpEGpybmhvSO+u738ULyu12Zi4KSHpuIZ2Tiq055LHX3sccW+V4WhnjslWkFsXKufgamTTuqqGB7V9dTKZhqcgu625RCXpLLlzBXHLjrr2jpSleNTJ0Qa+ZS+1o3TafC19lKQu3vsY0+d0eUT7X32BauL1ivjIWnRAIDHEQIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgiiuyDkfVG3jb8Xt5/14Ovaq1aZ7UOfLnv87+frBwRxacX58bzdu5+7P36XVtXmOPzTyg53b36t4Hqu7JrYkSNTy9BV3vMhH3ceqsvtadZefY4pTrZ5xt7Y+mQee7k5on1ZZDbXmf64vnZ1zTr1sT9R25zr7wZ7OrWmc49WVjpy5OP5v6Pg6nw+rHcvXV8HsxmNbrKonWB50l+1qO+zv77cIvHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBRXZFp0zUkr3LrKTh2cu1vnLG4d1jG897X2fv0H/4veL71ye8se+90ZOXfQ1Dmx/X+waY7NfFhOTU4/f8ocmzrR02mrC/bc3FCkcaprmTv8v7fNsQdeulfOvfq3TppjX3zVATl3+ZMivzhJkjPPEmm+X9TP5uS5do+K5X+pr8fn/7c9cvyGV37KHDvzL54l51Y2RXuLDX1O1e87I8f3/v/sHPWVN+heIws/P2uOtQ7ovOfhlNPCQpzWkW+5T87t/uh+c+z0Dw/k3MbHdXuM7qKdhz6u6XPqLtsntfQJ+7NvNNTP/MP4hQMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiCLNsszZrP/xZ3NzM5mdnU1uedUbkmLV2fv8MebPe9uOF3SKfDLSZSVJQ2xtv3VE58irGiC1dXiufVCf9IG/sPPozz5dtxCYvdse6+3R5zTWJQVJcRC+rf36TfbY3F36dXuL9rqrTh3XYFafc7Fvz+8u6bkzYjv+tZvTXdUHXXii/eA3TulzLooSss1r9Lqax/Wxe3vFfGcrf1Uf1Nmv17X0KV3jc+6ppeB6vZUn2q+99Ek998KT9Xt9+lhYq4fc3s/YN/LE8+3PgUmvlzzw+h9NNjY2kpkZuyaQXzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAorsj2BCMnk7q2koWlaCZJkqXhr53p7ONkVLcPPnHSi6/6cF+OH/+HVXNs9h597I3r7bH6WT03dXY1n3nATk098yx9wfb9pX3w08/Uc5c/Yc9deaKeOynqNN89n7PHN653cvbF81Vu6YdvVNXfLwcz9msvf0KnCJ96Tim45UL7gF731GlxvY7KqUnZ7uiRVNb13PZ+fZ8rdqeIpLegr3VddGTYvFq/bnVVDieTsn29CroDStJetu9jddW+T+P+zn678AsHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABDFFVmHU9nSdQGqpqUk8vp30p5gXBXb3q95c+2xqdN6S/Mzz9DFR/Xz4XVL5S17rNTd3Vb+nT12TUKpred25+25tQvO6+61v4sVdElTsvxZXbNy+ln22+7QR/Tck88VLQRO63VtXKu/X859wb5XZ55elnPnP28/f1tH9Os2zjjPyLR9rxqinsV7z0zE2Pa404pkUg5vY1IWLSrU+eZKnSy4/Yq6HtvH7ol1zdnzJmLel+MXDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgiiuyDmdUc/LcRe3I2K1J8Wp80uBeOok4tFfrMHVyFz1JnBT7Utse6yzrk2qe0Afv7LPn18+G11MVnToJ1Xcm1SVPSWufflsVhvbB+6J2KFfqpOH1ZaIGzKuJqp/Tx167yZ67/EmvLim870zTea7XbrLH9v21bsZ0/sml4Pqz6oZ+SFqH7Pfr9HE913uvq3ulaodymTh0ZV31w/E+vB7CLxwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUV2RadKazMJNRY2cpfhflTB1O7SLF82Z7bPZuPbe34CwsDW8xoNK5Sz39uuOKsyyRIdrbo4+t0qYnThuJhkgtbR/Qc2fv19drS6SmjpzU5XQcvvW8p37eXndvUa9rRpxza79+w5Vb+tiHPmLnH3/xdn3Shz9of6e+8ET98eelv8/fbad7H3+pnnz9O+y593yHPqe5O5x2DnMqvV1OTXoL4jeIetmddSfgFw4AIA4CDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIIorsg6n2Nfj6TgLai+QG3t1FCJfvS+2h8/Vzttjw6acmkyf0Fuxn3ua/d1j9j6dZL/yNfa6p5z2A5nzBBaG9tjEmVtp2a/dXXK26m/a47ULeu7WIec+rthjWVFfr9GUPV44I6cmlQ2nFYRoJbH4+aHeyv8p5eDWBl4Nx8b1dtFU7bj+ztyfyYLqjrbnzuv72J+xX7uwqde1cdS+XkXR7iNXdsYHs/ZY7YJTwzOdBrWJGHvtPr6EXzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAorsi06FFdjxcHafAW8OUtnXZYGIa3PlApxFUnxbN1UG8RP31MbE3vpIcWu/Z4mul1tffpY+/5nJ3OfeYZ+vvSQKQvjxp6Xaolw+ZROTVZ/JxzzvvtdY9rThsAkaLutd3o7Heu9Wfsa33yuaXgdQ1Finlu6rSTnjxrX69SLwlO8/W+bqu2G9vjBZE6f05PHornr3HGSdmfCU+bHtf03N5ee6x53F5zOthZfwJ+4QAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAorgi63CKg/A6ndqqU2NxQOfQz909McfWbtTxf6+ok7jwRF2EMXUyC2+rsLMU+4sqt/XkdKKvV2u/fV5zd+lj9/bax565X8/dvNa+FzP36LnDuj4n1VZh+kH7+citX2evq7biXGvdYSDpzReCW3qUuuK4i0lwu4bcaDGsfUWuOLSvScepAZu9R9+L7p5C8OdEKrqFjJ36n1LH+Qw6aB+guqoPXl2zx/oLoj6xv7P6Qn7hAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiuCLrcNKRHi+MAutV8l4V14uChLzO4k/s3P7Vf6zz66d/7gFzbOvgE+Rcr89KJr56eH1WVM1Bb05/p6lshH8l6i7rcypv2WMjp1amsm6PTSpy6q76v6zdUAiupxpNOX2LnJqVVJSdFESPqNxwyl5Xwal768851+uUvbD2Qa+pTVgtzE7Wpd4zsg9PkiTVDXEfnZ41iXu9xPtR1NJ4NYqq/od+OACAxxUCDgAgCgIOACAKAg4AIAoCDgAgCgIOACCKKzMtOtMpfCORQtw8obcsH31W9DZIkuTUN9hjy+/T+dpf/NGbg7fqHzZ0OmRvyZ4/+0U5Neku2cdunPHaIuhjd/bbxy63wlsjZM5u6mor9sa5bFfptGORVl1d1evavM4e2/Npnee7eovOb69s2c92baUQnApebunr1Toih5P5u+3zWrtZX+v5u+y5/YXdffyp52vlqfpezP+xPbbyVP26+z6m70VnyR7PnFOunrXPaetq2hMAAC4RBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUV2QdTrGXBG/R7W1rX2rrY9fW7FqHc1+n6ySu+aC98Ptfogta5u7U65oS9TLnnq7nXv3Bvjl23/fp63Xgg/oR7Oy3r8n8XXq//VPPsY89fUxOTfrz9vVonNVzJ2U9XrQvV9Jb1HOXPiXaW9ykn5/5u3QN2alvsO/V7Bf1fVRb/Q9m9dxD/4/uX3D/S+3vxTe+Uxdj3f3yKXNs8dO6Pqh9YGe1JRdd19v0B8Fdr7LXdcOv6w+oY/+TPdf7DJp+UD8DG0fta91fsm/ypOv0evgSfuEAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiuCLTor00zfp5O12yJ7at3+YMD0VadbGnJ68frZljtfN67qih15WJjNrp+/Sx1260U7KnPq/ntg6Ep7B39urHt7xpv/bEefJr58XrLutz8loytA7Z82urztx99o2qrThzD+jvl/N32PO7e+VU2Sqi6pzTyhNEv4b8+bvXHrvwlKac2zglttSv6XUVnEzf8qY9/8LTZuTc5j32ujau02/WYtcpzeiGtV7JFUSG+r6P2XNHwzR5UB75S8ffwd8BAGDXCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAorsg6HK8GYzBj55sX9I74ydguldlWHIZtW58bNlUNj5470juaJ5VN9bp6bjoJq1XIdZ2aluljWXA9VbaLr1PqnBK9w3syKet1qXqZ7pKeu/h5uzjk7NfrE164Qy985Wvs1567S9/HvqhP8+5DdS28bslrBzKqi9dd13Pd+1wR9VRnJ8HnVL4v29XPBNUqotTVx86Kom6pKura0p21cuAXDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgiiuyDkf1i8il4yy8341TgzEWrT/GdluZbY0zdm5/b0F/dyh19LFVTUu5pXP3O/vsubP36XqE4XQavK5iT6+rOrLnpplT65DZc/sLemr9XHjt0dRJPXfzKrsfzoxTv7FxVD8ji5+z79XaTfo+Ldwpns15/brt/XI4aZy2z6t1xKnjesCe2z6g53r3Qr1fW4f0OTcfFLVYznvZq2uaOmMX4qzdUNRzT4lrfVDU6PSpwwEAPI4QcAAAURBwAABREHAAAFEQcAAAURBwAABRXJFp0WrL8lw62VmK32PMpt1WW7fTR7eu0vG/3BEpi4eT4C3xvZYN45o+qeYJlUq5u3Tt0O3hcyPRKqIgUqa91NP6WSdd1rleDZF62hPb/Hv30UsRnvuiTlFfu6kQnCKs0vIH00nw8+Nt5b/nc2IvficNuLy1u1YjvT32uhbvCF9X7YKT7r+q17VxrX3sylZ4awPVmiVz2rY8coyd/TUAAHaHgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjiiqzDmYgWAbnOtQNz7Jr367n3f6fOoZ/5T+vm2NoNy3LusJEG1/+o/Prt+aKmZdTQc4t9e2zs1Dx5NQejRnjbhIlqFRFeauXW2aS63EW2XKiu63MaNu25lY0keO72/E17rD8fXh9UGOu5wyk5LO/VsK6/Mxd7SfBz3Tylb2R3qRjW4iTJz9keX/iCfrOe/XrdYmD2i2E1TV5tkqpLcm7xI/iFAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiOKKTIv20kernyqbY+e+Tuf/7f+wTqW87xXLwdu0t8VW/9U1PXcwo9dd7Iu01qGT85jtov2A85UnE0+ot5W/StdW7Ri2ifHmSZ222jqk01bVvfJSl4dNe6zcklOTgp3t76bWz92jz/nCk+0b2TitX3c4rc+5upoF38eSaOnhbdXfOqjvY8WucEhWb9YLq63YY+efoufO3eW0ihAp7FOnwz8n1Hs5Fe+1L8cvHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFFdkHY7XnqDQT4Pz/sdim39vC3ivxUCxK+YWwmtltoeLatt7Pbm7155bd9oP9MVW/V4tTXGUBZ+T155AnfPWkWLwmnO9RfvFC0M9tySegdS5x5Ny+Hh/Vn83bZxKg99v3vM1KdnHbh3Wx66fz4KfPa/9RSbWlTr1Z4WhqA9ad2qxnLYK6np7dV6DOXts6qS95vHAefi+hF84AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKK7ItOjylk7h6yzbqYPTx/XczWt12mFDbA+u0mW3557JglNPB3Phxx446aOq3YOXJl4Yh6fqphN97FE9/BmQrzuSU5NiVx9bpaYWRNqzu67x7lpU1EQbgFFDzx1X7bH6OaftxgF97Nn77ZYfnf1pcEuGYnl375nmg/a61m/Uc+e/YF+TjaN6bjoOT+fOUj1X3Sv17E12lhXNLxwAQBwEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBRXZh1ORyeNFwdq+3g9d//H9f7yD9xmX/Jrf0cXYdz7HXaxw8KnC8Hb2nt1FiXnenX32HNnHrBrFXJrTr3C3k/b81dv0W0CahfCa4uaJ+zXXXmSnJo0H9DHLrXtsd4efew9n7ULl9Zu1NfDa32g6je6S/qcpo9lwfU/ns5e+9kuODVRqmfDuJYE18zlegv2uqor+ti9efuaNB906pac2qP6eXusu5SEzxXv87Fo6fLl+IUDAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIjiiqzD2Tqs42x13c6Dbx/Qc3uLujHNzD322MZ1dV1XctrOde/PyalJXdSk5DavsY/dOJME1/ioWgWvfmN7XVfZtSW183puKkqAyltyatJbtNc9J3qZ5Aaz+tiJmD51Uh/7/Nfa61q4Q9c8bV6t74Wql6muyamyL03RqQEri7qkXDoO63eTm5S+MnO3xyvhNU8T0VvG64ul+hblunvt+bUVPTcTj4jsh6MfvUfwCwcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABDFFZkWXero8XHFTiusiJTpnYRwtcV3X2xZ7m1bPtbZ2O728ipNWLUu8K7JuKrn9vaGr6u6qe/FUKx7OCOnypRZb1v7ykYWfC+Gzlb+asv8/pzTFsFJTx5OpcFtAOR7ynnLqFTc3KQaniI8KdrnNHCegSTxnk37tSfiMyTXX0hMtXOJ5L2nVJr5sOm8l8V7auFOOz99NBS561+GXzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKP7/7d0NsN1leff7e72vtd+z9072TkgCQSxIQVFERZ6jPOKBOeX4aOGcM522Sqedaik4Ap2OtY+1VkexdE6dtsdiZ9ojPfNUceiRcmCsQ3gfH0AERQQkCAJJSHayk/2+93pf/zP/BYnEh/t37XXvepOQ72cmE8K9/m/3/7/2tVZyXfdFwAEAREHAAQBEcVzW4axFS9QqrEZxwT+W+FfiN5ePt7YtGDUrqm6gMaj3nRVzkl82lkNfMeoV1vnHMh29rVoi3loyXy3FbtWVqGXrUxmxfcaod1EtF3J1FzyXlryoObHqXaz5yFeNCRU1UY1B4/2YCatrW817amUyE9z+YuCFJOh9nsq2w+ua1PNj1fg0Bvw7bjdW992FbzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoSIvu0abt03L8mY+My/G8SAPefMeykebrz4d89v/Quct9e3SqpUqXLBopnipNuF0x0laNjNjCkthUpOKmmmJZ+1xdHzhfC0/FtdJWS7NJ8Hyo5eWb/Xrb8gEjDTgbvqy92lYteb/mfRvp2qrVSMNo52Ddi/4XRWrzsN53dcI/3r/HuCbxXFvPp5U63+oPKzMwsq1/vg/Xg2uvvdadc845bnBw0G3YsMF96EMfcjt27DjiNbVazV1xxRVubGzMDQwMuEsvvdTt27fviNfs3LnTXXzxxa6vr6+7nz/+4z92rZZRgAAAOKb1FHDuvffebjB58MEH3fbt212z2XQXXnihW17++Sfzq6++2t16663upptu6r5+z5497pJLLjk83m63u8Gm0Wi4+++/3/3zP/+zu+GGG9xnPvOZ/9grAwAcVXr6K7XvfOc7R/w5DRTpN5RHHnnEvec973Hz8/Pun/7pn9zXv/519773va/7mq997WvuTW96UzdIvetd73K33367e/LJJ90dd9zhJiYm3FlnneU+//nPu09+8pPus5/9rCsWjbJkAMDxlzSQBpjU6OhL/VLTwJN+63n/+99/+DWnnXaa27p1q3vggQe6f05/P/PMM7vB5pCLLrrILSwsuCeeeOJVj1Ov17vjr/wFADhOAk6n03FXXXWVO++889wZZ5zR/X9TU1PdbygjIyNHvDYNLunYode8MtgcGj805vu3o+Hh4cO/tmzZEnraAIBjLeCk/5bz+OOPuxtvvNH9sn3qU5/qfps69GvXrl2/9GMCAI6CtOgrr7zS3Xbbbe6+++5zmzdvPvz/Jycnu8kAc3NzR3zLSbPU0rFDr3nooYeO2N+hLLZDr/lFpVKp+wsAcJwEnCRJ3Mc//nF38803u3vuucdt27btiPGzzz7bFQoFd+edd3bToVNp2nSaBn3uued2/5z+/oUvfMHt37+/m3CQSjPehoaG3Omnn+6Ods/97+vl+MgOnUPfKvvHd79PF1KoHPoN39fFH4tb9VrrjSP/FvQI/buT4KX860atQ2lO7zsr6mGq4+H1G1Y9QmiNzmrqYXLiVuTEUvypjFHjo1j3Ir8Sfp9U3YlV71KasepOMsHXpNoTVPYZ9UFGvVVNPH95o/1FXlyzdU1WCwtVm1Q36oPU86XuQ1tNdGjASf8aLc1Au+WWW7q1OIf+zSX9d5VKpdL9/fd+7/fcNddc000kSINIGqDSIJNmqKXSNOo0sHz4wx921113XXcfn/70p7v75lsMALx+9RRwrr/++u7v559//hH/P019/p3f+Z3uf3/5y1922Wy2+w0nzS5LM9D+/u///vBrc7lc96/jLr/88m4g6u/vd5dddpn73Oc+9x9zRQCA18dfqVnK5bL7yle+0v3lc+KJJ7pvf/vbvRwaAHCMY/FOAEAUBBwAQBQEHABAFAQcAEAU9MPp0frz9srx5Mcv1Rb59E37O0d0CqKgxTm38m5/v5zij/W2/UY/nKn3+s+rsKAfk7KoKbB6d7T6wmsOKkZ/F1VTUBvVx1X1MKq+J1Wct/rOiJqVYb3vJBPW0yhVMs6r2S967eSN+aqqHj962/o6PV5Y8u+7aCyr2BL9mKw+PFbNU/mg6Idj1PAkoqYl2wx/flKtcia4hqyTD7uNxi0+jG84AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKEiL7tHSv716z55Dqmfq/MDKtD/G9+3Xea35O/u8Y1PvtI6rUy1HHhOPgpHyqFJAO0W9bdFY9r6TX8PS9GLXheXwJfGttGhLuxK+rUoRtpbEX9im56s45x8rz+hnc2mzf1KyRsuF0qxxL0Sab2NQ77tPtCBoDBnpxf63m/ls5lWauHOuJVLQV3QHFFfZr8fVsdVzbb1fVfuKxEjJP4RvOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKKjD6ZG1pHlxUW+f5PzbV9frfWcb/vz68owLrmVItcr+sf694W0AlgbWtjT9wIv+BP9cw6gpEE9301g+XtWOqFqY1dTpqPOylsTviOenNqq3LRnPSGNIDGb0RQ3s8t+n6rjetm60ilDtHvp1txC3MqFqZfS2ZaP9RVPU8ai2CKnCgqin8nch6apO6PHivOojoLfN1QLbE7jV4RsOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCtKie2SlUrZLejzT9icQLm9aQ7qj8dGhdFAnLubEddXGdIpnWyxpXjaOa6UBL2/yX1i7oLctiOXUC4vh7QmsJd6tdg65Rvh8ZMXzkxXXu5r72Lc/Cc57Ve0JMsnans2MWPp+aYuVUi3GRGryajRF+4LyjHFNiX9seZO+psHnjXRt0frAarmgWhDk6v7jZkTJxivxDQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAV1OD1SS6WnViaNmhVRwzHytN53fZ1/rNm/to8WqjakORC+rRpbTXsC1wmvdVCtIFp9meBl2nNGzUFDLFufyjbDak5S7aJ/3x1RD2XVpFj7Xkv9htmuwain6ojzKhjtQFRdU33EeAZE3YlVk6eePauVRHHWranWTz0HVh2huldJLmzslfiGAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgjqcHi2epPPrrboAVTti1bvUR/xjJSN3X9WzpFqVTHBtSCKeouaAMV9LutahOu7fvjaq961qDopGP5xE7Nqq4WlV5LDrn/UfO2/UfixvzAbXu1gyHXFeK/qaO+oniXFeVs3Kams8XvXQzSS4JmVlwqinavnHMi19HwtL/rFOwZpro3YtCbxPxvtR1TQlRr3dIXzDAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEAREFadI9aFZ3uWJoxUjxFiFdph1Z6qLVkuUrDfOnYSfhS/o3w9FC1JL61dH3flN63Ou+OkWqrtrXSxNeyBHx1LBv8DGSN1NScSBG2nr+VCb3v4oLasd7WStXNtMJbRbTLIq3eaOlhtb9Q6cvquFY7kcoBfVyLenbzy3rbfM1/7Op6/7PZrhs3+WV8wwEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEAREEdTo8Gdro1LaVeGwuv71j3lP8F9WGdB1/dEF4fJGssui0ZkuDjqhqeVGU6CZ6vnFjqvy3aMZiMTQvL4fVB7bLed7bpwttIGB8vVU1U1qgRU3KitqN7XOPZ7WTCnx9Z22bcx2a/fkH5oP+6aqKtRioR9WVl0b4iVTXacqhr7hj1evVh/0OSq4rzMuqhDuEbDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIArSontUntPpf8sbdcpiuyjGjNYHk19+0Du2+BvvktvOnhWe1zryuH5MRn5W844deJfeduAZkR+apmK6taQBZ4LSi1/aVoxZy+3njLRVcZuzYin+l3Yelga+mvOuj4a3t8iK1gfWfbKoFhU5/6PX1RoJb+egjmu2BDGyhEsH/WMNIx27tl6Pl2b9Y/kVfWIrE5mg9xPtCQAARxUCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIArqcHp04Cydbz7yE53nXhAzPn+qPvYzX/bX2ow+ps9r7BGjb4JaeXxYb7rn3RXv2MiP9LZ5Y+n6+jr/ddVFjYVV01JYsmpW/MftiFoqq86mOy7OK1sPb3/RKq+h5YJzbug50f5iJBvcdqM4r49r1YZ0Cpmg5yNVEnVzVs2cKRNeT1UfDWtfkeqbMtpfiOfAap8ia8RU7ZpR13Z4H6t7GQAAa0PAAQBEQcABAERBwAEAREHAAQBEQcABAERBWnSP1LL1q1nSXC3V3r9bp0OubArbbyoxUnVXRIpoZZ+R6i3GGkOZ4JTX7rGn/ceuD2eCl4+3Uk9V2nSmvbZ02uaQOq7eNl9NgufaenZV6rPV+iBb8R87Yzx8VquIXD38mlTqc2W/Pq/mYCY4/dhKje8Tqc1WywUrFVzdq7bxflM/R3JVdVJuVfiGAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgjqcHp1wr153/MCZRiFOEl5TMLzDP1ad0Pn12Ybe98jT/gT8pRP0iWXa/rHiorH0fE6fd200vE2AqhswWwyI25zJ6mtqFzPB+7bqXeR+rVos492eZMKfH/XsWvUsxfkk+PnqiFqrVJ+oIbPaOVjPiGolYbVcaJf8x15+g962st8Ft9aw5JfC5joR9+iV+IYDAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCOpweLW/QU1ZY1Ns3B/1jWV3iI9VHdO7+0M/09sVFfyL98la9bafkLwDZdLf+TNPJ6/Oefqu/pqB80Kg3yIT1WLFqR6z+LdZ9bPWJ41p9epaT4OM2RB+eVH457JxTpVlR72JcU3PAqNOZ8+87Meq4VH8qqy6pb69+NqsbMmvoWeO8KlOZ4J5Iqflf8Y9tvF8XzJRm/AVXz/+vFe9Yp7a6+jG+4QAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKIgLbpHVoqnSsW10iHrIy445Xrkab1tdVyf2MqEP3909DFr+ficd2x5Uh+3XZbDbvQJ/7EbIsU8lWskwWmr5YNJ8FL99RGjVUQ7vD2BbAPQr8+rNKfHmwPqvMJTda00cmeMN4b9x84Y2bhqvnJGKm/NeM+oViPWM9IUc735LtEjwDm3cLLOUc8v+S96/9v0j/x1O/zbbrnDf1GtVsM952x8wwEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEAREEdTo+sJd4r0zq3v1Pw5/ZXpvW+Swv+fTcrumbAWl4+XxPblo0l4Ev+scQoZcjrkgNXGzN2EFjzVD5gLD2/3n/c/IoLrrPpbj+fBNWcdCXh19Qp6l1nxH1u+VemX0WNjz4v65qLYr7axfD56uTD2yJYtVzqPZHqm/Lve+rcAX2fjDYU+araWG/bEG05auv8D1C70XHuHmfiGw4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAK0qJ71L/HSKedMPIOO/6h0ozRBkBsu7JRH7dgpB8XRcp1fVTvO6fSMP2dC1b1kUelVZeMtNWWSBXPtvS2KvW5baQIZ4y0aaWwqM+rOZAJvk+lWb3vyj7/+MG36G2HnveP9e3TeeK7368fksaQ/7o2/EDvW6U+7/lfdA+B0Qd1HvnE9/0P/rO/obetnuXP2T/l/9T9Gla26D4Ue87zz+fwM3JTWQKh0urbudWVL/ANBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBXU4Pco1jVqZdia4lsZapr2e8Y8XF1zwcbuS8OXQc3U9J4p1zVl1bKvkqaDGMsFtJtplfdzquLVkvmhRcdC6UX71kUxwDU/3tMTHz3VP6mPPnOEfG3tM19lsvlPX0rxwif9eHHizrncZfM6/7dh39bbND8ieC25qcJ13bORx4z2R+Iu5pt+uC73m36j3PfGQf7xj1MXtf4d/2/yyqNGpre5nAN9wAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABRUIfTo5UN2TXVu2T9bTBcrhHe36XVp49bnNf7Vr0uGsN639lWJrhGJ7+s910b94/lanpb1S+n2a9rUmqilqatyzfMPj1J1r/vlfXGZ0Bx2nljPnJGrYR6dlVNU6pvr//Elk4w6n/y+prXPeI/76xRF1dfJ3oi6bYzLnOnv84mlRfzlYiauVS24z/vVl8meK5TS5v8Y4VlPV9jP/KPLYueW+06/XAAAEcRAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCtKie2SlUlqppw2xhHx9VKcWDr7g33d5Vh93eaP+bNGqhKcf50Wq5fypetvJB/XS9MUl/3nPn6L33S75x/p3620r0/6c1/lTjNR4vaq9m3+jf98bvq+3Lc375+vAm3Xu8sIZ+uE96Sb/81c6qB+Cn10y4B0rzmfWlBqvUoytcoBELMefrKFlRyrbEm0A8karkRH/ePlAeLuPVHMoE9SCItXJh7UKSYwWJof3sbqXAQCwNgQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFNTh9EjVq6Q6BZ1/X1jy59gPP6cLA3J/sM87tvyNjXLbjfevyPFdF/gLGsoHdV1ATpR3nPLfZuW2Oz6qex9Mftd/7JGnw+so9l2ga1Im/95fdzLylD7si+cPBZ9YYUU/A9mGf7xd1kctTOs6nRcv8z8jW/5R92SYeMh/Xnv/k1FftmDUiIlam3w1/BN1YdGoXTPaKuRq/vHigt53+WB4K4jmgFHXVA1rcWLVEap6qUzDrQrfcAAAURBwAABREHAAAFEQcAAAURBwAABREHAAAFGQFt2jopFK2SrrtEOVlrg8YSx7///6U58bY/q4+96u13GvTPuvq9Wn9z37q/6U2PKMThHefIeez33n+OekfZJeMn/zf/M/3hvu0bmnz33QP15Y0vNRntHXpLY/cKZ+S+ZX/OMjT+uU6pX1+vnK7uz3js28KbxcoH/X2j72qvmcOVPP9bb/z5+v2ynoAzeGdCp4dUKkEFf1MzL6VN079rP/TfRUcM5tuluf98qGbFDLjlSm7T/vfFXMdWN1LRX4hgMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiII6nB7V1+n8+qyxTLdqT5Aztl3YJnLkdfcBVxJ1NlYdRUaXd7ihZ/2fWxZO0p9pOrrkwPXv9p93Zqdej3/2VHGv9G10pTn/WH4lWdM1NYb9B8+29Lbq2LVRPdeJcV45tW+jzivnLytxxXk9X1ZtSH3Ef+z+3fq8Dpzhf0aaA/q4lQNGWw5RX1Qf0fve+y7/RY/+UB+3Pqj33RE/1a2fMarFhXo2O6srw+EbDgAgDgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCupwepQY9RtWnruq/6iL+oxUthl+3I5u/+Kybf9Yzcj7VzUY5YM6Qb8xpK+5Nu4fLxu1RapuIDE+aql6KqtWpjlg1Kw0wu5xlyotahn1QcZ5Vddngu+j0hw05qOm951Jwt+PSX4Nz6Zx3qp+KF/V51VYToLruKz5zIjnszqpr3nLdtE/KO8/bqtlPbgv4RsOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCtKie5SvrW37dikT3AZApdPmqjrd0VpeXi0h3xzS+95wnz8PszqmH7H6WBK8XPoJ9+ibsTJZ9I+t17mnjWH/WLat5zK/LIddRqzlbqa8imOXZo00cSNztbjg375dMFKEVacII3XZaulRFm0CljYb96IW3q7Bms/6aCb4mloVcd5GBnppRr9AXVfdOK8X3+t/z4z/yP8DqtU0JvNlfMMBAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERBHU6PCos6B77ZbxQdZMKXvW/7U+RdXiwdbtXZWDVAYz/W2869weh9IJSn9XjftP/Ept5ZCa4NKSzq4w7sToJbKjTNdg7+7Qdf0MVY9RH/tktbjZqUJX1eqmVDrqGfgVwzvO2GVXuk+hNUjBYVquVCzqips+p0VFsOWZfknKuI2iJ1zqmVST1emhPztd9oUbHBv+38Nv+EtOvU4QAAjiIEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUpEX/RzMyPDvhGcRucKdY1r5Pb1tfp09MpWQXlq1l78Wy9kV93MR4AluqnYOxjHtBpAFbrSBUirB1j63l5VVqfceaj75McNqzdR9nT/ePT96v990/5c8R3n2+Tl/vFPV5FRf8Y4O7dM+Fg2/338jRHUb68Xr9ebwxGP58Dbzo7xMwsFtvu/MiUR+RPgcr4S1QSjP+bZv9/u3abnX4hgMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiII6nF5l1lC/0c2R94/l6roeQS2Lb9VvlGb1vtui3mVps74otcx7cSEJritJLZ/gH8/qEgxXPpgEL5mvlohv9YXXS3XPa85ftTB9lr6RGVHwsP6Hur/F0ia9hPzw0/77XDkg1uJ3zj37u2I+F3WVxrZv6fNe3OKvO9n1Eb3vydv889nU5UFu5mx9XuMP+vc99pjufzF17pB3bNMlz8ttx/5pqxwf+pn/h8zUuf36Z8yw/9nNNjLhtWmH9rG6lwEAsDYEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUmSRJjMXUjz4LCwtueHjYnf6xL7pcqRz12O2SHs+LFOFUpp0EpwirNOD8ytrSj1Wqr0rl7o5Xw9OPrX2rj0TWvcj6V4BfE6stQmKkiKrzzlfDn5/mgD5wzpiPvGhf0K5Y9QD+oZbOxHWuE57SnzXWxa+N+c87pzO9XWlWn9jSFv/DWZwL/7HaHDTuo3Heich+r4/obYee85/3wB7/D6BWq+b++52fdfPz825oyJ/yzTccAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUtCfokbU0fcnIv1c58lbdSH1d2FL8qZZRR6HaKmSMOgnZJsAq3zCeQNmyoaN33vSXA7iCXj1e1pV0Cmus01HXbGyrajDUs5Wqjenx/LJ/rDSnH4JFUZNiPT9l4z3TLvvvs1GGI4/d9nc96KqPZIPrrbK6s4FT19QY0fNRWNDPfWV/EtTiJFUbVe1A/A9uu7G6UMI3HABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFNTh9Kg0p8fbRZ3nrmo4mgPhNRgtkdefyrTD910b1duqmpaRZ/SB971Tf+bJVf3XNf6YcVF7/UPTZ+mileaQv4Dj5G/phiSLW3Wjnvl1/mtu/2ddINS33f+QjD+um+nsPr8ixxfe4B8rLOr7NLjLP1/Tb5ObuomHVSFXWtPi3/czv6V/hJVf9L/h+qZ0vcvEPdNy/Kn/6i/0Ki7qPl2ZluiLtVU31dr4DX3NK+v9441hXRTVLvnfb5Pfq8l+OKvBNxwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUpEX3KNs02g9kjPRksbm1jPvwz/xpwEubjTRfI+V68Hn/iWVH9DXNneofK83q89rwsE5tPvir/u2n3qU/L/Xv9p/3+kf1cQ+c6T/urvfr9OLighx2o0/6j708Myi3XdnkH+sU9XmNPd4OTumfP0U/Awsn+e/F+KP6PTNzqk4j74jhoZ+44HKAvv36Dbfrv2yQ4+Wnw9KeUwNT/hPrPKTv48HTw0schn+qt519i/+8GkP+FPNW02oU8RK+4QAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAoqAOp0flWZ1fvzyp6xVaIsU+29LHVuMtvRq6aw7o8x563r+8eNbIsX/+A/3eseXNutYhv6LrdNY/6r/oha368W3953nvWPH/0hM2+IL/s9jCyfoeL2/R17zp3mXvWGFJ12DsPtE/X/0vWs+ecd6T/msuzoXXpy2doI+bN1a2r0x3guveDpztP692ST97lWmr5k6MGR/lZ3/F/+yWjeO2jfuo7kXbaGMyeZ//xGdPE2016m3ntstdv3Ru9ksAAFg7Ag4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCtKie1QfCk97ThX8GbGuf0qn01bHRFqikRY9sFOf9+yv+HfQ6tPb9u1xwWmYHf+K512Lm/2PaHFBp49mtw95x2ZO1+dVH/aP5Yw03sKy/hxX2+B/SOa36QnpFPzPyOyv6vk45cYVOT70rH9Onvlt/YCN/Nh/zbmG3NRM6VflALV1etvSdPhn6uKSfj+2xLOt3qupTj6shUmq/0V9XisT/mM3RvTOB3f5x/v2+Y/bbhq9VV7GNxwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBTU4fSoNqbrN8oHdZ57p5gJWh4+1RgUxz0gN3XOyO1PxJPQHNDbFufD52N5Uyb4vAf26tz/TsE/n02j9qO44IJlW/qaVa2NVU81+Jz/mvIr+rgHz/S3kUhVx/33ov8FF1xPVTJaejQG9TMwv81/zZUDet/lGf94bVQfd+4U3b6gb8q/74xVlpL4h5pG3VvTmK+8KLda/6g+sf1nZ4PaMXRqxvv4ZXzDAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEQR1Oj7JtPd6qZIK3L8/oHPl8NRNUQ5HK1eWwyy36xzb8QDc02fM/+Yswsk19Xv0vGnUUs/45OXCGrpPINsNqKFKL2/xjpRl9TRljrrNN/7FXNuptc+IZKB/U2yZr+HjZHNDz1b8nCeobk+oUjWMP+fe9/rGWcc3+Y8+erd9vuTn94zFXEzVRVaP+bJ1/rG301GqM6fMefsL/vmgXjJ9P9UxQX6K22O6IfazqVQAArBEBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFadE9Ks7rdMd2yUgBFTO+skHHf7XUf74mN3WdXPj43Bt03qpKx232ra1tgpqTwrLetl0Ma8eQyjYyQUvxd/dtfIxTLSpyxn1U19yqhB83NbjLn25bW6cvamlzJmi5/FRlWj8EpVn/2ME36ZtRH/Pve/x+/abIGCUQS1vUtsZc/8x/XjldheDae6zvCf59L52gt03ElPS/4N9vu2G8kV/GNxwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBTU4fSokzeWpu+E151Yy7SrGp7SrM6Dt9oX1EczwUv5u6p/qDGoj9sc0uOlmSSoZqB77CEXvGT+wG7/cevDettWnx4vLPn33bfPBdd5qXuYGn5WF5aUD/rXn5/5Vf1wtjb5ezJsulXXyiTGyvYH3pINbrsx/qh/rqffajx7s3p87An/fFbH9Wf5g2/1b1uc0Q/2+h/qHzLVsWxwjVi2Efbzyfqxd3j/q3wdAABrQsABAERBwAEAREHAAQBEQcABAERBwAEAREFadI9yxjLcrUomeMl8a2l6lQZcH8is6aOFbH1Q1desljy3WgjkanrfGTHc7Nf7Vsvi51es++gfyzb1cWvDejzb9N+r/imdurx0gv8haAzpa1pZr9NtGwPZ4KX6N2wvBqXzp+ZP1g+nel8MiiXzrTTy8kH9nrHu8/zJ/vlsGW05Rn+UC071Pnimvo85UaZglTgsbxJp9+v8Y+268fPnZXzDAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEQR1Oj6xl7a3lv1VtSMGoDalu8B87a+Tuq2X+U01Rx2MtH58VNRrWeVntHpp9SXBtiDovqy2Cqg/K1fVc5qvhz0h9WD9A6pqt465FYUnvuzqWCa5nqUzr+WyIe7Uyqc+rOJ/8UureuuPi0IUFvW0n59+4tlFvW5wPbzHgrEdEjKt6qo7xXjyEbzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoSIvuUbalUzidkQLaHBR5hxmds5hf8o91isZx+42l2Fv+seKSvuaFcf++i2K/q0oxFqmr9XXGvkV6qJVSXV2fCU7jtVLQVXp7fUyfV2UqfOn5hnr2jOfPmi/VtiPJ6uNa7Qvyy0lQ+wFz3JgOlVKdWtoqnvu58J8jmY4xX0a6tppu9VynCuJnTHmm4x1rN/xjR5zbql4FAMAaEXAAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABRUIfTo+qEzmMv79fbt0v+sZFn28HL2u8/W392yNX1eQ8+568L6JvSxUWLW0rBdSVZ47wmv+fvb1Ce0QUJy5O54KXnC8v+81p4g67P2PCw3ndmKvz5Wtkk7tMeve34Y7pXxPP/xf/joLCgn691T7eDa612f0Q/X4P3Vbxjkw8sym33nD8YXP9j1cOc/P+86B17/jdOkNtWDvrnZP0PRTGMc27H7/nnI1Xe67+wjQ+o3gW6ZuqF3/Tf40614dw3nYlvOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAKDJJkhgNXo4+CwsLbnh42J3+sS+6XKkc9+BGDw1Lruaf7uaA0dtDXKrVo6VdDD9xq45C9Ryxax2McVGa1PSXWJh1S6VZfU2qB5DVW2h5kx7vf9G/74G9uhZr/iT/hNbG5aaub29476HljUavpqoYW9HHbfWFP/eFBbmpKy34H7DljfrzdtbobaXGOwW9basS/mxa51UfC3+vjz3h3/niZv+z127U3I//7//q5ufn3dDQkPd1fMMBAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEQXuCHuWrOmWxPqxTEpOMfzzb0scuibTW+qg+bk6kvKaaA/6xlUm97cjTSVA7hlTDmC+VPqrSi1N90/4U4+mz9KPfHPSfV3FBH9c6L3XNs4P6vDLiGWmXjfk40AlOnbdS0IvzmaCUaevZs+Qa+pqrY/7P1KWZJPgZsJ5tlZKfGnrBfy+aFaNFhZGiXlgWY4v6mudP9Odz1zb4t2vXVpeKzTccAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAU1OH0qGbUu1i1NAW1VLuxVP/iSZmg3PvV1A+pNgClWb3vhqhXyBjdL6x6hfIB//aJ8fQuT+SC50u1VVDtGFIVo96lMeLfvjau52vwef/YCffph+/AGXrN/Np6/3kPPy03lffZqrOx7mN+Jfz5yYrnurpB38fifPh7uWa0CFg40X/i5YNJ8Hsi1RH1VPV14W0mCov+sWxd7vbnr1vdywAAWBsCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAK0qJ7ZKXTJsYq3a2y/wUdnbXqBnb50yFbffrAjSHjxMSw1WKgOC/aE4jrTeWN+WyK87a2dSJ7NNs0tu2EtUxI1YfCW0VUpvS26hlRS8tbx00NvOD//JltGW0ARIpxcU5vm1/S56WWxe9/UW+bq/mPnTWezYVTdHr70LPZoPeE9b6orjdadgzqfQ/v8I8XjLle3pQJSkFvrzKS8A0HABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFdTg9yhj1CC5v5ND3+cdKs0YLAVEWYNXwqJqUVK4e1rqgu+uMaE9gtGtoDurx4oL/xEtGrUNtNBs8X4WlJGj599VQ82ntWbVNyHbW1qJC1X9k2tZS/qKNRFZvWx+Vw274p/4Lqw/rz8zLW/znNfKUno+hF/R4tu2/kXMn+1tjWO/19S/oN9zKuN53bX147dr6H/nfsHv+k/+4nZzxA+ZlfMMBAERBwAEAREHAAQBEQcABAERBwAEAREHAAQBEQcABAERBHU6P2pW11WConiSq30R3XBzaqisxe8cE1v9Yx+4U9bZW3xB17Oq4nrBs07/vjFEbUhvLBPfSyRt9Z5Y3+scSXWLhygdFfVBBX1NilEr07fO/oDlg9enJBNVSpUpzet9tUfdkPV+5qriPrU7wcVOLk9mgurZUUdR5LW3UD4HV+yprHFtp9vmvqTTrP267vrqfi3zDAQBEQcABAERBwAEAREHAAQBEQcABAERBwAEAREFadI/UsvWpdslK8VSjelu11H9lf7KmVMrqBv9Y+aDc1FUO+NNLm/1G+vF4JjgNOL9sLB8v5mvxJLmpnM/8it7WSm1W6e/qnK20euvZnPsVve/miP8+nnCX3lY9ulPv0J9rS3N61337xPM1kAtOq2+V9bOXrxkp++JeZYwU9PqIagWht7XSzBuD/n23+vW+l/P+ezWw2z+Z7YZRO/EyvuEAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAI6+Opxrr73Wfetb33JPPfWUq1Qq7t3vfrf7y7/8S3fqqacefs3555/v7r333iO2+9jHPua++tWvHv7zzp073eWXX+7uvvtuNzAw4C677LLuvvP5o78sqGPVWBirdOca/rFWRW9bFHUBLaNtQqus993/YhJcU7C4NRt0vdZxU41h/3VVt+ptywf82/bv1tvWR8VS7MZcWsvDl+b9Y02jTqIl6poq07oWIr+iP18WlvwPd3VUz5dqX1A+IDc165pqo/7zbg7obfv2JsGtDeplPV9q+6xVq5X1jy1t1tsOvqDHVfuMxPoRK86rPuIfbDdW992lp5/waSC54oor3DnnnONarZb70z/9U3fhhRe6J5980vX3//yd8vu///vuc5/73OE/9/X1/fzE2m138cUXu8nJSXf//fe7vXv3uo985COuUCi4L37xi72cDgDgGNJTwPnOd75zxJ9vuOEGt2HDBvfII4+497znPUcEmDSgvJrbb7+9G6DuuOMONzEx4c466yz3+c9/3n3yk590n/3sZ12xaHzsAAAcf/+GMz//0t8NjI6OHvH//+Vf/sWNj4+7M844w33qU59yKys//375wAMPuDPPPLMbbA656KKL3MLCgnviiSde9Tj1er07/spfAIBjS/A/mnQ6HXfVVVe58847rxtYDvnN3/xNd+KJJ7pNmza5xx57rPvNZceOHd1/+0lNTU0dEWxSh/6cjr2a9N93/uIv/iL0VAEAx3LASf8t5/HHH3ff/e53j/j/H/3oRw//d/pNZuPGje6CCy5wzz77rHvDG94QdKz0W9I111xz+M/pN5wtW7aEnjoA4Fj5K7Urr7zS3Xbbbd0ss82bdUrFO9/5zu7vzzzzTPf39N929u3bd8RrDv3Z9+8+pVLJDQ0NHfELAPA6/oaTJIn7+Mc/7m6++WZ3zz33uG3btpnbPProo93f0286qXPPPdd94QtfcPv37+8mHKS2b9/eDSKnn366O9q1jfTj2rhOHx1/1J+6mmvqbXf/z/5j5xf0Z4cNj+iU2cKSf3z+JP2YVA4kQemyqeqEHs9V/WPDP5WbuubPkyN7Si+2UkutFgKdgh4viLYKnbw+L5U2nWT0M2C1sKivywTfp8q+JDgV13pGXM0/Xlx0wWn1FqvdQ2nGP1YfywTXT6x7Sr9XrZYfKm1fPdepjrpXajqM0omggJP+NdrXv/51d8stt7jBwcHD/+YyPDzcrctJ/9osHf+1X/s1NzY21v03nKuvvrqbwfbmN7+5+9o0jToNLB/+8Ifddddd193Hpz/96e6+028yAIDXp57+Su3666/vZqalxZ3pN5ZDv775zW92x9OU5jTdOQ0qp512mvujP/ojd+mll7pbb7318D5yuVz3r+PS39NvO7/927/drcN5Zd0OAOD1p+e/UlPSf8j/xVUGXk2axfbtb3+7l0MDAI5xrKUGAIiCgAMAiIKAAwCIgoADAIji6O8HcJSx8tiHntXjc6f4l4DPtvW2E//dn5+/YtRJHDxd91UY3On/7JGv6fNa3Crqg4xl2guLOhFF1bw0+/Q1d0SWfWk2Cb4m6xkozxhFCWLYWm5fbZtfCZ9L6z7XRE1TqjkU3jahVTZqsURbDtVGwmoDYN2njDFfqr7I2rYsjm3V2bSM514+n0Z5UN9+/71aOFG0JzBachzCNxwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBTU4fSoPmK8oKMT3Qd3+vPcF7fq+D/9Vv++Rx/XNQWVaT2+MuE/9vIWXUcx9kP/WKfo1lSDsbjNf95Dz+pramf9+64Z9Rv9L/r3ndHT4ZY3GXUlomZh6Dm983bJv++VSaMuybgX637iP3Z+We+7MeIfnz9ZP9dDz+v7WB0XNVENF1x7ZD0DOaP+rLbef979u406r5PENRk1Lar/VKolenZZvZpUrU1O1fcYtWmH8A0HABAFAQcAEAUBBwAQBQEHABAFAQcAEAUBBwAQBWnRPSrN6vF2WY83ErGUv5GG6WZEmu+Y3rS4YIzPiTTgtv5cUhsPSwHujjd0imf/Lv81N4b1vjuiI0NhWW+bEafVMpaPt+5jRrSh6BSM1Gbxji0s6eNa86VS49Wy9VZatNXOoWm0PlDbW2m+Kl3Xegaa/Xq8PO2/5kxHP9fF+bDn1kqNt56vvPF+y4iyjlzVv23W2O/h163qVQAArBEBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAV1OD3Ki1z0l+gc+bZYjr9tLB+fr/rHcjVjqX6jDUCmvbo8+l4VlvR+66J+I5WImoRMyzi2mK9kDU++qkdINQf0NRVXxPZJeA1Gq6K3LRvL2jdFfdHKev3ZtLDg33cnn1lTXYm6V0VxXGupflWvksoZrQ9Uu4e20aYkI57d4vLa3suqNqmwuIYasaJoeyDqC1+JbzgAgCgIOACAKAg4AIAoCDgAgCgIOACAKAg4AIAoSIvuUWMwPI3XTKs2UilVxnWrT29rLRGfFemQ5Rmdprky4T92dUNmTam6ifhItLzZSD+eC0/XVve5OSA3dSWx9LyV3q5Sk1MtsZR/rhZ+3JfG/XNSX6fPa2Wjf7wynawtLVqM1Yf1tn37k+Btrfs8+HwSvG1T3Ockp8+rYKRNd5prmOtsWLq1kWF+GN9wAABREHAAAFEQcAAAURBwAABREHAAAFEQcAAAURBwAABRUIfTIysHvjG0umW6Q2plWiK3P1tfW1sFVT+0MmnUBYgl4pOs3rYxHF4XUJkKr++w7lNO1Kz0GcdtGrVa1XH/eN6opSnNJcG1WFYtjWpx0THqy4oLYfewe9x6eNsEddzutqJuqTGkt7WW8q+NZYKvuV3yj2WMFhWtcni9XqajN3ViPCvaNSRGK4fD+1jdywAAWBsCDgAgCgIOACAKAg4AIAoCDgAgCgIOACAK0qJ7NPS8zv+rjYk1vJ1zM6f7Y3x9oiW33XSXf9tcQ+c7Tp+lb3VpJmxJfCt9NNvSOZ5WmmZNpBBnjHYOOZEK3jbStVWaeNtIS83q2+g6RRdMzpeRTtsu63HVRsBKb6+Piv3u18dV6bbWfFltAEae9U9YfZ3O9a6P6gktzfjnJGeUKTQH/WMt6z4ZrTU6+UxQiwHrGSnN+o+baRgP38v4hgMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiII6nB5Nv7W0ppqCyrR/bGCnrguorfOPdXL6Vhbn9Hk1RF1A+YDeNr/iH6tuCG/X0D32QX9+f85Yyl+2CTDKBlS9QlG0Y7C2TbULYll7o65EtU2wWmdY7QuWN2WD7nGqNOsf6+T0cTM5fd6V/eEtGeZPzgW1ekhljfqhlUn/WGFJb9u/Owlub2G11iiIOp1sU2+rau5UK5F2fXXvc77hAACiIOAAAKIg4AAAoiDgAACiIOAAAKIg4AAAoiDgAACioA6nR1bth+qj0iX6mWTbetP6UHhdSV70hrH6neTqSXDPGqs3TN6oHVH9X1Y2ZoLvVc6ol8o2RZ3EgFFzYAwXlsNrfFqiF09jWB+3uKDH1X1Wx01l2uEfa2tjVl2Jf6w4n/zSarGs8y7NraHnUdY/lF/RJ2b1Y2r2Z4Kfe/WMqPdEm344AICjCQEHABAFAQcAEAUBBwAQBQEHABAFAQcAEAVp0T1Sy3evJm0621app3rbZn9Yimaqkw9f0lymvHbTaV3wNbUq4eeVF+nFZmqqkcWZaYVva6XGqxYDa9m3SrdOtY1U3YZIIe7blwTv20qpzjb1eeUa4e0J1PNXmtHHbVX0uLxXIp3famNiPT/G21G+H7MtowyhHZa+TnsCAMBRhYADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgjqcHjWGdR776G49Xl/nj/GNQX1stdR/ZVon/lfX688WC6f4x/p36xz7ROy6csCaD2OpddUKwPi4VBbHVi0VrNoQq+VCJxveBsCswRB1JR3j3TxgPJuN4UzwfVJ1S6rFxEsvCK8PsvY9uMv/gpp4L3aPO6L33f+ifz47xjNSH8kE1/oVF8Prg6x6vLxoQaDqe5waewW+4QAAoiDgAACiIOAAAKIg4AAAoiDgAACiIOAAAKIgLbpHIzv0eGPISvP1j2WNdccHRIrn4pbsmlJPh5/2j9XG9LYqlddq16DaD6SqIn05XzVSiEv+seJCeIqw1UKgsKxf0BRpvirF3EpNLSzq41qp4Oq61HL6Zqq3cU1yrrvXFb7cfm3Uf/CsSAFOlafDzzsxpro4L9pu1Nb2M0Ye13ju1bO7tNW/XbtmvClexjccAEAUBBwAQBQEHABAFAQcAEAUBBwAQBQEHABAFMdkWnSSvJSC124Y+YO/BO2GTv9rGzParoevfNtu+l/Qrq8tLVpdlzpnKy3amq+kvYb5slaoFcfOGFmc8pqNbbPWMyJuhpVCnIhViBMjzbedDU+LzrTD59q6Juv5UinZiZEW3c5kgre17rOaTystuq3mq7O2+Qo9bndcPEPtmv+4nXrtiJ/NPpnEesVRaPfu3W7Lli2v9WkAAF5h165dbvPmze51FXA6nY7bs2ePGxwcdJlMxi0sLHQDUHqxQ0NDr/XpHfWYr94wX6vHXB2f85UkiVtcXHSbNm1y2Wz29fVXaukFvVoUTW/YsXzTYmO+esN8rR5zdfzN1/DwsPkakgYAAFEQcAAAUbwuAk6pVHJ//ud/3v0dNuarN8zX6jFXvSkdZ/N1TCYNAACOPa+LbzgAgKMfAQcAEAUBBwAQBQEHABDFMR9wvvKVr7iTTjrJlctl9853vtM99NBDr/UpHRXuu+8+94EPfKBb+ZuuxvBv//ZvR4ynuSKf+cxn3MaNG12lUnHvf//73U9/+lN3vLr22mvdOeec0129YsOGDe5DH/qQ27HjyH7itVrNXXHFFW5sbMwNDAy4Sy+91O3bt88dj66//nr35je/+XDB4rnnnuv+/d///fA4c+X3pS99qfuevOqqq467+TqmA843v/lNd80113TTCn/wgx+4t7zlLe6iiy5y+/fvd8e75eXl7nykAfnVXHfdde5v//Zv3Ve/+lX3ve99z/X393fnLn3wj0f33ntv9w3/4IMPuu3bt7tms+kuvPDC7jwecvXVV7tbb73V3XTTTd3Xp8srXXLJJe54lK70kf7gfOSRR9zDDz/s3ve+97kPfvCD7oknnuiOM1ev7vvf/777h3/4h26wfqXjZr6SY9g73vGO5Iorrjj853a7nWzatCm59tprX9PzOtqkt/nmm28+/OdOp5NMTk4mf/VXf3X4/83NzSWlUin5xje+8Rqd5dFl//793Xm79957D89PoVBIbrrppsOv+clPftJ9zQMPPPAanunRY926dck//uM/Mlcei4uLyRvf+MZk+/btyXvf+97kE5/4RPf/H0/zdcx+w2k0Gt1PV+lfBb1yjbX0zw888MBrem5Hu+eee85NTU0dMXfpOkjpX0kydy+Zn5/v/j46Otr9PX3W0m89r5yz0047zW3duvW4n7N2u+1uvPHG7rfB9K/WmKtXd8UVV7iLL774iHlJHU/zdUwu3pk6cOBA90GfmJg44v+nf37qqades/M6FqTBJvVqc3do7HiWrkae/v36eeed584444zu/0vnpVgsupGRkSNeezzP2Y9//ONugEn/Gjb9d4ebb77ZnX766e7RRx9lrn5BGpB/8IMfdP9K7RcdT8/WMRtwgF/mJ9HHH3/cffe7332tT+Woduqpp3aDS/pt8F//9V/dZZdd1v33BxwpbT3wiU98ovtvg2ly0/HsmP0rtfHxcZfL5f6HTI70z5OTk6/ZeR0LDs0Pc/c/uvLKK91tt93m7r777iNaYKTzkv417tzc3BGvP57nLP1Ufsopp7izzz67m+WXJqn8zd/8DXP1C9K/MksTmd72tre5fD7f/ZUG5jRpJ/3v9JvM8TJf2WP5YU8f9DvvvPOIvwpJ/5x+zYfftm3bug/yK+cubQSVZqsdr3OX5lakwSb9a6G77rqrO0evlD5rhULhiDlL06Z37tx53M7ZL0rff/V6nbn6BRdccEH3rx8fffTRw7/e/va3u9/6rd86/N/HzXwlx7Abb7yxm1l1ww03JE8++WTy0Y9+NBkZGUmmpqaS412aEfPDH/6w+yu9zX/913/d/e8XXnihO/6lL32pO1e33HJL8thjjyUf/OAHk23btiXVajU5Hl1++eXJ8PBwcs899yR79+49/GtlZeXwa/7gD/4g2bp1a3LXXXclDz/8cHLuued2fx2P/uRP/qSbwffcc891n5/0z5lMJrn99tu748yV9t5XZKkdT/N1TAec1N/93d91b1SxWOymST/44IOv9SkdFe6+++5uoPnFX5dddtnh1Og/+7M/SyYmJrpB+4ILLkh27NiRHK9eba7SX1/72tcOvyYNxn/4h3/YTf/t6+tLfv3Xf70blI5Hv/u7v5uceOKJ3ffd+vXru8/PoWCTYq56CzjV42S+aE8AAIjimP03HADAsYWAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAIiCgAMAiIKAAwCIgoADAHAx/P/oh2Zl3ZsG+wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 4000x2600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: What light through yonder window breaks?\n",
      "Come, my soul of my swrying pierch;\n",
      "White's nose and the loggest be looks you have\n",
      "Carrain'd; a jealouragemen: I humble. But vassal purpose, for thee;\n",
      "When, good my lord; he sits the execution and will'd may life as mech must be fury drops of niname receive our tramends\n",
      "Of the unto their cidence and,\n",
      "The little;\n",
      "And hath hit. First ABRATUS:\n",
      "Like state,\n",
      "We weep in him.\n",
      "\n",
      "BENVOLIO:\n",
      "Unheld! mean myself,\n",
      "Mean my sir, but hide as you seem's the hold,\n",
      "Lest I leave you fire.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Now, for a pluck and tree bad spirated with suddenly sovereign's in this, enough?\n",
      "\n",
      "PETRUCHIO:\n",
      "No, ho! I means my come is stricture: amischance, our state in that body, and vipens are you:\n",
      "O, mightst you knows to your most honour fled!\n",
      "\n",
      "MENENIUS:\n",
      "She lamentable guilty mine own insire new-rush, and pleasure, I say.\n",
      "\n",
      "Sweeting\n",
      "speak no it. Destroy one thyself this pride,\n",
      "That Ane fierced take home; and my thee \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "\n",
    "prompt = \"ROMEO: What light through yonder window breaks?\"\n",
    "generated = decode_sequence_char(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=900,\n",
    "    block_size=1024,\n",
    "    use_fenchel=False,\n",
    "    tau=1.5,\n",
    "    fenchel_iters=2,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
