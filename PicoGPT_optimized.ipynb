{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copyright joshuah.rainstar@gmail.com\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# ===========================================================\n",
    "# Utilities\n",
    "# ===========================================================\n",
    "\n",
    "def _norm(v, eps: float = 1e-12):\n",
    "    return torch.linalg.vector_norm(v, dim=-1, keepdim=True).clamp_min(eps)\n",
    "\n",
    "\n",
    "def _unit(v, eps: float = 1e-12):\n",
    "    return v / _norm(v, eps)\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def phase_transport_between(\n",
    "    curr: torch.Tensor,\n",
    "    prev: torch.Tensor,\n",
    "    tau: float = 1e-6,          # semantic threshold (unchanged)\n",
    "    eps: float = 1e-12          # numeric epsilon (NEW: decoupled from tau)\n",
    ") -> torch.Tensor:\n",
    "    assert curr.shape == prev.shape and curr.dim() == 3\n",
    "    B, T, C = curr.shape\n",
    "\n",
    "    # Units (reuse norms) ‚Äî clamp with eps (NOT tau)\n",
    "    nu = torch.linalg.vector_norm(curr, dim=-1, keepdim=True).clamp_min(eps)   # (B,T,1)\n",
    "    nv = torch.linalg.vector_norm(prev, dim=-1, keepdim=True).clamp_min(eps)   # (B,T,1)\n",
    "    u = curr / nu\n",
    "    v = prev / nv\n",
    "\n",
    "    w = curr - prev\n",
    "    c = (u * v).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "\n",
    "    # Masks (semantic thresholds use tau)\n",
    "    near_pos = (c >  1.0 - tau)                                                # (B,T,1)\n",
    "    near_neg = (c < -1.0 + tau)                                                # (B,T,1)\n",
    "    small_u  = (nu < tau)                                                      # (B,T,1)\n",
    "    small_v  = (nv < tau)                                                      # (B,T,1)\n",
    "    trivial  = near_pos | small_u | small_v                                    # (B,T,1)\n",
    "\n",
    "    # General branch\n",
    "    denom = (1.0 + c).clamp_min(eps)                                           # (B,T,1)\n",
    "    a = (v * w).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "    b = (u * w).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "    Kw  = u * a - v * b                                                        # (B,T,C)\n",
    "    K2w = u * (a * c - b) + v * (b * c - a)                                    # (B,T,C)\n",
    "    y_gen = w - Kw + (K2w / denom)                                             # (B,T,C)\n",
    "\n",
    "    # Antipodal candidate\n",
    "    if C == 1:\n",
    "        y_neg = -w\n",
    "    else:\n",
    "        # Keep this normalization stable with eps as well\n",
    "        idx = torch.argmin(v.abs().reshape(-1, C), dim=1, keepdim=True)        # (B*T,1)\n",
    "        s = v.reshape(-1, C).gather(1, idx)                                    # (B*T,1)\n",
    "        p = -s * v.reshape(-1, C)\n",
    "        onehot = F.one_hot(idx.squeeze(-1), num_classes=C).to(s.dtype)\n",
    "        p = p + onehot\n",
    "        n = torch.linalg.vector_norm(p, dim=1, keepdim=True).clamp_min(eps)\n",
    "        p = (p / n).view(B, T, C)\n",
    "        proj_v = (v * w).sum(dim=-1, keepdim=True) * v                         # (B,T,C)\n",
    "        proj_p = (p * w).sum(dim=-1, keepdim=True) * p                         # (B,T,C)\n",
    "        y_neg = w - 2.0 * proj_v - 2.0 * proj_p\n",
    "\n",
    "    # Fuse selections\n",
    "    y = torch.where(trivial, w, y_gen)\n",
    "    y = torch.where(near_neg, y_neg, y)\n",
    "    return y\n",
    "\n",
    "\n",
    "        # ----- STREAMING STATE FOR INFERENCE ----\n",
    "        \n",
    "        \n",
    "# Lifted Pyramid Feature Generator\n",
    "# --------------------------------\n",
    "# Implements per-rank Stiefel lifts inside the feature generator itself.\n",
    "# Each rank s:\n",
    "#   - build dyadic centroid mu_s in base coords (equivalently, linear average in lifted coords)\n",
    "#   - lift mu_s to rank-s manifold via A_1..A_s\n",
    "#   - compute phase-transport deltas at rank s in lifted space\n",
    "#   - orthogonally project the delta back to the lifted column space\n",
    "#   - lower (left-inverse) to base C for output fusion\n",
    "# Outputs remain (B, T, K, C), preserving downstream interfaces.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "# Expect phase_transport_between(curr, prev, tau) to be available in scope\n",
    "# from the user's existing code.\n",
    "\n",
    "# -------------------------------\n",
    "# Dimension rule (~1/100th rounded)\n",
    "#   <100 -> 1; 200-300 -> 2; 400-599 -> 3; etc.\n",
    "# -------------------------------\n",
    "\n",
    "def add_dims(embed_dim: int) -> int:\n",
    "    return (embed_dim // 200) + 1\n",
    "\n",
    "\n",
    "class SharedStiefelTower(nn.Module):\n",
    "    def __init__(self, base_dim: int, K: int, t: float = 1.0, seed: int = 0, learn_t: bool = False):\n",
    "        super().__init__()\n",
    "        assert K >= 1\n",
    "        self.C = int(base_dim)\n",
    "        self.K = int(K)\n",
    "\n",
    "        # compute cumulative lifted dims per rank\n",
    "        dims = [self.C]\n",
    "        d_prev = self.C\n",
    "        for _ in range(1, K):\n",
    "            d_add = (d_prev // 200) + 1  # your 1/100th rule\n",
    "            d_prev = d_prev + d_add\n",
    "            dims.append(d_prev)\n",
    "        self.rank_dims = dims  # length K; dims[0]=C\n",
    "\n",
    "        D_final = dims[-1]\n",
    "        M = torch.randn(D_final, D_final)\n",
    "        S = M - M.T\n",
    "        S = S / (S.norm(p='fro') + 1e-12)\n",
    "        self.register_buffer(\"S\", S, persistent=False)\n",
    "\n",
    "        t_val = torch.tensor(float(t))\n",
    "        self.t = nn.Parameter(t_val) if learn_t else t_val\n",
    "        self._q_cache = {}  # keyed by (r, device, dtype)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _Qr(self, r: int, device, dtype):\n",
    "        key = (r, str(device), str(dtype), bool(isinstance(self.t, nn.Parameter)))\n",
    "        if key in self._q_cache:\n",
    "            return self._q_cache[key]\n",
    "        D_r = self.rank_dims[r]\n",
    "        S_r = self.S[:D_r, :D_r].to(device=device, dtype=dtype)\n",
    "        t = self.t.to(device=device, dtype=dtype) if isinstance(self.t, nn.Parameter) else self.t\n",
    "        Q_r = torch.matrix_exp(t * S_r)\n",
    "        self._q_cache[key] = Q_r\n",
    "        return Q_r\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _Ar(self, r: int, device, dtype):\n",
    "        # A_r ‚àà R^{D_r x C}, columns orthonormal\n",
    "        Q_r = self._Qr(r, device, dtype)\n",
    "        return Q_r[:, : self.C]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def up_to(self, r: int, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (..., C) -> (..., D_r)\n",
    "        A = self._Ar(r, x.device, x.dtype)\n",
    "        return F.linear(x, A)  # x @ A^T\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def down_from(self, r: int, y: torch.Tensor) -> torch.Tensor:\n",
    "        # y: (..., D_r) -> (..., C)\n",
    "        A = self._Ar(r, y.device, y.dtype)\n",
    "        return F.linear(y, A.T)  # y @ A\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def project_to_rank(self, r: int, y: torch.Tensor) -> torch.Tensor:\n",
    "        # orthogonal projector onto col(A_r)\n",
    "        return self.up_to(r, self.down_from(r, y))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# MultiRankLift: A_1..A_{K-1}\n",
    "#   dims[0] = C\n",
    "#   dims[r] = dims[r-1] + add_dims(dims[r-1])\n",
    "# -------------------------------\n",
    "\n",
    "# -------------------------------\n",
    "# Lifted Causal Pyramid (vectorized)\n",
    "# -------------------------------\n",
    "\n",
    "class CausalCentroidPyramidLifted(nn.Module):\n",
    "    def __init__(self, num_scales: int, tau: float = 1e-6, t: float = 1.0, learn_t: bool = False, seed0: int = 0):\n",
    "        super().__init__()\n",
    "        assert num_scales >= 1\n",
    "        self.K = num_scales\n",
    "        self.tau = float(tau)\n",
    "        self._lifts: SharedStiefelTower | None = None\n",
    "        self._t = t\n",
    "        self._learn_t = learn_t\n",
    "        self._seed0 = seed0\n",
    "\n",
    "    def _ensure_lifts(self, C: int):\n",
    "        if self._lifts is None:\n",
    "            self._lifts = SharedStiefelTower(C, self.K, t=self._t, learn_t=self._learn_t)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor, mask_early: bool = True) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        self._ensure_lifts(C)\n",
    "        lifts = self._lifts\n",
    "\n",
    "        # -------- token-level PT in base --------\n",
    "        prev_tok = torch.zeros_like(x)\n",
    "        if T > 1:\n",
    "            prev_tok[:, 1:, :] = x[:, :-1, :].contiguous()\n",
    "        d0 = phase_transport_between(x, prev_tok, tau=self.tau)  # (B,T,C)\n",
    "        if mask_early:\n",
    "            d0[:, :1, :].zero_()\n",
    "        if self.K == 1:\n",
    "            return d0.unsqueeze(2)\n",
    "\n",
    "        feats = [d0.unsqueeze(2)]\n",
    "\n",
    "        # -------- recursive dyadic centroids in base --------\n",
    "        mu_prev_base = x  # mu_0\n",
    "        for s in range(1, self.K):\n",
    "            W1 = 1 << (s - 1)\n",
    "            W = 1 << s\n",
    "            # mu_s(t) = 0.5 * (mu_{s-1}(t) + mu_{s-1}(t - 2^{s-1}))\n",
    "            mu_shift = torch.zeros_like(mu_prev_base)\n",
    "            if W1 > 0 and T > W1:\n",
    "                mu_shift[:, W1:, :] = mu_prev_base[:, :-W1, :]\n",
    "            mu_s_base = 0.5 * (mu_prev_base + mu_shift)\n",
    "            if mask_early:\n",
    "                if W - 1 < T:\n",
    "                    mu_s_base[:, : W - 1, :].zero_()\n",
    "\n",
    "            # prev for PT at this rank\n",
    "            prev_mu = torch.zeros_like(mu_s_base)\n",
    "            if T > W:\n",
    "                prev_mu[:, W:, :] = mu_s_base[:, :-W, :]\n",
    "\n",
    "            # lift both and compute PT in lifted space\n",
    "            mu_s_lift = lifts.up_to(s, mu_s_base)\n",
    "            prev_lift = lifts.up_to(s, prev_mu)\n",
    "            d_s_lift = phase_transport_between(mu_s_lift, prev_lift, tau=self.tau)\n",
    "            # ensure closure: project to lifted column space\n",
    "            d_s_lift = lifts.project_to_rank(s, d_s_lift)\n",
    "            # lower to base for output\n",
    "            d_s_base = lifts.down_from(s, d_s_lift)\n",
    "            if mask_early:\n",
    "                d_s_base[:, : W, :].zero_()\n",
    "\n",
    "            feats.append(d_s_base.unsqueeze(2))\n",
    "            mu_prev_base = mu_s_base  # next rank recursion\n",
    "\n",
    "        return torch.cat(feats, dim=2)  # (B,T,K,C)\n",
    "\n",
    "# -------------------------------\n",
    "# Lifted Streaming State\n",
    "# -------------------------------\n",
    "\n",
    "class CausalPyramidStateLifted:\n",
    "    def __init__(self, num_scales: int, C: int, device, batch_size: int = 1, tau: float = 1e-6,\n",
    "                 t: float = 1.0, learn_t: bool = False, seed0: int = 0):\n",
    "        self.K = num_scales\n",
    "        self.C = C\n",
    "        self.B = batch_size\n",
    "        self.device = device\n",
    "        self.tau = float(tau)\n",
    "        self.t = 0\n",
    "        self.lifts = SharedStiefelTower(C, self.K, t=t, learn_t=learn_t).to(device)\n",
    "        # ring buffers for mu_s in base coords (economical)\n",
    "        self.buffers = []\n",
    "        self.ptrs = []\n",
    "        for s in range(self.K):\n",
    "            L = 1 << s\n",
    "            self.buffers.append(torch.zeros(self.B, L, C, device=device))\n",
    "            self.ptrs.append(0)\n",
    "\n",
    "    def _read(self, level: int, r: int):\n",
    "        if self.t < r:\n",
    "            return torch.zeros(self.B, self.C, device=self.device)\n",
    "        L = self.buffers[level].size(1)\n",
    "        idx = (self.ptrs[level] - r) % L\n",
    "        return self.buffers[level][:, idx, :]\n",
    "\n",
    "    def _push(self, level: int, value: torch.Tensor):\n",
    "        L = self.buffers[level].size(1)\n",
    "        self.buffers[level][:, self.ptrs[level], :] = value\n",
    "        self.ptrs[level] = (self.ptrs[level] + 1) % L\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor) -> torch.Tensor:\n",
    "        B, C = x_t.shape\n",
    "        lifts = self.lifts\n",
    "        feats = []\n",
    "\n",
    "        # token-level in base\n",
    "        prev_x = self._read(0, 1)\n",
    "        d0 = phase_transport_between(x_t[:, None, :], prev_x[:, None, :], tau=self.tau).squeeze(1)\n",
    "        if self.t == 0:\n",
    "            d0.zero_()\n",
    "        feats.append(d0)\n",
    "\n",
    "        # compute mu_s in base, then PT in lifted, lower result\n",
    "        mu_prev = x_t\n",
    "        for s in range(1, self.K):\n",
    "            W1 = 1 << (s - 1)\n",
    "            W = 1 << s\n",
    "            mu_back = self._read(s - 1, W1)\n",
    "            mu_s = 0.5 * (mu_prev + mu_back)\n",
    "            if self.t < (W - 1):\n",
    "                mu_s.zero_()\n",
    "\n",
    "            mu_prevW = self._read(s, W)\n",
    "            # lift + PT\n",
    "            curr_lift = lifts.up_to(s, mu_s)\n",
    "            prev_lift = lifts.up_to(s, mu_prevW)\n",
    "            d_s_lift = phase_transport_between(curr_lift[:, None, :], prev_lift[:, None, :], tau=self.tau).squeeze(1)\n",
    "            d_s_lift = lifts.project_to_rank(s, d_s_lift)\n",
    "            d_s = lifts.down_from(s, d_s_lift)\n",
    "            if self.t + 1 <= W:\n",
    "                d_s.zero_()\n",
    "            feats.append(d_s)\n",
    "\n",
    "            # push mu_s (base) and continue\n",
    "            self._push(s, mu_s)\n",
    "            mu_prev = mu_s\n",
    "\n",
    "        # push level-0\n",
    "        self._push(0, x_t)\n",
    "        self.t += 1\n",
    "        return torch.stack(feats, dim=1)   # (B, K, C)\n",
    "\n",
    "# -------------------------------\n",
    "# Wrapper: SemanticClusterFeaturesCausalLifted\n",
    "# -------------------------------\n",
    "\n",
    "class SemanticClusterFeaturesCausalLifted(nn.Module):\n",
    "    def __init__(self, num_scales: int, tau: float = 1e-6, t: float = 1.0, learn_t: bool = False, seed0: int = 0):\n",
    "        super().__init__()\n",
    "        self.K = num_scales\n",
    "        self.tau = float(tau)\n",
    "        self.t = float(t)\n",
    "        self.learn_t = bool(learn_t)\n",
    "        self.seed0 = int(seed0)\n",
    "        self.pyr = None  # lazy init with C known\n",
    "\n",
    "    def _ensure(self, C: int):\n",
    "        if self.pyr is None:\n",
    "            self.pyr = CausalCentroidPyramidLifted(self.K, tau=self.tau, t=self.t, learn_t=self.learn_t, seed0=self.seed0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        self._ensure(x.size(-1))\n",
    "        return self.pyr(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor, state: 'CausalPyramidStateLifted') -> torch.Tensor:\n",
    "        return state.step(x_t)\n",
    "\n",
    "\n",
    "\n",
    "class GroupedChannelMLP(nn.Module):\n",
    "    def __init__(self, k_dim: int, c_dim: int):\n",
    "        super().__init__()\n",
    "        hidden_dim = c_dim // 2\n",
    "        self.k_dim = k_dim\n",
    "        self.c_dim = c_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # shapes chosen for direct einsum without expands\n",
    "        # fc1: (K, H, C)   fc2: (K, C, H)   b2: (K, C)\n",
    "        self.fc1_weight = nn.Parameter(torch.empty(k_dim, hidden_dim, c_dim*2))\n",
    "        self.fc2_weight = nn.Parameter(torch.empty(k_dim, c_dim, hidden_dim))\n",
    "        self.fc2_bias   = nn.Parameter(torch.empty(k_dim, c_dim))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.fc1_weight, a=5**0.5)\n",
    "        nn.init.kaiming_uniform_(self.fc2_weight, a=5**0.5)\n",
    "        nn.init.zeros_(self.fc2_bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, K, C) or (B, K, C)\n",
    "        returns: same leading dims, last two dims (K,C)\n",
    "        \"\"\"\n",
    "        squeeze_time = False\n",
    "        if x.dim() == 3:  # (B,K,C)\n",
    "            x = x.unsqueeze(1)  # -> (B,1,K,C)\n",
    "            squeeze_time = True\n",
    "        elif x.dim() != 4:\n",
    "            raise ValueError(\"Input must be (B,K,C) or (B,T,K,C)\")\n",
    "\n",
    "        # (B,T,K,C) x (K,H,C) -> (B,T,K,H)\n",
    "        h = torch.einsum('btkc,khc->btkh', x, self.fc1_weight)\n",
    "        h = F.gelu(h)\n",
    "\n",
    "        # (B,T,K,H) x (K,C,H) -> (B,T,K,C)\n",
    "        y = torch.einsum('btkh,kch->btkc', h, self.fc2_weight) + self.fc2_bias\n",
    "\n",
    "        if squeeze_time:\n",
    "            y = y[:, 0, :, :]  # (B,K,C)\n",
    "        return y\n",
    "        \n",
    "        \n",
    "class Cell(nn.Module):\n",
    "    def __init__(self, dim_in: int, hidden: int,dim_out:int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, hidden, bias=False) #dont change, false intentional\n",
    "        self.fc2 = nn.Linear(hidden, dim_out, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, x):\n",
    "      \n",
    "        return self.fc2(self.act(self.fc1(x))) \n",
    "\n",
    "class GPTSemanticBlock(nn.Module):\n",
    "    def __init__(self, config: GPTConfig,features):\n",
    "        super().__init__()\n",
    "        C = config.n_embd\n",
    "        self.C = C\n",
    "        self.K = config.n_scales\n",
    "        # L = number of feature groups concatenated: token (1) + K scales\n",
    "        self.L = 1 + self.K\n",
    "        self.features = features #reuse to reduce param/mechanism counts\n",
    "        #lifting is needed to preserve uniqueness of centroid averaging.\n",
    "        #this ensures that all information on all scales is useful.\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.ln = nn.LayerNorm(self.C)\n",
    "        self.mlp = Cell(self.C,self.C*4,self.C)\n",
    "\n",
    "        # Each bottleneck maps C -> small_hidden -> C\n",
    "        self.bottleneck = GroupedChannelMLP(self.K, self.C)\n",
    "        #bottleneck to drop out meaningful features\n",
    "\n",
    "    # vectorized\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        feats = self.features(x)               # (B, T, K, C)\n",
    "        x_expanded = x.unsqueeze(2)\n",
    "        # Step 2: tile along K -> (B, T, K, C)\n",
    "        x_tiled = x_expanded.expand(-1, -1, self.K, -1)\n",
    "        # Step 3: concatenate along last dim -> (B, T, K, 2C)\n",
    "        out = torch.cat([x_tiled, feats], dim=-1) #location hint must be issued\n",
    "        feats = self.bottleneck(out) # (B, T, K, C)#bottlenecked\n",
    "        feats= feats.sum(dim=2)\n",
    "        # concat token embedding with processed features\n",
    "        x_in = x+feats\n",
    "        out = x + self.drop(self.ln(self.mlp(x_in)))\n",
    "\n",
    "        return out\n",
    "\n",
    "    # single-step incremental\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor, feat_state: CausalPyramidStateLifted) -> torch.Tensor:\n",
    "        # x_t: (B, C)\n",
    "        B, C = x_t.shape\n",
    "        feats_t = self.features.step(x_t, feat_state)  # (B, K, C)\n",
    "        x_expanded = x_t.unsqueeze(1)\n",
    "        # Step 2: tile along K -> (B, T, K, C)\n",
    "        x_tiled = x_expanded.expand(-1, self.K, -1)\n",
    "        # Step 3: concatenate along last dim -> (B, T, K, 2C)\n",
    "        out = torch.cat([x_tiled, feats_t], dim=-1)\n",
    "        feats_t = self.bottleneck(out)\n",
    "        feats_t= feats_t.sum(dim=1)\n",
    "        x_in = x_t+feats_t\n",
    "        out = x_t + self.drop(self.ln(self.mlp(x_in)))\n",
    "        return out\n",
    "\n",
    "\n",
    "def _is_prime(n: int) -> bool:\n",
    "    if n < 2: return False\n",
    "    if n % 2 == 0: return n == 2\n",
    "    r = int(n**0.5)\n",
    "    for f in range(3, r+1, 2):\n",
    "        if n % f == 0: return False\n",
    "    return True\n",
    "\n",
    "def _factorize(n: int):\n",
    "    f, cnt = [], {}\n",
    "    d = 2\n",
    "    while d * d <= n:\n",
    "        while n % d == 0:\n",
    "            cnt[d] = cnt.get(d, 0) + 1\n",
    "            n //= d\n",
    "        d += 1 if d == 2 else 2\n",
    "    if n > 1: cnt[n] = cnt.get(n, 0) + 1\n",
    "    return list(cnt.keys())\n",
    "\n",
    "def _primitive_root(p: int) -> int:\n",
    "    # p must be prime\n",
    "    phi = p - 1\n",
    "    factors = _factorize(phi)\n",
    "    for g in range(2, p):\n",
    "        ok = True\n",
    "        for q in factors:\n",
    "            if pow(g, phi // q, p) == 1:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            return g\n",
    "    raise RuntimeError(\"no primitive root found\")\n",
    "\n",
    "def _welch_costas_perm(V: int, device=None):\n",
    "    \"\"\"\n",
    "    Welch Costas permutation œÉ on {0..V-1}, where V = p-1 for prime p.\n",
    "    œÉ[i] = g^(i+1) mod p, mapped to 0..V-1 by subtracting 1.\n",
    "    \"\"\"\n",
    "    p = V + 1\n",
    "    if not _is_prime(p):\n",
    "        return None\n",
    "    g = _primitive_root(p)\n",
    "    sigma = torch.empty(V, dtype=torch.long, device=device)\n",
    "    for i in range(V):\n",
    "        sigma[i] = pow(g, i + 1, p) - 1\n",
    "    return sigma  # permutation of 0..V-1\n",
    "\n",
    "def _coprime_mul_perm(V: int, device=None):\n",
    "    \"\"\"\n",
    "    Fallback: œÉ[i] = (a*i + b) % V with gcd(a, V)=1 and a not ‚â° ¬±1 mod V.\n",
    "    Not Costas, but non-monotone and well-distributed.\n",
    "    \"\"\"\n",
    "    # pick a\n",
    "    a = None\n",
    "    for cand in range(2, V):\n",
    "        if math.gcd(cand, V) == 1 and cand % V not in (1, V-1):\n",
    "            a = cand\n",
    "            break\n",
    "    if a is None:\n",
    "        a = 1  # degenerate small V\n",
    "    b = V // 3\n",
    "    i = torch.arange(V, device=device)\n",
    "    return ((a * i + b) % V).long()\n",
    "\n",
    "def _perm_inverse(sigma: torch.Tensor) -> torch.Tensor:\n",
    "    inv = torch.empty_like(sigma)\n",
    "    inv[sigma] = torch.arange(sigma.numel(), device=sigma.device)\n",
    "    return inv\n",
    "\n",
    "class FlatRollEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Replacement for nn.Embedding that maps token id i -> cyclic roll^i of a base\n",
    "    length-V vector whose non-DC spectrum is flat (DC=0). Requires V == n_embd.\n",
    "    Weights are frozen by default.\n",
    "    this yields an optimal embedding that is considered perfect.\n",
    "    The 'eye' is mixed at 0.5 and then rows are permuted by a Costas-like order\n",
    "    to maximize uniqueness while keeping even collapse.\n",
    "    but wait, you're asking, my embeds/vocab is not orthagonal!\n",
    "    the solution is simple, clever, efficient- \n",
    "    use  Smooth full-space rotation matrix via Lie algebra exponential map.\n",
    "        A = exp(t¬∑G), where G ‚àà so(D) is skew-symmetric and full-rank.\n",
    "    partition vocab idx space by modulo over chosen block size, use different rotation\n",
    "    range from 0 to pi(evenly divided) for all partitions, use ONE embed matrix,\n",
    "    embed->shift. Minimizes necessary parameter count. up-project to desired embed dim.\n",
    "    for decoder, you're operating over a larger dimensional space as-is. that's fine.\n",
    "    if you like, you can try down-project and repeat-decode invert on all blocks,\n",
    "    and use stiefel inverting by transpose but use two sets of slices of rotation ranges\n",
    "    so that you have blue noise coverage with a partition going from original bound to bound\n",
    "    but also overlap going from mid to mid, try decode on all, hard route to one,\n",
    "    take logits from that one- > bam, no learned decode either.\n",
    "    down-projection tied to up-projection and you have a learned high efficiency mapping.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config, scale: str = \"box\", seed: int = 0,\n",
    "                 freeze: bool = True, dtype=None, device=None):\n",
    "        super().__init__()\n",
    "        assert config.n_embd == config.vocab_size, (\n",
    "            f\"Expected n_embd == vocab_size, got {config.n_embd} != {config.vocab_size}\"\n",
    "        )\n",
    "        V = int(config.vocab_size)\n",
    "        dtype = dtype or torch.float32\n",
    "\n",
    "        eye = torch.eye(V, dtype=dtype, device=device)\n",
    "        weight = self._make_weight(V, scale=scale, seed=seed,\n",
    "                                   dtype=dtype, device=device)  # [V, V]\n",
    "        M = int(torch.argmax(weight[0]))        # index of max in base x (row 0)\n",
    "        pm = weight[0, M]                       # scalar\n",
    "        N = 1.0 / pm\n",
    "        \n",
    "        eye = torch.roll(eye, shifts=M, dims=1) # shift spike position within each row\n",
    "        eye = eye * N\n",
    "        mixed =  weight + eye  # add identity towers\n",
    "\n",
    "        # --- compute a strong-scramble row order (Costas if possible) ---\n",
    "        sigma = _welch_costas_perm(V, device=device)\n",
    "        if sigma is None:\n",
    "            sigma = _coprime_mul_perm(V, device=device)\n",
    "        # We want ones at (row = œÉ[i], col = i). For row-permutation via index_select,\n",
    "        # use r_idx = œÉ^{-1} so that new_row j pulls old_row r_idx[j] with 1 at column j=œÉ[i].\n",
    "        r_idx = _perm_inverse(sigma)\n",
    "\n",
    "        # keep for reference / decoding\n",
    "        self.register_buffer(\"row_perm\", r_idx, persistent=False)\n",
    "        self.register_buffer(\"sigma\", sigma, persistent=False)\n",
    "\n",
    "        mixed = mixed.index_select(0, r_idx)\n",
    "        self.embed = nn.Embedding.from_pretrained(mixed, freeze=freeze)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _row_perm_max_equidistant(V: int, device=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Row permutation that evenly offsets the identity's '1' away from the diagonal.\n",
    "        Uses a single cyclic shift by k = floor(V/2).\n",
    "        \"\"\"\n",
    "        if V <= 1:\n",
    "            return torch.arange(V, device=device, dtype=torch.long)\n",
    "        k = V // 2\n",
    "        if k == 0:  # only happens when V == 1, handled above; keep for safety\n",
    "            k = 1\n",
    "        return ((torch.arange(V, device=device) + k) % V).long()\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_weight(V: int, scale: str = \"box\", seed: int = 0,\n",
    "                     dtype=torch.float32, device=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns a (V, V) tensor whose rows are cyclic rolls of a base vector x in R^V\n",
    "        with |FFT(x)|^2 flat for k=1..V-1 and DC=0.\n",
    "        scale:\n",
    "          - \"unit\": ||x||_2 = 1\n",
    "          - \"box\":  max|x_i| = 1\n",
    "        \"\"\"\n",
    "        # build on CPU, move at end\n",
    "        complex_dtype = torch.complex64 if dtype == torch.float32 else torch.complex128\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "        X = torch.zeros(V, dtype=complex_dtype)\n",
    "        # DC bin\n",
    "        X[0] = torch.tensor(0, dtype=complex_dtype)\n",
    "\n",
    "        if V % 2 == 0:\n",
    "            # bins 1..V/2-1 are complex-conjugate pairs; Nyquist bin must be real\n",
    "            for k in range(1, V // 2):\n",
    "                phi = torch.rand((), generator=g) * (2 * math.pi)\n",
    "                val = torch.cos(phi) + 1j * torch.sin(phi)\n",
    "                X[k] = val\n",
    "                X[V - k] = torch.conj(val)\n",
    "            X[V // 2] = 1.0 if torch.rand((), generator=g) < 0.5 else -1.0\n",
    "        else:\n",
    "            for k in range(1, (V - 1) // 2 + 1):\n",
    "                phi = torch.rand((), generator=g) * (2 * math.pi)\n",
    "                val = torch.cos(phi) + 1j * torch.sin(phi)\n",
    "                X[k] = val\n",
    "                X[V - k] = torch.conj(val)\n",
    "\n",
    "        x = torch.fft.ifft(X).real  # real length-V base vector\n",
    "\n",
    "        if scale == \"unit\":\n",
    "            x = x / (x.norm() + 1e-12)\n",
    "        elif scale == \"box\":\n",
    "            x = x / (x.abs().max() + 1e-12)\n",
    "        else:\n",
    "            raise ValueError(\"scale must be 'unit' or 'box'\")\n",
    "\n",
    "        rows = [torch.roll(x, shifts=r, dims=0) for r in range(V)]\n",
    "        W = torch.stack(rows, dim=0).to(dtype=dtype)\n",
    "        if device is not None:\n",
    "            W = W.to(device)\n",
    "        return W\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        # (batch, seq_len, V)\n",
    "        return self.embed(input_ids)\n",
    "\n",
    "\n",
    "        \n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 66 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 6\n",
    "    n_head:int = 6\n",
    "    n_embd: int = 128\n",
    "    n_scales:int = 9\n",
    "    dropout: float = 0.1\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.n_embd = config.n_embd\n",
    "        self.drop = nn.Dropout(0.6)\n",
    "        self.features = SemanticClusterFeaturesCausalLifted(num_scales=config.n_scales, tau=1e-8)\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = FlatRollEmbed(config),\n",
    "            h = nn.ModuleList([GPTSemanticBlock(config,self.features) for _ in range(config.n_layer)]),\n",
    "\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(self.config.n_embd, self.config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.transformer.wte.embed.weight\n",
    "\n",
    "\n",
    "    # ---------- forward ----------\n",
    "    def forward(self, idx, targets=None, eprint=False):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        x = self.transformer.wte(idx) \n",
    "        x = x.detach()                 # sever any stale history just in case\n",
    "        x.requires_grad_(True)         # make x a grad leaf for œÑ at layer 0\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "                x= block(x)\n",
    "\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_greedy(model: nn.Module, idx: torch.LongTensor, max_new_tokens: int, block_size: int):\n",
    "        \"\"\"\n",
    "        model: your GPT with:\n",
    "           - transformer.wte (embedding)\n",
    "           - transformer.h : list[GPTSemanticBlock]\n",
    "           - lm_head\n",
    "        idx: (B, T0) prompt token ids\n",
    "        \"\"\"\n",
    "        device = next(model.parameters()).device\n",
    "        B = idx.size(0)\n",
    "        # per-block feature caches\n",
    "        feat_states = [CausalPyramidStateLifted(model.config.n_scales, model.config.n_embd, device, batch_size=B)\n",
    "                       for _ in model.transformer.h]\n",
    "    \n",
    "        # 1) prime caches with the prompt (causal, one step at a time)\n",
    "        x_all = model.transformer.wte(idx)  # (B,T0,C); fixed embeddings in your code\n",
    "        for t in range(idx.size(1)):\n",
    "            x_t = x_all[:, t, :]\n",
    "            for blk, st in zip(model.transformer.h, feat_states):\n",
    "                x_t = blk.step(x_t, st)      # per-block step\n",
    "            # we discard logits during priming\n",
    "    \n",
    "        # 2) roll out new tokens\n",
    "        out = [idx]\n",
    "        cur = idx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # last token embedding\n",
    "            last_idx = cur[:, -1]                      # (B,)\n",
    "            x_t = model.transformer.wte(last_idx)      # (B,C)\n",
    "            for blk, st in zip(model.transformer.h, feat_states):\n",
    "                x_t = blk.step(x_t, st)                # (B,C)\n",
    "            logits = model.lm_head(x_t)                # (B,V)\n",
    "            next_idx = torch.argmax(logits, dim=-1, keepdim=True)  # greedy; swap to sampling if you like\n",
    "            out.append(next_idx)\n",
    "            cur = torch.cat([cur, next_idx], dim=1)\n",
    "            # keep only last block_size tokens in cur (typical AR convenience)\n",
    "            if cur.size(1) > block_size:\n",
    "                cur = cur[:, -block_size:]\n",
    "        return torch.cat(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFGVJvlN_yfW",
    "outputId": "f11f6493-0761-458d-9be0-4ebc604e53e1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading aochildes.txt...\n",
      "üì• Downloading cbt.txt...\n",
      "üì• Downloading children_stories.txt...\n",
      "üì• Downloading gutenberg.txt...\n",
      "üì• Downloading qed.txt...\n",
      "üì• Downloading simple_wikipedia.txt...\n",
      "üì• Downloading switchboard.txt...\n",
      "üì• Downloading wikipedia.txt...\n",
      "üì• Downloading shakespeare.txt...\n",
      "‚úÖ Done. Files saved to ./babylm_10m_cleaned\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"üì• Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"‚úÖ Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0fFuL2a_sAF",
    "outputId": "79c1170f-c818-4568-cf7b-939e02bc33e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Char tokenizer finalized.\n",
      "üßæ Train tokens: 1016242 | Val tokens: 99152\n",
      "üî§ Vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,#\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Char tokenizer finalized.\")\n",
    "print(f\"üßæ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"üî§ Vocab size: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g42l_Fa8_v9z",
    "outputId": "7bbfe691-e965-43cd-e53e-a4438ff8d7ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 2048\n",
    "batch_size = 8\n",
    "\n",
    "# === Load tokenizer metadata ===\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "\n",
    "# === Load mmap edata (char-level tokens, uint16) ===\n",
    "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
    "\n",
    "# === Efficient GPU Batch Sampler ===\n",
    "class GPUBatchDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size, batch_size, device, jitter=63, p_aligned=0.5, pad_len=0):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.pad_len = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len  # X length\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = self.total // self.sample_len\n",
    "        self.jitter = int(jitter)          # small random offset added to aligned start\n",
    "        self.p_aligned = float(p_aligned)  # mix aligned and jittered\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = np.empty((self.batch_size, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # choose a base aligned block\n",
    "            base_block = np.random.randint(0, self.n_blocks)\n",
    "            start = base_block * self.sample_len\n",
    "\n",
    "            # with probability, add a small jitter (keeps cache-friendly contiguous reads)\n",
    "            if np.random.rand() > self.p_aligned:\n",
    "                j = np.random.randint(0, self.jitter + 1)\n",
    "                start = min(start + j, self.total)  # stay in range\n",
    "\n",
    "            X[i] = self.data[start : start + self.sample_len]\n",
    "            # targets correspond to the final block_size visible steps\n",
    "            Y[i] = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + self.block_size]\n",
    "\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
    "            torch.from_numpy(Y).to(self.device, non_blocking=True)\n",
    "        )\n",
    "\n",
    "\n",
    "config = GPTConfig(\n",
    "    vocab_size=len(stoi),\n",
    "    n_layer=4,\n",
    "    n_embd=vocab_size,\n",
    "    block_size=block_size,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "train_dataset = GPUBatchDataset(train_ids, block_size, batch_size, device, pad_len=0)\n",
    "# === DataLoader ===\n",
    "train_loader  = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "model = GPT(config)\n",
    "model = torch.compile(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382140\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2650952339172363\n",
      "1.9930673837661743\n",
      "1.8950951099395752\n",
      "1.721228003501892\n",
      "1.693058967590332\n",
      "1.6537836790084839\n",
      "1.6259695291519165\n",
      "1.5851123332977295\n",
      "1.554604172706604\n",
      "1.5707091093063354\n",
      "1.5284051895141602\n",
      "1.5427179336547852\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m optimizer.zero_grad()\n\u001b[32m     10\u001b[39m it = it + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss = loss\n\u001b[32m     13\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:375\u001b[39m, in \u001b[36mOptimizedModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.nn.modules.module._has_any_global_hook():\n\u001b[32m    366\u001b[39m     warnings.warn(\n\u001b[32m    367\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `torch.compile(module)` when there are global hooks on \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodules (e.g., from `register_module_forward_hook`); this will\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    374\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:736\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    733\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 695\u001b[39m, in \u001b[36mGPT.forward\u001b[39m\u001b[34m(self, idx, targets, eprint)\u001b[39m\n\u001b[32m    693\u001b[39m x = \u001b[38;5;28mself\u001b[39m.transformer.wte(idx) \n\u001b[32m    694\u001b[39m x = x.detach()                 \u001b[38;5;66;03m# sever any stale history just in case\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m x.requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)         \u001b[38;5;66;03m# make x a grad leaf for œÑ at layer 0\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer.h:\n\u001b[32m    698\u001b[39m         x= block(x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 698\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_forward_at_695\u001b[39m\u001b[34m(___stack0, self, targets, x)\u001b[39m\n\u001b[32m    695\u001b[39m x.requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)         \u001b[38;5;66;03m# make x a grad leaf for œÑ at layer 0\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer.h:\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m         x= \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    702\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.lm_head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 431\u001b[39m, in \u001b[36mGPTSemanticBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# x: (B, T, C)\u001b[39;00m\n\u001b[32m    430\u001b[39m     B, T, C = x.shape\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     feats = \u001b[38;5;28mself\u001b[39m.features(x)               \u001b[38;5;66;03m# (B, T, K, C)\u001b[39;00m\n\u001b[32m    432\u001b[39m     x_expanded = x.unsqueeze(\u001b[32m2\u001b[39m)\n\u001b[32m    433\u001b[39m     \u001b[38;5;66;03m# Step 2: tile along K -> (B, T, K, C)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 431\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_forward_at_431\u001b[39m\u001b[34m(___stack0, self, x)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    429\u001b[39m     \u001b[38;5;66;03m# x: (B, T, C)\u001b[39;00m\n\u001b[32m    430\u001b[39m     B, T, C = x.shape\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     feats = \u001b[38;5;28mself\u001b[39m.features(x)               \u001b[38;5;66;03m# (B, T, K, C)\u001b[39;00m\n\u001b[32m    432\u001b[39m     x_expanded = x.unsqueeze(\u001b[32m2\u001b[39m)\n\u001b[32m    433\u001b[39m     \u001b[38;5;66;03m# Step 2: tile along K -> (B, T, K, C)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1241\u001b[39m, in \u001b[36maot_module_simplified.<locals>.forward\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1239\u001b[39m full_args.extend(params_flat)\n\u001b[32m   1240\u001b[39m full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:370\u001b[39m, in \u001b[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m    366\u001b[39m         torch.autograd._force_original_view_tracking(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    367\u001b[39m         torch.enable_grad(),\n\u001b[32m    368\u001b[39m     ):\n\u001b[32m    369\u001b[39m         record_runtime_wrapper_prologue_exit(cm)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         all_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001b[39;00m\n\u001b[32m    375\u001b[39m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[32m    378\u001b[39m     \u001b[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001b[39;00m\n\u001b[32m    379\u001b[39m     grad_enabled = torch.is_grad_enabled()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001b[39m, in \u001b[36mmake_boxed_func.<locals>.g\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mg\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:576\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    575\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2074\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001b[39m\u001b[34m(ctx, *deduped_flat_tensor_args)\u001b[39m\n\u001b[32m   2065\u001b[39m     args = (*args, *fwd_rng_states)\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2072\u001b[39m \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[32m   2073\u001b[39m \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2074\u001b[39m fw_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_fw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2080\u001b[39m num_outputs = CompiledFunction.metadata.num_outputs\n\u001b[32m   2081\u001b[39m num_outputs_aliased = CompiledFunction.metadata.num_outputs_aliased\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:556\u001b[39m, in \u001b[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    549\u001b[39m     out = \u001b[38;5;28mself\u001b[39m._functionalized_rng_runtime_epilogue(\n\u001b[32m    550\u001b[39m         runtime_metadata,\n\u001b[32m    551\u001b[39m         out,\n\u001b[32m    552\u001b[39m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    553\u001b[39m         runtime_metadata.num_forward_returns,\n\u001b[32m    554\u001b[39m     )\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/b4/cb4gmnwrho66jj7gdxhnfstm2v55pkfy6fra5yg5dwjyb2zoihe6.py:450\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    448\u001b[39m buf3 = empty_strided_cpu((\u001b[32m9\u001b[39m, \u001b[32m16384\u001b[39m, \u001b[32m33\u001b[39m), (\u001b[32m540672\u001b[39m, \u001b[32m33\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [h], Original ATen: [aten.bmm]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m \u001b[43mextern_kernels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m132\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m132\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1188\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimals_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m132\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m33\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4356\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m132\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuf3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m buf4 = empty_strided_cpu((\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m9\u001b[39m, \u001b[32m33\u001b[39m), (\u001b[32m608256\u001b[39m, \u001b[32m297\u001b[39m, \u001b[32m33\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n\u001b[32m    452\u001b[39m buf19 = empty_strided_cpu((\u001b[32m9\u001b[39m, \u001b[32m33\u001b[39m, \u001b[32m16384\u001b[39m), (\u001b[32m540672\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m33\u001b[39m), torch.float32)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    it = 0\n",
    "    for xb, yb in train_loader:\n",
    "          xb, yb = xb[0], yb[0]  # unwrap batch dimension\n",
    "          optimizer.zero_grad()\n",
    "          it = it + 1\n",
    "          logits, loss = model(xb, yb)\n",
    "          loss = loss\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "          optimizer.step()\n",
    "          total_loss += loss.item()\n",
    "          losses.append(loss.item())\n",
    "          if it%100==0: print(loss.item()) \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4945813417434692\n",
      "1.3901453018188477\n",
      "1.4406445026397705\n",
      "1.5177819728851318\n",
      "1.4712311029434204\n",
      "1.417723536491394\n",
      "1.3983992338180542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m logits, loss = model(xb, yb)\n\u001b[32m     12\u001b[39m loss = loss\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     15\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:311\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    310\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2259\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   2257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   2258\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2245\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimpl_fn\u001b[39m(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     out = \u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _backward_epilogue_functional(\n\u001b[32m   2247\u001b[39m         CompiledFunction.metadata,\n\u001b[32m   2248\u001b[39m         CompiledFunction.maybe_subclass_metadata,\n\u001b[32m   2249\u001b[39m         out,\n\u001b[32m   2250\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2376\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2360\u001b[39m     torch._functorch.config.donated_buffer\n\u001b[32m   2361\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[32m   2362\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata.bw_donated_idxs != []\n\u001b[32m   2363\u001b[39m ):\n\u001b[32m   2364\u001b[39m     torch._check(\n\u001b[32m   2365\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2366\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m   2373\u001b[39m         ),\n\u001b[32m   2374\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2376\u001b[39m out = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/g2/cg2frl4njpsmspjo4n33lvmehbk5a4ubt6pp4uy3st5putzmh7d5.py:637\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    635\u001b[39m buf16 = empty_strided_cpu((\u001b[32m9\u001b[39m, \u001b[32m16384\u001b[39m, \u001b[32m132\u001b[39m), (\u001b[32m2162688\u001b[39m, \u001b[32m132\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n\u001b[32m    636\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [], Original ATen: [aten.bmm]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m637\u001b[39m \u001b[43mextern_kernels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf14\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m16384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m33\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m33\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m297\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuf16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m buf14\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m primals_3\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_sequence_char_rolling(\n",
    "    model, stoi, itos, prompt,\n",
    "    max_new_tokens=100,\n",
    "    block_size=1024,\n",
    "    temperature=1.0,\n",
    "    space_fallback=' ',\n",
    "    strict_window=False,          # if True, periodically re-prime caches on the last block\n",
    "    reprime_every=None            # if strict_window, how often to re-prime (int). Default: block_size\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolling-block generator that:\n",
    "      - keeps the ENTIRE generated text (no trimming of output),\n",
    "      - maintains a rolling block window internally,\n",
    "      - optionally re-primes feature caches on the last `block_size` tokens to strictly\n",
    "        mimic block-window semantics seen during training.\n",
    "\n",
    "    If strict_window=False (default): fastest path; caches stream forever.\n",
    "    If strict_window=True: we periodically reinitialize the per-layer states using the\n",
    "      most recent `block_size` tokens. This ensures exact 'sliding window' behavior.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    B = 1\n",
    "\n",
    "    # ---- encode prompt (fallback to space if empty) ----\n",
    "    space_id = stoi.get(space_fallback, 0)\n",
    "    prompt_ids = encode_chars(prompt, stoi)\n",
    "    if len(prompt_ids) == 0:\n",
    "        prompt_ids = [space_id]\n",
    "\n",
    "    # ---- left-pad ONCE to match your training forward's left-pad-to-block ----\n",
    "    pad_len = max(0, block_size - len(prompt_ids))\n",
    "    pad_ids = [space_id] * pad_len\n",
    "    priming_ids = pad_ids + prompt_ids  # padding only used for priming; not returned\n",
    "\n",
    "    # ---- per-block feature caches (one state per block) ----\n",
    "    feat_states = [\n",
    "        CausalPyramidStateLifted(\n",
    "            num_scales=model.config.n_scales,\n",
    "            C=model.config.n_embd,\n",
    "            device=device,\n",
    "            batch_size=B,\n",
    "            tau=1e-6\n",
    "        ) for _ in model.transformer.h\n",
    "    ]\n",
    "\n",
    "    # helper: (re-)prime caches with a sequence of token ids (left-pad to block if shorter)\n",
    "    def _reprime_with_ids(tok_ids):\n",
    "        # optionally left-pad the window up to block_size (only needed if strict semantics desired)\n",
    "        if len(tok_ids) < block_size:\n",
    "            tok_ids = [space_id] * (block_size - len(tok_ids)) + tok_ids\n",
    "        ids_t = torch.tensor([tok_ids], dtype=torch.long, device=device)  # (1, T)\n",
    "        x_last = None\n",
    "        # fresh states\n",
    "        new_states = [\n",
    "            CausalPyramidStateLifted(\n",
    "                num_scales=model.config.n_scales,\n",
    "                C=model.config.n_embd,\n",
    "                device=device,\n",
    "                batch_size=B,\n",
    "                tau=1e-6\n",
    "            ) for _ in model.transformer.h\n",
    "        ]\n",
    "        for t in range(ids_t.size(1)):\n",
    "            x_last = model.transformer.wte(ids_t[:, t])  # (1,C)\n",
    "            for blk, st in zip(model.transformer.h, new_states):\n",
    "                x_last = blk.step(x_last, st)\n",
    "        return new_states, x_last\n",
    "\n",
    "    # ---- initial priming with left-padded prompt ----\n",
    "    ids = torch.tensor([priming_ids], dtype=torch.long, device=device)\n",
    "    x_t = None\n",
    "    for t in range(ids.size(1)):\n",
    "        x_t = model.transformer.wte(ids[:, t])  # (1,C)\n",
    "        for blk, st in zip(model.transformer.h, feat_states):\n",
    "            x_t = blk.step(x_t, st)\n",
    "\n",
    "    # ---- FULL output accumulator (never trimmed) ----\n",
    "    out_full = list(prompt_ids)  # store ints\n",
    "\n",
    "    # ---- rolling window buffer of last block_size tokens (prompt + generated) ----\n",
    "    window = deque(prompt_ids, maxlen=block_size)\n",
    "\n",
    "    # strict-window settings\n",
    "    if reprime_every is None:\n",
    "        reprime_every = block_size\n",
    "    steps_since_reprime = 0\n",
    "\n",
    "    # ---- incremental rollout ----\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.lm_head(x_t)  # (1,V)\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / float(temperature)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "        next_id = int(next_token.item())\n",
    "\n",
    "        # record full output\n",
    "        out_full.append(next_id)\n",
    "\n",
    "        # advance rolling window\n",
    "        window.append(next_id)\n",
    "\n",
    "        # step one token\n",
    "        x_t = model.transformer.wte(next_token.squeeze(-1))  # (1,C)\n",
    "        for blk, st in zip(model.transformer.h, feat_states):\n",
    "            x_t = blk.step(x_t, st)\n",
    "\n",
    "        # optionally re-prime to strict sliding-window semantics\n",
    "        if strict_window:\n",
    "            steps_since_reprime += 1\n",
    "            if steps_since_reprime >= reprime_every and len(window) == block_size:\n",
    "                feat_states, x_t = _reprime_with_ids(list(window))\n",
    "                steps_since_reprime = 0\n",
    "\n",
    "    # decode full continuation (prompt + all generated)\n",
    "    return decode_chars(out_full, itos)\n",
    "    \n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "import time\n",
    "then = time.time()\n",
    "prompt = \"Juliet! My Juliet! come forth to me! \"\n",
    "generated = decode_sequence_char_rolling(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=4096,\n",
    "    block_size=2048,\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "print(generated)\n",
    "print(time.time()-then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.arange(128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "UzqWUbNRlmiD"
   },
   "outputs": [],
   "source": [
    "file_path = 'simple_model_tiny.pth'\n",
    "\n",
    "# 3. Save the model's state_dict\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
