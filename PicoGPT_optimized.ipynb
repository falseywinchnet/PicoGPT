{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copyright joshuah.rainstar@gmail.com\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# ===========================================================\n",
    "# Utilities\n",
    "# ===========================================================\n",
    "\n",
    "def _norm(v, eps: float = 1e-12):\n",
    "    return torch.linalg.vector_norm(v, dim=-1, keepdim=True).clamp_min(eps)\n",
    "\n",
    "\n",
    "def _unit(v, eps: float = 1e-12):\n",
    "    return v / _norm(v, eps)\n",
    "\n",
    "    \n",
    "@torch.no_grad()\n",
    "def phase_transport_between(\n",
    "    curr: torch.Tensor,\n",
    "    prev: torch.Tensor,\n",
    "    tau: float = 1e-6,          # semantic threshold (unchanged)\n",
    "    eps: float = 1e-12          # numeric epsilon (NEW: decoupled from tau)\n",
    ") -> torch.Tensor:\n",
    "    assert curr.shape == prev.shape and curr.dim() == 3\n",
    "    B, T, C = curr.shape\n",
    "\n",
    "    # Units (reuse norms) — clamp with eps (NOT tau)\n",
    "    nu = torch.linalg.vector_norm(curr, dim=-1, keepdim=True).clamp_min(eps)   # (B,T,1)\n",
    "    nv = torch.linalg.vector_norm(prev, dim=-1, keepdim=True).clamp_min(eps)   # (B,T,1)\n",
    "    u = curr / nu\n",
    "    v = prev / nv\n",
    "\n",
    "    w = curr - prev\n",
    "    c = (u * v).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "\n",
    "    # Masks (semantic thresholds use tau)\n",
    "    near_pos = (c >  1.0 - tau)                                                # (B,T,1)\n",
    "    near_neg = (c < -1.0 + tau)                                                # (B,T,1)\n",
    "    small_u  = (nu < tau)                                                      # (B,T,1)\n",
    "    small_v  = (nv < tau)                                                      # (B,T,1)\n",
    "    trivial  = near_pos | small_u | small_v                                    # (B,T,1)\n",
    "\n",
    "    # General branch\n",
    "    denom = (1.0 + c).clamp_min(eps)                                           # (B,T,1)\n",
    "    a = (v * w).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "    b = (u * w).sum(dim=-1, keepdim=True)                                      # (B,T,1)\n",
    "    Kw  = u * a - v * b                                                        # (B,T,C)\n",
    "    K2w = u * (a * c - b) + v * (b * c - a)                                    # (B,T,C)\n",
    "    y_gen = w - Kw + (K2w / denom)                                             # (B,T,C)\n",
    "\n",
    "    # Antipodal candidate\n",
    "    if C == 1:\n",
    "        y_neg = -w\n",
    "    else:\n",
    "        # Keep this normalization stable with eps as well\n",
    "        idx = torch.argmin(v.abs().reshape(-1, C), dim=1, keepdim=True)        # (B*T,1)\n",
    "        s = v.reshape(-1, C).gather(1, idx)                                    # (B*T,1)\n",
    "        p = -s * v.reshape(-1, C)\n",
    "        onehot = F.one_hot(idx.squeeze(-1), num_classes=C).to(s.dtype)\n",
    "        p = p + onehot\n",
    "        n = torch.linalg.vector_norm(p, dim=1, keepdim=True).clamp_min(eps)\n",
    "        p = (p / n).view(B, T, C)\n",
    "        proj_v = (v * w).sum(dim=-1, keepdim=True) * v                         # (B,T,C)\n",
    "        proj_p = (p * w).sum(dim=-1, keepdim=True) * p                         # (B,T,C)\n",
    "        y_neg = w - 2.0 * proj_v - 2.0 * proj_p\n",
    "\n",
    "    # Fuse selections\n",
    "    y = torch.where(trivial, w, y_gen)\n",
    "    y = torch.where(near_neg, y_neg, y)\n",
    "    return y\n",
    "\n",
    "# ===========================================================\n",
    "# Multi-scale features (vectorized pyramid)\n",
    "# ===========================================================\n",
    "class CausalCentroidPyramid(nn.Module):\n",
    "    \"\"\"Identical outputs to CausalCentroidPyramid, but faster.\n",
    "\n",
    "    Key changes:\n",
    "    - Builds all dyadic centroids directly via cumsum (no sequential dependency).\n",
    "    - Computes all cluster deltas in a single batched call to phase_transport_between.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, tau: float = 1e-6):\n",
    "        super().__init__()\n",
    "        assert num_scales >= 1\n",
    "        self.K = num_scales\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor, mask_early: bool = True) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "    \n",
    "        # token-level PT (scale-1)\n",
    "        prev_tok = torch.zeros_like(x)\n",
    "        if T > 1:\n",
    "            prev_tok[:, 1:, :] = x[:, :-1, :].contiguous()\n",
    "        d1 = phase_transport_between(x, prev_tok, tau=self.tau)  # (B,T,C)\n",
    "        if mask_early:\n",
    "            d1[:, :1, :].zero_()\n",
    "        if self.K == 1:\n",
    "            return d1.unsqueeze(2)\n",
    "    \n",
    "        # constants (avoid .item() / data-dependent Python ints)\n",
    "        K1 = self.K - 1\n",
    "        W_vec = (2 ** torch.arange(1, self.K, device=device, dtype=torch.long))  # (K1,)\n",
    "        Wmax = (1 << (self.K - 1)) if self.K > 1 else 1  # Python int\n",
    "    \n",
    "        # dyadic centroids via windowed means (vectorized)\n",
    "        csum = torch.cumsum(x, dim=1)  # (B,T,C)\n",
    "        csum_pad = torch.cat([torch.zeros(B, 1, C, device=device, dtype=dtype), csum], dim=1)  # (B,T+1,C)\n",
    "    \n",
    "        t_end = torch.arange(1, T + 1, device=device, dtype=torch.long)                         # (T,)\n",
    "        idx_start_jt = (t_end.unsqueeze(0) - W_vec.unsqueeze(1)).clamp_min(0)                  # (K1,T)\n",
    "        idx_start_tk = idx_start_jt.transpose(0, 1).contiguous()                                # (T,K1)\n",
    "        idx_end_tk = t_end.unsqueeze(1).expand(T, K1).contiguous()                              # (T,K1)\n",
    "    \n",
    "        csum_ext = csum_pad.unsqueeze(2).expand(B, T + 1, K1, C)                                # (B,T+1,K1,C)\n",
    "    \n",
    "        gather_shape = (B, T, K1, C)\n",
    "        idx_start = idx_start_tk.unsqueeze(0).unsqueeze(-1).expand(gather_shape)                # (B,T,K1,C)\n",
    "        idx_end = idx_end_tk.unsqueeze(0).unsqueeze(-1).expand(gather_shape)                    # (B,T,K1,C)\n",
    "    \n",
    "        start_vals = torch.gather(csum_ext, dim=1, index=idx_start)\n",
    "        end_vals = torch.gather(csum_ext, dim=1, index=idx_end)\n",
    "        window_sums = end_vals - start_vals                                                     # (B,T,K1,C)\n",
    "        mu_all = window_sums / W_vec.to(dtype).view(1, 1, -1, 1)                                # (B,T,K1,C)\n",
    "    \n",
    "        if mask_early:\n",
    "            t_idx = torch.arange(T, device=device).unsqueeze(1)                                 # (T,1)\n",
    "            valid_mu = (t_idx >= (W_vec - 1).view(1, -1))                                       # (T,K1)\n",
    "            mu_all = mu_all * valid_mu.view(1, T, -1, 1)\n",
    "    \n",
    "        # previous centroids (shift by W per scale), vectorized with padding\n",
    "        mu_pad = torch.cat([torch.zeros(B, Wmax, K1, C, device=device, dtype=dtype), mu_all], dim=1)  # (B,Wmax+T,K1,C)\n",
    "        idx_prev_tk = torch.arange(T, device=device).unsqueeze(1) - W_vec.view(1, -1) + Wmax          # (T,K1)\n",
    "        idx_prev = idx_prev_tk.unsqueeze(0).unsqueeze(-1).expand(gather_shape)                        # (B,T,K1,C)\n",
    "        prev_mu_all = torch.gather(mu_pad, dim=1, index=idx_prev)                                     # (B,T,K1,C)\n",
    "    \n",
    "        # all cluster deltas in one batched PT call\n",
    "        mu_flat = mu_all.reshape(B * K1, T, C).contiguous()\n",
    "        prev_flat = prev_mu_all.reshape(B * K1, T, C).contiguous()\n",
    "        d_flat = phase_transport_between(mu_flat, prev_flat, tau=self.tau)                            # (B*K1,T,C)\n",
    "        d_clusters = d_flat.view(B, T, K1, C)\n",
    "    \n",
    "        if mask_early:\n",
    "            valid_d = (torch.arange(T, device=device).unsqueeze(1) >= W_vec.view(1, -1))              # (T,K1)\n",
    "            d_clusters = d_clusters * valid_d.view(1, T, -1, 1)\n",
    "    \n",
    "        return torch.cat([d1.unsqueeze(2), d_clusters], dim=2)  # (B,T,K,C)\n",
    "\n",
    "        # ----- STREAMING STATE FOR INFERENCE -----\n",
    "class CausalPyramidState:\n",
    "    \"\"\"\n",
    "    O(K) step-time updates, no recompute.\n",
    "    For level ℓ we keep a ring buffer of length 2^ℓ storing μ_ℓ (with μ_0=x).\n",
    "    That suffices both to:\n",
    "      - build μ_{ℓ+1}(t) from μ_ℓ(t) and μ_ℓ(t-2^ℓ)\n",
    "      - compute deltas at scale s=ℓ via μ_s(t-2^s)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, C: int, device, batch_size: int = 1, tau: float = 1e-6):\n",
    "        self.K = num_scales\n",
    "        self.C = C\n",
    "        self.B = batch_size\n",
    "        self.device = device\n",
    "        self.tau = float(tau)\n",
    "        self.t = 0  # number of tokens processed so far\n",
    "\n",
    "        # ring buffers: list over levels ℓ = 0..K-1, each [B, L=2^ℓ, C]\n",
    "        self.buffers = []\n",
    "        self.ptrs = []\n",
    "        for l in range(self.K):\n",
    "            L = 1 << l\n",
    "            self.buffers.append(torch.zeros(self.B, L, C, device=device))\n",
    "            self.ptrs.append(0)\n",
    "\n",
    "    def _read_lookback(self, level: int, r: int):\n",
    "        \"\"\"return μ_level(t - r); zeros if not enough history yet\"\"\"\n",
    "        if self.t < r:\n",
    "            return torch.zeros(self.B, self.C, device=self.device)\n",
    "        L = self.buffers[level].size(1)\n",
    "        idx = (self.ptrs[level] - r) % L\n",
    "        return self.buffers[level][:, idx, :]\n",
    "\n",
    "    def _push(self, level: int, value: torch.Tensor):\n",
    "        \"\"\"write current μ_level(t) and advance ptr\"\"\"\n",
    "        L = self.buffers[level].size(1)\n",
    "        self.buffers[level][:, self.ptrs[level], :] = value\n",
    "        self.ptrs[level] = (self.ptrs[level] + 1) % L\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x_t: (B, C)\n",
    "        returns d(t): (B, K, C)  [token PT + (K-1) cluster PTs]\n",
    "        \"\"\"\n",
    "        B, C = x_t.shape\n",
    "        feats = []\n",
    "\n",
    "        # ------- token PT (read BEFORE any push) -------\n",
    "        prev_x = self._read_lookback(level=0, r=1)  # μ0(t-1)\n",
    "        d1 = phase_transport_between(x_t[:, None, :], prev_x[:, None, :], tau=self.tau).squeeze(1)\n",
    "        if self.t == 0:\n",
    "            d1.zero_()\n",
    "        feats.append(d1)\n",
    "\n",
    "        # ------- (A) compute all μ_s(t) with pre-push lookbacks -------\n",
    "        mu_curr = [None] * self.K\n",
    "        mu_curr[0] = x_t                      # μ0(t)\n",
    "        mu_prev = x_t\n",
    "        for s in range(1, self.K):\n",
    "            W1 = 1 << (s - 1)\n",
    "            W  = 1 << s\n",
    "            mu_back = self._read_lookback(level=s-1, r=W1)   # μ_{s-1}(t - 2^{s-1})  (pre-push!)\n",
    "            mu_s_t  = 0.5 * (mu_prev + mu_back)              # μ_s(t)\n",
    "            if self.t < (W - 1):                             # early mask (global t)\n",
    "                mu_s_t.zero_()\n",
    "            mu_curr[s] = mu_s_t\n",
    "            mu_prev = mu_s_t\n",
    "\n",
    "        # ------- (B) compute all deltas d_s using μ_s(t−W) (pre-push) -------\n",
    "        for s in range(1, self.K):\n",
    "            W = 1 << s\n",
    "            mu_prevW = self._read_lookback(level=s, r=W)     # μ_s(t - 2^s)  (pre-push!)\n",
    "            d_s = phase_transport_between(mu_curr[s][:, None, :], mu_prevW[:, None, :], tau=self.tau).squeeze(1)\n",
    "            if self.t + 1 <= W:\n",
    "                d_s.zero_()\n",
    "            feats.append(d_s)\n",
    "\n",
    "        # ------- (C) push μ_ℓ(t) for all levels, exactly once -------\n",
    "        self._push(level=0, value=mu_curr[0])\n",
    "        for s in range(1, self.K):\n",
    "            self._push(level=s, value=mu_curr[s])\n",
    "\n",
    "        self.t += 1\n",
    "        return torch.stack(feats, dim=1)  # (B, K, C)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SemanticClusterFeaturesCausal(nn.Module):\n",
    "    \"\"\"\n",
    "    Unified wrapper:\n",
    "      - forward(x): vectorized for training\n",
    "      - step(x_t, state): single-step for inference with cache\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, tau: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.pyramid = CausalCentroidPyramid(num_scales=num_scales, tau=tau)\n",
    "        self.K = num_scales\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pyramid(x)  # (B,T,K,C)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor, state: CausalPyramidState) -> torch.Tensor:\n",
    "        return state.step(x_t)  # (B,K,C)\n",
    "\n",
    "\n",
    "class GroupedChannelMLP(nn.Module):\n",
    "    def __init__(self, k_dim: int, c_dim: int):\n",
    "        super().__init__()\n",
    "        hidden_dim = c_dim // 2\n",
    "        self.k_dim = k_dim\n",
    "        self.c_dim = c_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # shapes chosen for direct einsum without expands\n",
    "        # fc1: (K, H, C)   fc2: (K, C, H)   b2: (K, C)\n",
    "        self.fc1_weight = nn.Parameter(torch.empty(k_dim, hidden_dim, c_dim))\n",
    "        self.fc2_weight = nn.Parameter(torch.empty(k_dim, c_dim, hidden_dim))\n",
    "        self.fc2_bias   = nn.Parameter(torch.empty(k_dim, c_dim))\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.fc1_weight, a=5**0.5)\n",
    "        nn.init.kaiming_uniform_(self.fc2_weight, a=5**0.5)\n",
    "        nn.init.zeros_(self.fc2_bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T, K, C) or (B, K, C)\n",
    "        returns: same leading dims, last two dims (K,C)\n",
    "        \"\"\"\n",
    "        squeeze_time = False\n",
    "        if x.dim() == 3:  # (B,K,C)\n",
    "            x = x.unsqueeze(1)  # -> (B,1,K,C)\n",
    "            squeeze_time = True\n",
    "        elif x.dim() != 4:\n",
    "            raise ValueError(\"Input must be (B,K,C) or (B,T,K,C)\")\n",
    "\n",
    "        # (B,T,K,C) x (K,H,C) -> (B,T,K,H)\n",
    "        h = torch.einsum('btkc,khc->btkh', x, self.fc1_weight)\n",
    "        h = F.gelu(h)\n",
    "\n",
    "        # (B,T,K,H) x (K,C,H) -> (B,T,K,C)\n",
    "        y = torch.einsum('btkh,kch->btkc', h, self.fc2_weight) + self.fc2_bias\n",
    "\n",
    "        if squeeze_time:\n",
    "            y = y[:, 0, :, :]  # (B,K,C)\n",
    "        return y\n",
    "        \n",
    "        \n",
    "class Cell(nn.Module):\n",
    "    def __init__(self, dim_in: int, hidden: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, hidden, bias=False) #dont change, false intentional\n",
    "        self.fc2 = nn.Linear(hidden, dim_in, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, x):\n",
    "      \n",
    "        return self.fc2(self.act(self.fc1(x))) \n",
    "\n",
    "class GPTSemanticBlock(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        C = config.n_embd\n",
    "        self.C = C\n",
    "        self.K = config.n_scales\n",
    "        # L = number of feature groups concatenated: token (1) + K scales\n",
    "        self.L = 1 + self.K\n",
    "        self.features = SemanticClusterFeaturesCausal(num_scales=self.K, tau=1e-6)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.ln = nn.LayerNorm(self.C)\n",
    "        self.mlp = Cell(self.C,self.C*2)\n",
    "\n",
    "        # Each bottleneck maps C -> small_hidden -> C\n",
    "        self.bottleneck = GroupedChannelMLP(self.K, self.C)\n",
    "\n",
    "    # vectorized\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        feats = self.features(x)               # (B, T, K, C)\n",
    "        feats = self.bottleneck(feats) # (B, T, K, C)#bottlenecked\n",
    "        feats= feats.sum(dim=2)\n",
    "        # concat token embedding with processed features\n",
    "        x_in = x + feats\n",
    "        out = x + self.drop(self.ln(self.mlp(x_in)))\n",
    "\n",
    "        return out\n",
    "\n",
    "    # single-step incremental\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor, feat_state: CausalPyramidState) -> torch.Tensor:\n",
    "        # x_t: (B, C)\n",
    "        B, C = x_t.shape\n",
    "        feats_t = self.features.step(x_t, feat_state)  # (B, K, C)\n",
    "        feats_t = self.bottleneck(feats_t)\n",
    "        feats_t= feats_t.sum(dim=1)\n",
    "\n",
    "        x_in = x_t+feats_t     # (B, (1+K)*C)\n",
    "        out = x_t + self.drop(self.ln(self.mlp(x_in)))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def _is_prime(n: int) -> bool:\n",
    "    if n < 2: return False\n",
    "    if n % 2 == 0: return n == 2\n",
    "    r = int(n**0.5)\n",
    "    for f in range(3, r+1, 2):\n",
    "        if n % f == 0: return False\n",
    "    return True\n",
    "\n",
    "def _factorize(n: int):\n",
    "    f, cnt = [], {}\n",
    "    d = 2\n",
    "    while d * d <= n:\n",
    "        while n % d == 0:\n",
    "            cnt[d] = cnt.get(d, 0) + 1\n",
    "            n //= d\n",
    "        d += 1 if d == 2 else 2\n",
    "    if n > 1: cnt[n] = cnt.get(n, 0) + 1\n",
    "    return list(cnt.keys())\n",
    "\n",
    "def _primitive_root(p: int) -> int:\n",
    "    # p must be prime\n",
    "    phi = p - 1\n",
    "    factors = _factorize(phi)\n",
    "    for g in range(2, p):\n",
    "        ok = True\n",
    "        for q in factors:\n",
    "            if pow(g, phi // q, p) == 1:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            return g\n",
    "    raise RuntimeError(\"no primitive root found\")\n",
    "\n",
    "def _welch_costas_perm(V: int, device=None):\n",
    "    \"\"\"\n",
    "    Welch Costas permutation σ on {0..V-1}, where V = p-1 for prime p.\n",
    "    σ[i] = g^(i+1) mod p, mapped to 0..V-1 by subtracting 1.\n",
    "    \"\"\"\n",
    "    p = V + 1\n",
    "    if not _is_prime(p):\n",
    "        return None\n",
    "    g = _primitive_root(p)\n",
    "    sigma = torch.empty(V, dtype=torch.long, device=device)\n",
    "    for i in range(V):\n",
    "        sigma[i] = pow(g, i + 1, p) - 1\n",
    "    return sigma  # permutation of 0..V-1\n",
    "\n",
    "def _coprime_mul_perm(V: int, device=None):\n",
    "    \"\"\"\n",
    "    Fallback: σ[i] = (a*i + b) % V with gcd(a, V)=1 and a not ≡ ±1 mod V.\n",
    "    Not Costas, but non-monotone and well-distributed.\n",
    "    \"\"\"\n",
    "    # pick a\n",
    "    a = None\n",
    "    for cand in range(2, V):\n",
    "        if math.gcd(cand, V) == 1 and cand % V not in (1, V-1):\n",
    "            a = cand\n",
    "            break\n",
    "    if a is None:\n",
    "        a = 1  # degenerate small V\n",
    "    b = V // 3\n",
    "    i = torch.arange(V, device=device)\n",
    "    return ((a * i + b) % V).long()\n",
    "\n",
    "def _perm_inverse(sigma: torch.Tensor) -> torch.Tensor:\n",
    "    inv = torch.empty_like(sigma)\n",
    "    inv[sigma] = torch.arange(sigma.numel(), device=sigma.device)\n",
    "    return inv\n",
    "\n",
    "class FlatRollEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Replacement for nn.Embedding that maps token id i -> cyclic roll^i of a base\n",
    "    length-V vector whose non-DC spectrum is flat (DC=0). Requires V == n_embd.\n",
    "    Weights are frozen by default.\n",
    "    The 'eye' is mixed at 0.5 and then rows are permuted by a Costas-like order\n",
    "    to maximize uniqueness while keeping even collapse.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config, scale: str = \"box\", seed: int = 0,\n",
    "                 freeze: bool = True, dtype=None, device=None):\n",
    "        super().__init__()\n",
    "        assert config.n_embd == config.vocab_size, (\n",
    "            f\"Expected n_embd == vocab_size, got {config.n_embd} != {config.vocab_size}\"\n",
    "        )\n",
    "        V = int(config.vocab_size)\n",
    "        dtype = dtype or torch.float32\n",
    "\n",
    "        eye = torch.eye(V, dtype=dtype, device=device)\n",
    "        weight = self._make_weight(V, scale=scale, seed=seed,\n",
    "                                   dtype=dtype, device=device)  # [V, V]\n",
    "        mixed = 0.5 * weight + 0.5 * eye  # add identity towers\n",
    "\n",
    "        # --- compute a strong-scramble row order (Costas if possible) ---\n",
    "        sigma = _welch_costas_perm(V, device=device)\n",
    "        if sigma is None:\n",
    "            sigma = _coprime_mul_perm(V, device=device)\n",
    "        # We want ones at (row = σ[i], col = i). For row-permutation via index_select,\n",
    "        # use r_idx = σ^{-1} so that new_row j pulls old_row r_idx[j] with 1 at column j=σ[i].\n",
    "        r_idx = _perm_inverse(sigma)\n",
    "\n",
    "        # keep for reference / decoding\n",
    "        self.register_buffer(\"row_perm\", r_idx, persistent=False)\n",
    "        self.register_buffer(\"sigma\", sigma, persistent=False)\n",
    "\n",
    "        mixed = mixed.index_select(0, r_idx)\n",
    "        self.embed = nn.Embedding.from_pretrained(mixed, freeze=freeze)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _row_perm_max_equidistant(V: int, device=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Row permutation that evenly offsets the identity's '1' away from the diagonal.\n",
    "        Uses a single cyclic shift by k = floor(V/2).\n",
    "        \"\"\"\n",
    "        if V <= 1:\n",
    "            return torch.arange(V, device=device, dtype=torch.long)\n",
    "        k = V // 2\n",
    "        if k == 0:  # only happens when V == 1, handled above; keep for safety\n",
    "            k = 1\n",
    "        return ((torch.arange(V, device=device) + k) % V).long()\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_weight(V: int, scale: str = \"box\", seed: int = 0,\n",
    "                     dtype=torch.float32, device=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns a (V, V) tensor whose rows are cyclic rolls of a base vector x in R^V\n",
    "        with |FFT(x)|^2 flat for k=1..V-1 and DC=0.\n",
    "        scale:\n",
    "          - \"unit\": ||x||_2 = 1\n",
    "          - \"box\":  max|x_i| = 1\n",
    "        \"\"\"\n",
    "        # build on CPU, move at end\n",
    "        complex_dtype = torch.complex64 if dtype == torch.float32 else torch.complex128\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "        X = torch.zeros(V, dtype=complex_dtype)\n",
    "        # DC bin\n",
    "        X[0] = torch.tensor(0, dtype=complex_dtype)\n",
    "\n",
    "        if V % 2 == 0:\n",
    "            # bins 1..V/2-1 are complex-conjugate pairs; Nyquist bin must be real\n",
    "            for k in range(1, V // 2):\n",
    "                phi = torch.rand((), generator=g) * (2 * math.pi)\n",
    "                val = torch.cos(phi) + 1j * torch.sin(phi)\n",
    "                X[k] = val\n",
    "                X[V - k] = torch.conj(val)\n",
    "            X[V // 2] = 1.0 if torch.rand((), generator=g) < 0.5 else -1.0\n",
    "        else:\n",
    "            for k in range(1, (V - 1) // 2 + 1):\n",
    "                phi = torch.rand((), generator=g) * (2 * math.pi)\n",
    "                val = torch.cos(phi) + 1j * torch.sin(phi)\n",
    "                X[k] = val\n",
    "                X[V - k] = torch.conj(val)\n",
    "\n",
    "        x = torch.fft.ifft(X).real  # real length-V base vector\n",
    "\n",
    "        if scale == \"unit\":\n",
    "            x = x / (x.norm() + 1e-12)\n",
    "        elif scale == \"box\":\n",
    "            x = x / (x.abs().max() + 1e-12)\n",
    "        else:\n",
    "            raise ValueError(\"scale must be 'unit' or 'box'\")\n",
    "\n",
    "        rows = [torch.roll(x, shifts=r, dims=0) for r in range(V)]\n",
    "        W = torch.stack(rows, dim=0).to(dtype=dtype)\n",
    "        if device is not None:\n",
    "            W = W.to(device)\n",
    "        return W\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        # (batch, seq_len, V)\n",
    "        return self.embed(input_ids)\n",
    "\n",
    "\n",
    "        \n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 66 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 6\n",
    "    n_head:int = 6\n",
    "    n_embd: int = 128\n",
    "    n_scales:int = 9\n",
    "    dropout: float = 0.1\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.n_embd = config.n_embd\n",
    "        self.drop = nn.Dropout(0.6)\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = FlatRollEmbed(config),\n",
    "            h = nn.ModuleList([GPTSemanticBlock(config) for _ in range(config.n_layer)]),\n",
    "\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(self.config.n_embd, self.config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.transformer.wte.embed.weight\n",
    "        #TODO- ADD A LEARNED, tied gate that regulate both LMhead and flatrollembed,\n",
    "        #such that the mixture between eye and convolution-even generation is \n",
    "        #conditionally between 100% eye- strong individuality, risk of neighbors\n",
    "        #100% even-convolution- strong generalization, risk of chaos\n",
    "        #model may benefit from initially learning with one, then moving to the other\n",
    "        #thus more rapidly acquiring meaningful structure\n",
    "\n",
    "\n",
    "    # ---------- forward ----------\n",
    "    def forward(self, idx, targets=None, eprint=False):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        x = self.transformer.wte(idx) \n",
    "        x = x.detach()                 # sever any stale history just in case\n",
    "        x.requires_grad_(True)         # make x a grad leaf for τ at layer 0\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "                x= block(x)\n",
    "\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_greedy(model: nn.Module, idx: torch.LongTensor, max_new_tokens: int, block_size: int):\n",
    "        \"\"\"\n",
    "        model: your GPT with:\n",
    "           - transformer.wte (embedding)\n",
    "           - transformer.h : list[GPTSemanticBlock]\n",
    "           - lm_head\n",
    "        idx: (B, T0) prompt token ids\n",
    "        \"\"\"\n",
    "        device = next(model.parameters()).device\n",
    "        B = idx.size(0)\n",
    "        # per-block feature caches\n",
    "        feat_states = [CausalPyramidState(model.config.n_scales, model.config.n_embd, device, batch_size=B)\n",
    "                       for _ in model.transformer.h]\n",
    "    \n",
    "        # 1) prime caches with the prompt (causal, one step at a time)\n",
    "        x_all = model.transformer.wte(idx)  # (B,T0,C); fixed embeddings in your code\n",
    "        for t in range(idx.size(1)):\n",
    "            x_t = x_all[:, t, :]\n",
    "            for blk, st in zip(model.transformer.h, feat_states):\n",
    "                x_t = blk.step(x_t, st)      # per-block step\n",
    "            # we discard logits during priming\n",
    "    \n",
    "        # 2) roll out new tokens\n",
    "        out = [idx]\n",
    "        cur = idx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # last token embedding\n",
    "            last_idx = cur[:, -1]                      # (B,)\n",
    "            x_t = model.transformer.wte(last_idx)      # (B,C)\n",
    "            for blk, st in zip(model.transformer.h, feat_states):\n",
    "                x_t = blk.step(x_t, st)                # (B,C)\n",
    "            logits = model.lm_head(x_t)                # (B,V)\n",
    "            next_idx = torch.argmax(logits, dim=-1, keepdim=True)  # greedy; swap to sampling if you like\n",
    "            out.append(next_idx)\n",
    "            cur = torch.cat([cur, next_idx], dim=1)\n",
    "            # keep only last block_size tokens in cur (typical AR convenience)\n",
    "            if cur.size(1) > block_size:\n",
    "                cur = cur[:, -block_size:]\n",
    "        return torch.cat(out, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFGVJvlN_yfW",
    "outputId": "f11f6493-0761-458d-9be0-4ebc604e53e1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading aochildes.txt...\n",
      "📥 Downloading cbt.txt...\n",
      "📥 Downloading children_stories.txt...\n",
      "📥 Downloading gutenberg.txt...\n",
      "📥 Downloading qed.txt...\n",
      "📥 Downloading simple_wikipedia.txt...\n",
      "📥 Downloading switchboard.txt...\n",
      "📥 Downloading wikipedia.txt...\n",
      "📥 Downloading shakespeare.txt...\n",
      "✅ Done. Files saved to ./babylm_10m_cleaned\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"📥 Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"❌ Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"✅ Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0fFuL2a_sAF",
    "outputId": "79c1170f-c818-4568-cf7b-939e02bc33e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Char tokenizer finalized.\n",
      "🧾 Train tokens: 1016242 | Val tokens: 99152\n",
      "🔤 Vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,#\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"✅ Char tokenizer finalized.\")\n",
    "print(f\"🧾 Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"🔤 Vocab size: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g42l_Fa8_v9z",
    "outputId": "7bbfe691-e965-43cd-e53e-a4438ff8d7ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 2048\n",
    "batch_size = 8\n",
    "\n",
    "# === Load tokenizer metadata ===\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "\n",
    "# === Load mmap edata (char-level tokens, uint16) ===\n",
    "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
    "\n",
    "# === Efficient GPU Batch Sampler ===\n",
    "class GPUBatchDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size, batch_size, device, jitter=63, p_aligned=0.5, pad_len=0):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.pad_len = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len  # X length\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = self.total // self.sample_len\n",
    "        self.jitter = int(jitter)          # small random offset added to aligned start\n",
    "        self.p_aligned = float(p_aligned)  # mix aligned and jittered\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = np.empty((self.batch_size, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # choose a base aligned block\n",
    "            base_block = np.random.randint(0, self.n_blocks)\n",
    "            start = base_block * self.sample_len\n",
    "\n",
    "            # with probability, add a small jitter (keeps cache-friendly contiguous reads)\n",
    "            if np.random.rand() > self.p_aligned:\n",
    "                j = np.random.randint(0, self.jitter + 1)\n",
    "                start = min(start + j, self.total)  # stay in range\n",
    "\n",
    "            X[i] = self.data[start : start + self.sample_len]\n",
    "            # targets correspond to the final block_size visible steps\n",
    "            Y[i] = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + self.block_size]\n",
    "\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
    "            torch.from_numpy(Y).to(self.device, non_blocking=True)\n",
    "        )\n",
    "\n",
    "\n",
    "config = GPTConfig(\n",
    "    vocab_size=len(stoi),\n",
    "    n_layer=1,\n",
    "    n_embd=vocab_size,\n",
    "    block_size=block_size,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "train_dataset = GPUBatchDataset(train_ids, block_size, batch_size, device, pad_len=0)\n",
    "# === DataLoader ===\n",
    "train_loader  = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "model = GPT(config)\n",
    "model = torch.compile(model,mode=\"max-autotune\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4248459339141846\n",
      "2.267223834991455\n",
      "2.215237855911255\n",
      "2.226175308227539\n",
      "2.156221389770508\n",
      "2.1747803688049316\n",
      "2.101888418197632\n",
      "2.0438454151153564\n",
      "2.010363817214966\n",
      "2.011732578277588\n",
      "2.026224136352539\n",
      "2.0078017711639404\n",
      "1.97121262550354\n",
      "1.9109714031219482\n",
      "1.9306507110595703\n",
      "1.8956797122955322\n",
      "1.898237943649292\n",
      "1.9327778816223145\n",
      "1.9002705812454224\n",
      "1.9405431747436523\n",
      "1.9158666133880615\n",
      "1.8525692224502563\n",
      "1.8828603029251099\n",
      "1.874499797821045\n",
      "1.7933162450790405\n",
      "1.8483089208602905\n",
      "1.8752532005310059\n",
      "1.850698709487915\n",
      "1.8211157321929932\n",
      "1.812084674835205\n",
      "1.8197087049484253\n",
      "1.8460789918899536\n",
      "1.7804791927337646\n",
      "1.8103175163269043\n",
      "1.8493680953979492\n",
      "1.8176980018615723\n",
      "1.735107183456421\n",
      "1.8106229305267334\n",
      "1.8291242122650146\n",
      "1.762618064880371\n",
      "1.7982959747314453\n",
      "1.7748730182647705\n",
      "1.808021903038025\n",
      "1.8309447765350342\n",
      "1.8393099308013916\n",
      "1.7504090070724487\n",
      "1.7045764923095703\n",
      "1.7763054370880127\n",
      "1.8377323150634766\n",
      "1.7397518157958984\n",
      "1.8211332559585571\n",
      "1.7997101545333862\n",
      "1.7955690622329712\n",
      "1.799057960510254\n",
      "1.7753806114196777\n",
      "1.7839293479919434\n",
      "1.7318611145019531\n",
      "1.8142427206039429\n",
      "1.7819639444351196\n",
      "1.7538217306137085\n",
      "1.7972545623779297\n",
      "1.734282374382019\n",
      "1.8140634298324585\n",
      "1.7719922065734863\n",
      "1.741957187652588\n",
      "1.7714380025863647\n",
      "1.7582790851593018\n",
      "1.7871496677398682\n",
      "1.7908296585083008\n",
      "1.6674420833587646\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m logits, loss = model(xb, yb)\n\u001b[32m     12\u001b[39m loss = loss\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     15\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:311\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    310\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2259\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   2257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   2258\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2245\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimpl_fn\u001b[39m(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     out = \u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _backward_epilogue_functional(\n\u001b[32m   2247\u001b[39m         CompiledFunction.metadata,\n\u001b[32m   2248\u001b[39m         CompiledFunction.maybe_subclass_metadata,\n\u001b[32m   2249\u001b[39m         out,\n\u001b[32m   2250\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2376\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2360\u001b[39m     torch._functorch.config.donated_buffer\n\u001b[32m   2361\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[32m   2362\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata.bw_donated_idxs != []\n\u001b[32m   2363\u001b[39m ):\n\u001b[32m   2364\u001b[39m     torch._check(\n\u001b[32m   2365\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2366\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m   2373\u001b[39m         ),\n\u001b[32m   2374\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2376\u001b[39m out = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/ki/ckikdinafqfwoyo6xyufsfyknnnw4pjcytjhpwhavc34pcndnp3y.py:729\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    727\u001b[39m buf14 = reinterpret_tensor(buf2, (\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m66\u001b[39m), (\u001b[32m135168\u001b[39m, \u001b[32m66\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m0\u001b[39m); \u001b[38;5;28;01mdel\u001b[39;00m buf2  \u001b[38;5;66;03m# reuse\u001b[39;00m\n\u001b[32m    728\u001b[39m buf15 = empty_strided_cpu((\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m9\u001b[39m, \u001b[32m66\u001b[39m), (\u001b[32m594\u001b[39m, \u001b[32m594\u001b[39m, \u001b[32m66\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[43mcpp_fused_add_sum_3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf14\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf13\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf15\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    730\u001b[39m buf16 = empty_strided_cpu((\u001b[32m9\u001b[39m, \u001b[32m33\u001b[39m, \u001b[32m66\u001b[39m), (\u001b[32m2178\u001b[39m, \u001b[32m66\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n\u001b[32m    731\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [], Original ATen: [aten.bmm]\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    it = 0\n",
    "    for xb, yb in train_loader:\n",
    "          xb, yb = xb[0], yb[0]  # unwrap batch dimension\n",
    "          optimizer.zero_grad()\n",
    "          it = it + 1\n",
    "          logits, loss = model(xb, yb)\n",
    "          loss = loss\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "          optimizer.step()\n",
    "          total_loss += loss.item()\n",
    "          losses.append(loss.item())\n",
    "          if it%100==0: print(loss.item())\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7179797887802124\n",
      "1.840389370918274\n",
      "1.7476829290390015\n",
      "1.789048194885254\n",
      "1.762560248374939\n",
      "1.7769278287887573\n",
      "1.7655577659606934\n",
      "1.7587710618972778\n",
      "1.7428169250488281\n",
      "1.7512731552124023\n",
      "1.6718738079071045\n",
      "1.8342667818069458\n",
      "1.7842442989349365\n",
      "1.73983633518219\n",
      "1.7651057243347168\n",
      "1.7372163534164429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m optimizer.zero_grad()\n\u001b[32m     10\u001b[39m it = it + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss = loss\n\u001b[32m     13\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:375\u001b[39m, in \u001b[36mOptimizedModule.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.nn.modules.module._has_any_global_hook():\n\u001b[32m    366\u001b[39m     warnings.warn(\n\u001b[32m    367\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing `torch.compile(module)` when there are global hooks on \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodules (e.g., from `register_module_forward_hook`); this will\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    374\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:736\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    733\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 473\u001b[39m, in \u001b[36mGPT.forward\u001b[39m\u001b[34m(self, idx, targets, eprint)\u001b[39m\n\u001b[32m    471\u001b[39m x = \u001b[38;5;28mself\u001b[39m.transformer.wte(idx) \n\u001b[32m    472\u001b[39m x = x.detach()                 \u001b[38;5;66;03m# sever any stale history just in case\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m x.requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)         \u001b[38;5;66;03m# make x a grad leaf for τ at layer 0\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer.h:\n\u001b[32m    476\u001b[39m         x= block(x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 473\u001b[39m, in \u001b[36mtorch_dynamo_resume_in_forward_at_473\u001b[39m\u001b[34m(___stack0, self, targets, x)\u001b[39m\n\u001b[32m    471\u001b[39m x = \u001b[38;5;28mself\u001b[39m.transformer.wte(idx) \n\u001b[32m    472\u001b[39m x = x.detach()                 \u001b[38;5;66;03m# sever any stale history just in case\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m x.requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)         \u001b[38;5;66;03m# make x a grad leaf for τ at layer 0\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer.h:\n\u001b[32m    476\u001b[39m         x= block(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py:1241\u001b[39m, in \u001b[36maot_module_simplified.<locals>.forward\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1239\u001b[39m full_args.extend(params_flat)\n\u001b[32m   1240\u001b[39m full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:370\u001b[39m, in \u001b[36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[32m    366\u001b[39m         torch.autograd._force_original_view_tracking(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    367\u001b[39m         torch.enable_grad(),\n\u001b[32m    368\u001b[39m     ):\n\u001b[32m    369\u001b[39m         record_runtime_wrapper_prologue_exit(cm)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         all_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# When we have an inference graph, we run with grad disabled.\u001b[39;00m\n\u001b[32m    375\u001b[39m     \u001b[38;5;66;03m# It's possible to get an inference graph with inputs that require grad,\u001b[39;00m\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# in which case we want to make sure autograd is disabled\u001b[39;00m\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\u001b[39;00m\n\u001b[32m    378\u001b[39m     \u001b[38;5;66;03m# NOTE: We use _set_grad_enabled directly to reduce runtime overhead\u001b[39;00m\n\u001b[32m    379\u001b[39m     grad_enabled = torch.is_grad_enabled()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:100\u001b[39m, in \u001b[36mmake_boxed_func.<locals>.g\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mg\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:576\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    575\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2074\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.forward\u001b[39m\u001b[34m(ctx, *deduped_flat_tensor_args)\u001b[39m\n\u001b[32m   2065\u001b[39m     args = (*args, *fwd_rng_states)\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# There is a pretty complicated calling convention around what the compiled fw returns.\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# The full list of outputs and their relative order is:\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# (*tokens, *mutated_inputs, *fw_outs, *fw_intermediate_bases, *saved_tensors, *saved_symints)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2072\u001b[39m \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[32m   2073\u001b[39m \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2074\u001b[39m fw_outs = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_fw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2078\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2080\u001b[39m num_outputs = CompiledFunction.metadata.num_outputs\n\u001b[32m   2081\u001b[39m num_outputs_aliased = CompiledFunction.metadata.num_outputs_aliased\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:556\u001b[39m, in \u001b[36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    549\u001b[39m     out = \u001b[38;5;28mself\u001b[39m._functionalized_rng_runtime_epilogue(\n\u001b[32m    550\u001b[39m         runtime_metadata,\n\u001b[32m    551\u001b[39m         out,\n\u001b[32m    552\u001b[39m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    553\u001b[39m         runtime_metadata.num_forward_returns,\n\u001b[32m    554\u001b[39m     )\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/ux/cuxja64lcituoghtlpsfc3ctetb5gmbxh7hfk7dkrlpiweuhb4yp.py:2045\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m   2043\u001b[39m buf33 = reinterpret_tensor(buf30, (\u001b[32m64\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m66\u001b[39m), (\u001b[32m135168\u001b[39m, \u001b[32m66\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m0\u001b[39m); \u001b[38;5;28;01mdel\u001b[39;00m buf30  \u001b[38;5;66;03m# reuse\u001b[39;00m\n\u001b[32m   2044\u001b[39m buf35 = reinterpret_tensor(buf36, (\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m66\u001b[39m), (\u001b[32m1216512\u001b[39m, \u001b[32m594\u001b[39m, \u001b[32m66\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m66\u001b[39m)  \u001b[38;5;66;03m# alias\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2045\u001b[39m \u001b[43mcpp_fused__to_copy_add_arange_argmin_bitwise_or_cat_clamp_min_div_eq_gather_gt_linalg_vector_norm_lt_mul_neg_sub_sum_where_zero_zeros_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf33\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf19\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf14\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf22\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf17\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf18\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf20\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf21\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf24\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf29\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf31\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf23\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf25\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf26\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf27\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf35\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2046\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m buf14\n\u001b[32m   2047\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m buf16\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Juliet, do you love me?  JULIET:\n",
      "Yet God world whantss\n",
      "Wher's chour both thBxecurry my shall ing\n",
      "hones the gelanct our enot our Poress tore\n",
      "Heavy the fairly Apelinatlessnaarly;\n",
      "for your dommeast th3 \n",
      "Thereast by my my orrorey basely,\n",
      "By thy my trotlyss, lootly reayAy, bair\n",
      "ISignaatant in my corlayery this hill undred,\n",
      "Forrow your nor thate thatipely toRN eve,\n",
      "That arrughter be wrongrow drothYstal's,\n",
      "Oncurtaters pre3 art all the unto a day band onevenather beent ofL\n",
      "And for what did liess to useep thee.\n",
      "\n",
      "KING EDWARD:\n",
      "And that steare that prince, for so me,\n",
      "Is mourn'd the your have your arnough fast: in herefore the king, if are will lain.\n",
      "\n",
      "CATESBY:\n",
      "Or merropluned, and thou have the  dear inded this much!\n",
      "The her proved this but hather provey to heJfight.\n",
      "\n",
      "ESgots And strumers, I well theFore him my It could words, therefore sperearewer suchall our shall,\n",
      "Joour b: I may\n",
      "Unper deady his for would some like\n",
      "do swite ther looks and sucked ;ut Angre herepar'd\n",
      "Commine holy, alt presO rabless.\n",
      "The which a reat out should here self.\n",
      "\n",
      "MENENIUS:\n",
      "What is this pray, youch:\n",
      "You much say the cound your will but seee\n",
      "Thunucce dready for Warwick the with encessenge,\n",
      "What Duke mean the sat and the it below,\n",
      "It in her he hebeence, the pering pon from all ashater, and my from your:\n",
      "In my zour flatter's mine of Norfor\n",
      "That of the re mind acliant the deed!\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Are did the spirious,\n",
      "Then have deeddly in winds lepce,\n",
      "Unto che 'hear lies reet his on't;\n",
      "But that spetch ; thim his with combit looked and the have lot such geld\n",
      "In fair oath her hath humble soureds: your truey, and I seecured\n",
      "And uppose not Roment to my ojear\n",
      "In bost his in your bear thee of my more mehend and Xom for should,\n",
      "Or, my good part somether him such carged stand\n",
      "Marr'd aVpery day band.\n",
      "\n",
      "WASTINGs:\n",
      "I marry, that mad, to should but hand thou down: I have him not me in the  by by\n",
      "Thank, shall and from hinmanch as will be\n",
      "It King no 's your how ; it he not must purder , wetroner Edward blothere fair love with hame sus 3r\n",
      "Yorks Ware then to chomble bare--things,\n",
      "If not Henry a caust rue ladved worse\n",
      "The pleasely.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "BUCKINGHAM:\n",
      "Wo domfortures ? I we pots alay so;\n",
      "And bus\n",
      "when these cares Bure my to with the proved:\n",
      "Their any pray.\n",
      "\n",
      "MENENIUS:\n",
      "Novertery! my I speak ther\n",
      "come, way, your happipossed but theest my reword,\n",
      "And ifee man; noward,\n",
      "It it be keep my predoth hin my ceend before,\n",
      "And Marchard's my faints York and bear!\n",
      "The care my for execused be him flencest\n",
      "A more have here say, !e murdo me that with\n",
      "Can he sign morethh  had eye lect now, and penight therp,\n",
      "Besping sers, grace the to to traire,\n",
      "To brother done in groun herse, our bovow ;ave hall there look\n",
      "And most me hour fLtress time stans.\n",
      "\n",
      "LADY ANNE:\n",
      "I do' estake a beforced -aminaly?\n",
      "\n",
      "DUKE VINCENTE:\n",
      "If thou can pass of my night,\n",
      "And all be faul both the dispacre here burn .\n",
      "The she this is that their terreits that be brothO\n",
      "And and beginger: and here was shall\n",
      "Dece be your randLicieventarlSice of her had\n",
      "Then -his the Xod knust not beater.\n",
      "\n",
      "et did suity, chold werefore, here&O,\n",
      "Yet delier stands the hoom of mother the\n",
      "And which ElNTengurous do me I duke thends hence some and li's death\n",
      "That as my broy, greet by my bBid in querry,\n",
      "When his your all offel liants of the bour all the bries sue:\n",
      "Juch hath usbarlands bear sudder to the comes him.\n",
      "\n",
      "First Murderer:\n",
      "They lie dest offeing sires,\n",
      "And !let a cause, and the must comples;\n",
      "Where hese in her breath, I may,\n",
      "Dear Jus, we late Norfor the carer:\n",
      "Sickes the is sin me of thing of SBalike thee thelEsed. CoursD the maids,\n",
      "And give your sir, and wenty lords,\n",
      "Ih hing is that puriare wath  cause,\n",
      "Proftend; be she quits with deal tewar of thirtuaaw;\n",
      "Our not feel cafel before my princent;\n",
      "Indeelied have blands st conced to gues in.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "The and ase all de$se with is Clarder ame for !\n",
      "\n",
      "BRUTUS:\n",
      "\n",
      "MANELLIA:\n",
      "I , would sir:\n",
      "Go lake ey of indermity and and maday, whose they strument.\n",
      "\n",
      "LEONTES:\n",
      "Murdo my hand my death not speech a consury to to our be QeeE:\n",
      "And the lost tin and one thy have\n",
      "But it trudk, tuth theefore, where the promeFore at\n",
      "Of but we many fear !e zorther it ;o un\n",
      "4.1537926197052\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_sequence_char_rolling(\n",
    "    model, stoi, itos, prompt,\n",
    "    max_new_tokens=100,\n",
    "    block_size=1024,\n",
    "    temperature=1.0,\n",
    "    space_fallback=' ',\n",
    "    strict_window=False,          # if True, periodically re-prime caches on the last block\n",
    "    reprime_every=None            # if strict_window, how often to re-prime (int). Default: block_size\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolling-block generator that:\n",
    "      - keeps the ENTIRE generated text (no trimming of output),\n",
    "      - maintains a rolling block window internally,\n",
    "      - optionally re-primes feature caches on the last `block_size` tokens to strictly\n",
    "        mimic block-window semantics seen during training.\n",
    "\n",
    "    If strict_window=False (default): fastest path; caches stream forever.\n",
    "    If strict_window=True: we periodically reinitialize the per-layer states using the\n",
    "      most recent `block_size` tokens. This ensures exact 'sliding window' behavior.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    B = 1\n",
    "\n",
    "    # ---- encode prompt (fallback to space if empty) ----\n",
    "    space_id = stoi.get(space_fallback, 0)\n",
    "    prompt_ids = encode_chars(prompt, stoi)\n",
    "    if len(prompt_ids) == 0:\n",
    "        prompt_ids = [space_id]\n",
    "\n",
    "    # ---- left-pad ONCE to match your training forward's left-pad-to-block ----\n",
    "    pad_len = max(0, block_size - len(prompt_ids))\n",
    "    pad_ids = [space_id] * pad_len\n",
    "    priming_ids = pad_ids + prompt_ids  # padding only used for priming; not returned\n",
    "\n",
    "    # ---- per-block feature caches (one state per block) ----\n",
    "    feat_states = [\n",
    "        CausalPyramidState(\n",
    "            num_scales=model.config.n_scales,\n",
    "            C=model.config.n_embd,\n",
    "            device=device,\n",
    "            batch_size=B,\n",
    "            tau=1e-6\n",
    "        ) for _ in model.transformer.h\n",
    "    ]\n",
    "\n",
    "    # helper: (re-)prime caches with a sequence of token ids (left-pad to block if shorter)\n",
    "    def _reprime_with_ids(tok_ids):\n",
    "        # optionally left-pad the window up to block_size (only needed if strict semantics desired)\n",
    "        if len(tok_ids) < block_size:\n",
    "            tok_ids = [space_id] * (block_size - len(tok_ids)) + tok_ids\n",
    "        ids_t = torch.tensor([tok_ids], dtype=torch.long, device=device)  # (1, T)\n",
    "        x_last = None\n",
    "        # fresh states\n",
    "        new_states = [\n",
    "            CausalPyramidState(\n",
    "                num_scales=model.config.n_scales,\n",
    "                C=model.config.n_embd,\n",
    "                device=device,\n",
    "                batch_size=B,\n",
    "                tau=1e-6\n",
    "            ) for _ in model.transformer.h\n",
    "        ]\n",
    "        for t in range(ids_t.size(1)):\n",
    "            x_last = model.transformer.wte(ids_t[:, t])  # (1,C)\n",
    "            for blk, st in zip(model.transformer.h, new_states):\n",
    "                x_last = blk.step(x_last, st)\n",
    "        return new_states, x_last\n",
    "\n",
    "    # ---- initial priming with left-padded prompt ----\n",
    "    ids = torch.tensor([priming_ids], dtype=torch.long, device=device)\n",
    "    x_t = None\n",
    "    for t in range(ids.size(1)):\n",
    "        x_t = model.transformer.wte(ids[:, t])  # (1,C)\n",
    "        for blk, st in zip(model.transformer.h, feat_states):\n",
    "            x_t = blk.step(x_t, st)\n",
    "\n",
    "    # ---- FULL output accumulator (never trimmed) ----\n",
    "    out_full = list(prompt_ids)  # store ints\n",
    "\n",
    "    # ---- rolling window buffer of last block_size tokens (prompt + generated) ----\n",
    "    window = deque(prompt_ids, maxlen=block_size)\n",
    "\n",
    "    # strict-window settings\n",
    "    if reprime_every is None:\n",
    "        reprime_every = block_size\n",
    "    steps_since_reprime = 0\n",
    "\n",
    "    # ---- incremental rollout ----\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.lm_head(x_t)  # (1,V)\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / float(temperature)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "        next_id = int(next_token.item())\n",
    "\n",
    "        # record full output\n",
    "        out_full.append(next_id)\n",
    "\n",
    "        # advance rolling window\n",
    "        window.append(next_id)\n",
    "\n",
    "        # step one token\n",
    "        x_t = model.transformer.wte(next_token.squeeze(-1))  # (1,C)\n",
    "        for blk, st in zip(model.transformer.h, feat_states):\n",
    "            x_t = blk.step(x_t, st)\n",
    "\n",
    "        # optionally re-prime to strict sliding-window semantics\n",
    "        if strict_window:\n",
    "            steps_since_reprime += 1\n",
    "            if steps_since_reprime >= reprime_every and len(window) == block_size:\n",
    "                feat_states, x_t = _reprime_with_ids(list(window))\n",
    "                steps_since_reprime = 0\n",
    "\n",
    "    # decode full continuation (prompt + all generated)\n",
    "    return decode_chars(out_full, itos)\n",
    "    \n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "import time\n",
    "then = time.time()\n",
    "prompt = \"ROMEO: Juliet, do you love me?  JULIET:\"\n",
    "generated = decode_sequence_char_rolling(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=4096,\n",
    "    block_size=2048,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(generated)\n",
    "print(time.time()-then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "UzqWUbNRlmiD"
   },
   "outputs": [],
   "source": [
    "file_path = 'simple_model_tiny.pth'\n",
    "\n",
    "# 3. Save the model's state_dict\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
