{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p4-dcQlo_qoG",
    "outputId": "fdca09cd-ff69-4fbb-8982-70b40a8b01b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#copyright joshuah.rainstar@gmail.com\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "def phase_transport_between(curr: torch.Tensor, prev: torch.Tensor, tau: float = 1e-6) -> torch.Tensor:\n",
    "    B, T, C = curr.shape\n",
    "    eps = tau\n",
    "\n",
    "    u = _unit(curr)\n",
    "    v = _unit(prev)\n",
    "    w = curr - prev\n",
    "\n",
    "    c = (u * v).sum(dim=-1, keepdim=True)  # (B,T,1)\n",
    "\n",
    "    # masks (all as (B,T))\n",
    "    near_pos = (c > 1.0 - tau).squeeze(-1)           # (B,T)\n",
    "    near_neg = (c < -1.0 + tau).squeeze(-1)          # (B,T)\n",
    "    small_u  = (_norm(curr) < tau).squeeze(-1)       # (B,T)  <-- FIX\n",
    "    small_v  = (_norm(prev) < tau).squeeze(-1)       # (B,T)  <-- FIX\n",
    "    trivial  = near_pos | small_u | small_v          # (B,T)\n",
    "\n",
    "    # general branch\n",
    "    denom  = (1.0 + c).clamp_min(eps)                # (B,T,1)\n",
    "    a = (v * w).sum(dim=-1, keepdim=True)\n",
    "    b = (u * w).sum(dim=-1, keepdim=True)\n",
    "    Kw  = u * a - v * b\n",
    "    K2w = u * (a * c - b) + v * (b * c - a)\n",
    "    y_gen = w - Kw + (K2w / denom)                   # (B,T,C)\n",
    "\n",
    "    # antipodal branch\n",
    "    if C == 1:\n",
    "        y_neg = -w\n",
    "    else:\n",
    "        v_flat = v.reshape(-1, C)\n",
    "        p_flat = _orthonormal_perp(v_flat)\n",
    "        p = p_flat.view(B, T, C)\n",
    "        proj_v = (v * w).sum(dim=-1, keepdim=True) * v\n",
    "        proj_p = (p * w).sum(dim=-1, keepdim=True) * p\n",
    "        y_neg = w - 2.0 * proj_v - 2.0 * proj_p\n",
    "\n",
    "    # blend (no in-place masked writes)\n",
    "    y = torch.where(trivial.unsqueeze(-1), w, y_gen)\n",
    "    y = torch.where(near_neg.unsqueeze(-1), y_neg, y)\n",
    "    return y\n",
    "\n",
    "def _norm(v, eps: float = 1e-12):\n",
    "    return torch.linalg.vector_norm(v, dim=-1, keepdim=True).clamp_min(eps)\n",
    "\n",
    "def _unit(v, eps: float = 1e-12):\n",
    "    return v / _norm(v, eps)\n",
    "\n",
    "def _orthonormal_perp(v: torch.Tensor):\n",
    "    # v: (..., C), returns p ‚üÇ v, ||p||=1\n",
    "    *batch, C = v.shape\n",
    "    flat = v.reshape(-1, C)\n",
    "    idx = torch.argmin(torch.abs(flat), dim=-1)\n",
    "    e = torch.zeros_like(flat)\n",
    "    e.scatter_(1, idx.unsqueeze(1), 1.0)\n",
    "    proj = (e * flat).sum(dim=-1, keepdim=True) * flat\n",
    "    p = e - proj\n",
    "    p = p / _norm(p)\n",
    "    return p.view(*batch, C)\n",
    "\n",
    "\n",
    "# ---------- materialize left-aligned centroids at every t for each scale from two trees ----------\n",
    "def materialize_centroids_from_trees(x: torch.Tensor, levels0, levels1, K: int):\n",
    "    \"\"\"\n",
    "    For each scale W=2^f (f>=1), build mu^{(W)}(t) as the centroid of the left-aligned\n",
    "    block that contains t. Uses offset-0 tree if block starts at 0 mod W; otherwise offset-1 tree.\n",
    "    Returns: mu_all: (B, T, K-1, C)   (no scale-1 here; K-1 levels for f=1..K-1)\n",
    "    \"\"\"\n",
    "    B, T, C = x.shape\n",
    "    device = x.device\n",
    "    t_idx = torch.arange(T, device=device)  # (T,)\n",
    "    mus = []\n",
    "    for f in range(1, K):\n",
    "        W = 1 << f\n",
    "        # block start for t: s = floor(t/W)*W\n",
    "        s = (t_idx // W) * W           # (T,)\n",
    "        use_offset1 = (s % 2 == 1)     # whether the block start is odd (needs tree-1)\n",
    "        if f-1 < len(levels0):\n",
    "            L0 = levels0[f-1]          # (B, N0, C)\n",
    "            N0 = L0.shape[1]\n",
    "        else:\n",
    "            N0 = 0\n",
    "        if f-1 < len(levels1):\n",
    "            L1 = levels1[f-1]          # (B, N1, C)\n",
    "            N1 = L1.shape[1]\n",
    "        else:\n",
    "            N1 = 0\n",
    "\n",
    "        # index within chosen tree:\n",
    "        # for offset-0 (blocks [0..W-1], [W..2W-1], ...): idx0 = floor(t/W)\n",
    "        # for offset-1 (blocks [1..W], [W+1..2W], ...):   idx1 = floor((t-1)/W)\n",
    "        idx0 = (t_idx // W).clamp_max(max(N0-1, 0))\n",
    "        idx1 = ((t_idx - 1).clamp_min(0) // W).clamp_max(max(N1-1, 0))\n",
    "\n",
    "        # gather from the two trees\n",
    "        mu0 = L0.index_select(1, idx0) if N0 > 0 else x.new_zeros(B, T, C)\n",
    "        mu1 = L1.index_select(1, idx1) if N1 > 0 else x.new_zeros(B, T, C)\n",
    "\n",
    "        mu = torch.where(use_offset1.view(1, T, 1), mu1, mu0)  # (B,T,C)\n",
    "\n",
    "        # early region safety: if t < W-1 there is no full left-aligned block yet ‚Üí zero\n",
    "        mu = torch.where((t_idx < (W-1)).view(1, T, 1), torch.zeros_like(mu), mu)\n",
    "        mus.append(mu)\n",
    "    if len(mus) == 0:\n",
    "        return x.new_zeros(B, T, 0, C)\n",
    "    return torch.stack(mus, dim=2)  # (B, T, K-1, C)\n",
    "\n",
    "class CausalCentroidPyramid(nn.Module):\n",
    "    \"\"\"\n",
    "    Vectorized causal pyramid:\n",
    "      inputs x: (B, T, C)\n",
    "      returns deltas: (B, T, K, C)\n",
    "        where K = 1 (token PT) + (num_scales-1) (cluster PTs)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, tau: float = 1e-6):\n",
    "        super().__init__()\n",
    "        assert num_scales >= 1\n",
    "        self.K = num_scales\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        device = x.device\n",
    "\n",
    "        # token-level PT (scale-1)\n",
    "        prev_tok = torch.zeros_like(x)\n",
    "        if T > 1:\n",
    "            prev_tok[:, 1:, :] = x[:, :-1, :].contiguous()\n",
    "        d1 = phase_transport_between(x, prev_tok, tau=self.tau)  # (B,T,C)\n",
    "        d1[:, :1, :].zero_()  # mask early region (no previous token)\n",
    "\n",
    "        if self.K == 1:\n",
    "            return d1.unsqueeze(2)  # (B,T,1,C)\n",
    "\n",
    "        # Build centroids causally with recursive halves\n",
    "        mus = []\n",
    "        mu_prev = x  # Œº_0 = x (width=1)\n",
    "        for s in range(1, self.K):              # s=1..K-1 (width W=2^s)\n",
    "            W1 = 1 << (s - 1)\n",
    "            shifted = torch.zeros_like(mu_prev)\n",
    "            if T > W1:\n",
    "                shifted[:, W1:, :] = mu_prev[:, :-W1, :].contiguous()  # Œº_{s-1}(t - 2^{s-1})\n",
    "\n",
    "            mu_s = 0.5 * (mu_prev + shifted)     # Œº_s(t)\n",
    "            # zero early region t < W-1\n",
    "            W = 1 << s\n",
    "            if W > 1:\n",
    "                mu_s[:, :W-1, :].zero_()\n",
    "            mus.append(mu_s)\n",
    "            mu_prev = mu_s\n",
    "\n",
    "        mu_all = torch.stack(mus, dim=2) if mus else x.new_zeros(B, T, 0, C)  # (B,T,K-1,C)\n",
    "\n",
    "        # PT deltas between adjacent causal chunks\n",
    "        d_list = []\n",
    "        for j in range(self.K - 1):\n",
    "            W = 1 << (j + 1)\n",
    "            prev_mu = torch.zeros_like(mu_all[:, :, j, :])\n",
    "            if T > W:\n",
    "                prev_mu[:, W:, :] = mu_all[:, :-W, j, :].contiguous()\n",
    "            d = phase_transport_between(mu_all[:, :, j, :], prev_mu, tau=self.tau)\n",
    "            d[:, :W, :].zero_()  # mask early region\n",
    "            d_list.append(d)\n",
    "\n",
    "        d_clusters = torch.stack(d_list, dim=2) if d_list else x.new_zeros(B, T, 0, C)\n",
    "        return torch.cat([d1.unsqueeze(2), d_clusters], dim=2)  # (B,T,K,C)\n",
    "\n",
    "\n",
    "# ----- STREAMING STATE FOR INFERENCE -----\n",
    "class CausalPyramidState:\n",
    "    \"\"\"\n",
    "    O(K) step-time updates, no recompute.\n",
    "    For level ‚Ñì we keep a ring buffer of length 2^‚Ñì storing Œº_‚Ñì (with Œº_0=x).\n",
    "    That suffices both to:\n",
    "      - build Œº_{‚Ñì+1}(t) from Œº_‚Ñì(t) and Œº_‚Ñì(t-2^‚Ñì)\n",
    "      - compute deltas at scale s=‚Ñì via Œº_s(t-2^s)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, C: int, device, batch_size: int = 1, tau: float = 1e-6):\n",
    "        self.K = num_scales\n",
    "        self.C = C\n",
    "        self.B = batch_size\n",
    "        self.device = device\n",
    "        self.tau = float(tau)\n",
    "        self.t = 0  # number of tokens processed so far\n",
    "\n",
    "        # ring buffers: list over levels ‚Ñì = 0..K-1, each [B, L=2^‚Ñì, C]\n",
    "        self.buffers = []\n",
    "        self.ptrs = []\n",
    "        for l in range(self.K):\n",
    "            L = 1 << l\n",
    "            self.buffers.append(torch.zeros(self.B, L, C, device=device))\n",
    "            self.ptrs.append(0)\n",
    "\n",
    "    def _read_lookback(self, level: int, r: int):\n",
    "        \"\"\"return Œº_level(t - r); zeros if not enough history yet\"\"\"\n",
    "        if self.t < r:\n",
    "            return torch.zeros(self.B, self.C, device=self.device)\n",
    "        L = self.buffers[level].size(1)\n",
    "        idx = (self.ptrs[level] - r) % L\n",
    "        return self.buffers[level][:, idx, :]\n",
    "\n",
    "    def _push(self, level: int, value: torch.Tensor):\n",
    "        \"\"\"write current Œº_level(t) and advance ptr\"\"\"\n",
    "        L = self.buffers[level].size(1)\n",
    "        self.buffers[level][:, self.ptrs[level], :] = value\n",
    "        self.ptrs[level] = (self.ptrs[level] + 1) % L\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x_t: (B, C)\n",
    "        returns d(t): (B, K, C)  [token PT + (K-1) cluster PTs]\n",
    "        \"\"\"\n",
    "        B, C = x_t.shape\n",
    "        feats = []\n",
    "\n",
    "        # ------- token PT (read BEFORE any push) -------\n",
    "        prev_x = self._read_lookback(level=0, r=1)  # Œº0(t-1)\n",
    "        d1 = phase_transport_between(x_t[:, None, :], prev_x[:, None, :], tau=self.tau).squeeze(1)\n",
    "        if self.t == 0:\n",
    "            d1.zero_()\n",
    "        feats.append(d1)\n",
    "\n",
    "        # ------- (A) compute all Œº_s(t) with pre-push lookbacks -------\n",
    "        mu_curr = [None] * self.K\n",
    "        mu_curr[0] = x_t                      # Œº0(t)\n",
    "        mu_prev = x_t\n",
    "        for s in range(1, self.K):\n",
    "            W1 = 1 << (s - 1)\n",
    "            W  = 1 << s\n",
    "            mu_back = self._read_lookback(level=s-1, r=W1)   # Œº_{s-1}(t - 2^{s-1})  (pre-push!)\n",
    "            mu_s_t  = 0.5 * (mu_prev + mu_back)              # Œº_s(t)\n",
    "            if self.t < (W - 1):                             # early mask (global t)\n",
    "                mu_s_t.zero_()\n",
    "            mu_curr[s] = mu_s_t\n",
    "            mu_prev = mu_s_t\n",
    "\n",
    "        # ------- (B) compute all deltas d_s using Œº_s(t‚àíW) (pre-push) -------\n",
    "        for s in range(1, self.K):\n",
    "            W = 1 << s\n",
    "            mu_prevW = self._read_lookback(level=s, r=W)     # Œº_s(t - 2^s)  (pre-push!)\n",
    "            d_s = phase_transport_between(mu_curr[s][:, None, :], mu_prevW[:, None, :], tau=self.tau).squeeze(1)\n",
    "            if self.t + 1 <= W:\n",
    "                d_s.zero_()\n",
    "            feats.append(d_s)\n",
    "\n",
    "        # ------- (C) push Œº_‚Ñì(t) for all levels, exactly once -------\n",
    "        self._push(level=0, value=mu_curr[0])\n",
    "        for s in range(1, self.K):\n",
    "            self._push(level=s, value=mu_curr[s])\n",
    "\n",
    "        self.t += 1\n",
    "        return torch.stack(feats, dim=1)  # (B, K, C)\n",
    "\n",
    "\n",
    "class SemanticClusterFeaturesCausal(nn.Module):\n",
    "    \"\"\"\n",
    "    Unified wrapper:\n",
    "      - forward(x): vectorized for training\n",
    "      - step(x_t, state): single-step for inference with cache\n",
    "    \"\"\"\n",
    "    def __init__(self, num_scales: int, tau: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.pyramid = CausalCentroidPyramid(num_scales=num_scales, tau=tau)\n",
    "        self.K = num_scales\n",
    "        self.tau = float(tau)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.pyramid(x)  # (B,T,K,C)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor, state: CausalPyramidState) -> torch.Tensor:\n",
    "        return state.step(x_t)  # (B,K,C)\n",
    "\n",
    "        \n",
    "class Cell(nn.Module):\n",
    "    def __init__(self, dim_in: int, hidden: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_in, hidden, bias=False) #dont change, false intentional\n",
    "        self.fc2 = nn.Linear(hidden, dim_in, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "    def forward(self, x):\n",
    "      \n",
    "        return self.fc2(self.act(self.fc1(x))) \n",
    "\n",
    "class GPTSemanticBlock(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        C = config.n_embd\n",
    "        self.C = C\n",
    "        self.K = config.n_scales\n",
    "        # L = number of feature groups concatenated: token (1) + K scales\n",
    "        self.L = 1 + self.K\n",
    "        self.features = SemanticClusterFeaturesCausal(num_scales=self.K, tau=1e-6)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.ln = nn.LayerNorm(self.C)\n",
    "        self.mlp = Cell(self.C,self.C*4)\n",
    "  \n",
    "\n",
    "        # NOTE: remove trailing comma so this is a ModuleList, not a tuple\n",
    "        # Each bottleneck maps C -> small_hidden -> C\n",
    "        self.bottlenecks = nn.ModuleList(\n",
    "            [Cell(self.C,32) for _ in range(self.K)]\n",
    "        )\n",
    "\n",
    "    # vectorized\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        feats = self.features(x)               # (B, T, K, C)\n",
    "\n",
    "        # apply per-scale bottlenecks in parallel (vectorized)\n",
    "        if self.K > 0:\n",
    "            # feats_per_scale: list of (B, T, C) after bottleneck\n",
    "            # do this with a list comprehension to ensure modules are registered\n",
    "            processed = [self.bottlenecks[j](feats[:, :, j, :]) for j in range(self.K)]\n",
    "            # stack back to (B, T, K, C)\n",
    "            proc_feats = torch.stack(processed, dim=2)\n",
    "            # reshape to (B, T, K*C)\n",
    "            proc_feats_t = proc_feats.sum(dim=2)\n",
    "\n",
    "        else:\n",
    "            proc_feats_t = x.new_zeros(B, T, 0)\n",
    "   \n",
    "        # concat token embedding with processed features\n",
    "        x_in = x + proc_feats_t\n",
    "        out = x + self.drop(self.ln(self.mlp(x_in)))\n",
    "        #if z isnt identical to x, this will penalize x\n",
    "        return out\n",
    "\n",
    "    # single-step incremental\n",
    "    @torch.no_grad()\n",
    "    def step(self, x_t: torch.Tensor, feat_state: CausalPyramidState) -> torch.Tensor:\n",
    "        # x_t: (B, C)\n",
    "        B, C = x_t.shape\n",
    "        feats_t = self.features.step(x_t, feat_state)  # (B, K, C)\n",
    "\n",
    "        if self.K > 0:\n",
    "            # apply each bottleneck to the corresponding (B, C) slice\n",
    "            processed = [self.bottlenecks[j](feats_t[:, j, :]) for j in range(self.K)]\n",
    "            proc_feats = torch.stack(processed, dim=1)    # (B, K, C)\n",
    "            proc_feats_t = proc_feats.sum(dim=1)\n",
    "        else:\n",
    "            proc_feats_t = x_t.new_zeros(B, 0)\n",
    "        x_in = x_t+proc_feats_t     # (B, (1+K)*C)\n",
    "        out = x_t + self.drop(self.ln(self.mlp(x_in)))\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, config, seed=0):\n",
    "        super().__init__()\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        W = torch.randn(config.vocab_size, config.n_embd, generator=g)\n",
    "        # row-center and row-normalize so rows are zero-mean, unit-norm\n",
    "        W = W - W.mean(dim=1, keepdim=True)\n",
    "        W = W / (W.norm(dim=1, keepdim=True) + 1e-8)\n",
    "        self.weight = nn.Parameter(W, requires_grad=False)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        return self.weight[idx]\n",
    "\n",
    "# ---- BlockFast wired for list-in/list-out mixer ----\n",
    "\n",
    "\n",
    "class ProjectionEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Replacement for NN.embed: maps token id i -> one-hot basis e_i in R^{V},\n",
    "    with V = config.vocab_size = config.n_embd. Weights are frozen.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, dtype=None, device=None):\n",
    "        super().__init__()\n",
    "        assert config.n_embd == config.vocab_size, (\n",
    "            f\"Expected n_embd == vocab_size, got {config.n_embd} != {config.vocab_size}\"\n",
    "        )\n",
    "        V = config.vocab_size\n",
    "        eye = torch.eye(V, dtype=dtype or torch.float32, device=device)\n",
    "        self.embed = nn.Embedding.from_pretrained(eye, freeze=True)  # weight = I_V\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        # shape: (batch, seq_len, V)\n",
    "        return self.embed(input_ids)\n",
    "        \n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 66 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 6\n",
    "    n_head:int = 6\n",
    "    n_embd: int = 128\n",
    "    n_scales:int = 9\n",
    "    dropout: float = 0.1\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.n_embd = config.n_embd\n",
    "        self.drop = nn.Dropout(0.6)\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = ProjectionEmbed(config),\n",
    "            h = nn.ModuleList([GPTSemanticBlock(config) for _ in range(config.n_layer)]),\n",
    "\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(self.config.n_embd, self.config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.transformer.wte.embed.weight\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- forward ----------\n",
    "    def forward(self, idx, targets=None, eprint=False):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        x = self.transformer.wte(idx) \n",
    "        x = x.detach()                 # sever any stale history just in case\n",
    "        x.requires_grad_(True)         # make x a grad leaf for œÑ at layer 0\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "                x= block(x)\n",
    "\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-1\n",
    "            )\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_greedy(model: nn.Module, idx: torch.LongTensor, max_new_tokens: int, block_size: int):\n",
    "        \"\"\"\n",
    "        model: your GPT with:\n",
    "           - transformer.wte (embedding)\n",
    "           - transformer.h : list[GPTSemanticBlock]\n",
    "           - lm_head\n",
    "        idx: (B, T0) prompt token ids\n",
    "        \"\"\"\n",
    "        device = next(model.parameters()).device\n",
    "        B = idx.size(0)\n",
    "        # per-block feature caches\n",
    "        feat_states = [CausalPyramidState(model.config.n_scales, model.config.n_embd, device, batch_size=B)\n",
    "                       for _ in model.transformer.h]\n",
    "    \n",
    "        # 1) prime caches with the prompt (causal, one step at a time)\n",
    "        x_all = model.transformer.wte(idx)  # (B,T0,C); fixed embeddings in your code\n",
    "        for t in range(idx.size(1)):\n",
    "            x_t = x_all[:, t, :]\n",
    "            for blk, st in zip(model.transformer.h, feat_states):\n",
    "                x_t = blk.step(x_t, st)      # per-block step\n",
    "            # we discard logits during priming\n",
    "    \n",
    "        # 2) roll out new tokens\n",
    "        out = [idx]\n",
    "        cur = idx\n",
    "        for _ in range(max_new_tokens):\n",
    "            # last token embedding\n",
    "            last_idx = cur[:, -1]                      # (B,)\n",
    "            x_t = model.transformer.wte(last_idx)      # (B,C)\n",
    "            for blk, st in zip(model.transformer.h, feat_states):\n",
    "                x_t = blk.step(x_t, st)                # (B,C)\n",
    "            logits = model.lm_head(x_t)                # (B,V)\n",
    "            next_idx = torch.argmax(logits, dim=-1, keepdim=True)  # greedy; swap to sampling if you like\n",
    "            out.append(next_idx)\n",
    "            cur = torch.cat([cur, next_idx], dim=1)\n",
    "            # keep only last block_size tokens in cur (typical AR convenience)\n",
    "            if cur.size(1) > block_size:\n",
    "                cur = cur[:, -block_size:]\n",
    "        return torch.cat(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFGVJvlN_yfW",
    "outputId": "f11f6493-0761-458d-9be0-4ebc604e53e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading aochildes.txt...\n",
      "üì• Downloading cbt.txt...\n",
      "üì• Downloading children_stories.txt...\n",
      "üì• Downloading gutenberg.txt...\n",
      "üì• Downloading qed.txt...\n",
      "üì• Downloading simple_wikipedia.txt...\n",
      "üì• Downloading switchboard.txt...\n",
      "üì• Downloading wikipedia.txt...\n",
      "üì• Downloading shakespeare.txt...\n",
      "‚úÖ Done. Files saved to ./babylm_10m_cleaned\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"üì• Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"‚úÖ Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0fFuL2a_sAF",
    "outputId": "79c1170f-c818-4568-cf7b-939e02bc33e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Char tokenizer finalized.\n",
      "üßæ Train tokens: 1016242 | Val tokens: 99152\n",
      "üî§ Vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,#\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Char tokenizer finalized.\")\n",
    "print(f\"üßæ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"üî§ Vocab size: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g42l_Fa8_v9z",
    "outputId": "7bbfe691-e965-43cd-e53e-a4438ff8d7ca",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 2048\n",
    "batch_size = 8\n",
    "\n",
    "# === Load tokenizer metadata ===\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "\n",
    "# === Load mmap edata (char-level tokens, uint16) ===\n",
    "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
    "\n",
    "# === Efficient GPU Batch Sampler ===\n",
    "class GPUBatchDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size, batch_size, device, jitter=63, p_aligned=0.5, pad_len=0):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.pad_len = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len  # X length\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = self.total // self.sample_len\n",
    "        self.jitter = int(jitter)          # small random offset added to aligned start\n",
    "        self.p_aligned = float(p_aligned)  # mix aligned and jittered\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = np.empty((self.batch_size, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((self.batch_size, self.block_size), dtype=np.int64)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # choose a base aligned block\n",
    "            base_block = np.random.randint(0, self.n_blocks)\n",
    "            start = base_block * self.sample_len\n",
    "\n",
    "            # with probability, add a small jitter (keeps cache-friendly contiguous reads)\n",
    "            if np.random.rand() > self.p_aligned:\n",
    "                j = np.random.randint(0, self.jitter + 1)\n",
    "                start = min(start + j, self.total)  # stay in range\n",
    "\n",
    "            X[i] = self.data[start : start + self.sample_len]\n",
    "            # targets correspond to the final block_size visible steps\n",
    "            Y[i] = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + self.block_size]\n",
    "\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
    "            torch.from_numpy(Y).to(self.device, non_blocking=True)\n",
    "        )\n",
    "\n",
    "\n",
    "config = GPTConfig(\n",
    "    vocab_size=len(stoi),\n",
    "    n_layer=1,\n",
    "    n_embd=vocab_size,\n",
    "    block_size=block_size,\n",
    "    dropout = 0.1,\n",
    ")\n",
    "train_dataset = GPUBatchDataset(train_ids, block_size, batch_size, device, pad_len=0)\n",
    "# === DataLoader ===\n",
    "train_loader  = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "model = GPT(config)\n",
    "model = torch.compile(model)\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 78012\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6831278800964355\n",
      "3.887181282043457\n",
      "3.613970994949341\n",
      "3.5044593811035156\n",
      "3.419283151626587\n",
      "3.467473030090332\n",
      "3.4235458374023438\n",
      "3.4108786582946777\n",
      "3.3578720092773438\n",
      "3.3691325187683105\n",
      "3.3216023445129395\n",
      "3.244929790496826\n",
      "3.240009069442749\n",
      "3.178504467010498\n",
      "3.0674238204956055\n",
      "3.0410728454589844\n",
      "2.982834815979004\n",
      "2.9636106491088867\n",
      "2.9537124633789062\n",
      "2.9425857067108154\n",
      "2.903386354446411\n",
      "2.8558349609375\n",
      "2.8386101722717285\n",
      "2.8064475059509277\n",
      "2.8081915378570557\n",
      "2.8157896995544434\n",
      "2.794247627258301\n",
      "2.7758631706237793\n",
      "2.7473983764648438\n",
      "2.736024856567383\n",
      "2.746248245239258\n",
      "2.713498830795288\n",
      "2.6928422451019287\n",
      "2.7138795852661133\n",
      "2.691347599029541\n",
      "2.695099353790283\n",
      "2.650240898132324\n",
      "2.6827034950256348\n",
      "2.690004825592041\n",
      "2.637037515640259\n",
      "2.661160469055176\n",
      "2.6441245079040527\n",
      "2.648069143295288\n",
      "2.6614952087402344\n",
      "2.6206412315368652\n",
      "2.6216495037078857\n",
      "2.6349596977233887\n",
      "2.6402387619018555\n",
      "2.62166166305542\n",
      "2.6132683753967285\n",
      "2.592533826828003\n",
      "2.609917402267456\n",
      "2.6108312606811523\n",
      "2.617013454437256\n",
      "2.6205201148986816\n",
      "2.600663661956787\n",
      "2.588595390319824\n",
      "2.5739572048187256\n",
      "2.5932302474975586\n",
      "2.5807912349700928\n",
      "2.598942279815674\n",
      "2.5728280544281006\n",
      "2.5766234397888184\n",
      "2.5790953636169434\n",
      "2.603882074356079\n",
      "2.560609817504883\n",
      "2.567690372467041\n",
      "2.5820775032043457\n",
      "2.5702571868896484\n",
      "2.5734965801239014\n",
      "2.5599253177642822\n",
      "2.566929817199707\n",
      "2.542038917541504\n",
      "2.555318832397461\n",
      "2.5575153827667236\n",
      "2.5516552925109863\n",
      "2.54018497467041\n",
      "2.534806728363037\n",
      "2.5180373191833496\n",
      "2.5406222343444824\n",
      "2.548916816711426\n",
      "2.5293192863464355\n",
      "2.516559600830078\n",
      "2.5539517402648926\n",
      "2.546633243560791\n",
      "2.517146110534668\n",
      "2.5394482612609863\n",
      "2.5119788646698\n",
      "2.515462875366211\n",
      "2.5351850986480713\n",
      "2.489917039871216\n",
      "2.492826223373413\n",
      "2.4951202869415283\n",
      "2.5187835693359375\n",
      "2.5203514099121094\n",
      "2.5019023418426514\n",
      "2.494086742401123\n",
      "2.505551815032959\n",
      "2.521768093109131\n",
      "2.5138845443725586\n",
      "2.532755136489868\n",
      "2.479016065597534\n",
      "2.4572246074676514\n",
      "2.4936695098876953\n",
      "2.498901605606079\n",
      "2.50551700592041\n",
      "2.478973865509033\n",
      "2.4905734062194824\n",
      "2.5012035369873047\n",
      "2.5346622467041016\n",
      "2.4819822311401367\n",
      "2.4897301197052\n",
      "2.4922494888305664\n",
      "2.491428852081299\n",
      "2.4799346923828125\n",
      "2.5075278282165527\n",
      "2.51080322265625\n",
      "2.4917187690734863\n",
      "2.5006215572357178\n",
      "2.4650869369506836\n",
      "2.513154983520508\n",
      "2.4894556999206543\n",
      "2.46181583404541\n",
      "2.476724624633789\n",
      "2.4894165992736816\n",
      "2.4888358116149902\n",
      "2.4971556663513184\n",
      "2.457520008087158\n",
      "2.4864416122436523\n",
      "2.474712371826172\n",
      "2.4949569702148438\n",
      "2.463397979736328\n",
      "2.4602856636047363\n",
      "2.456789970397949\n",
      "2.466085910797119\n",
      "2.4686732292175293\n",
      "2.4615602493286133\n",
      "2.4835357666015625\n",
      "2.426307439804077\n",
      "2.4555680751800537\n",
      "2.463123083114624\n",
      "2.484560966491699\n",
      "2.482342481613159\n",
      "2.4636101722717285\n",
      "2.442166328430176\n",
      "2.4487459659576416\n",
      "2.4473156929016113\n",
      "2.451409339904785\n",
      "2.47945499420166\n",
      "2.4628548622131348\n",
      "2.4037864208221436\n",
      "2.449495792388916\n",
      "2.4247608184814453\n",
      "2.4382102489471436\n",
      "2.42281436920166\n",
      "2.4191577434539795\n",
      "2.4497458934783936\n",
      "2.410637140274048\n",
      "2.4499006271362305\n",
      "2.421964645385742\n",
      "2.426229238510132\n",
      "2.423652172088623\n",
      "2.4242653846740723\n",
      "2.4432716369628906\n",
      "2.4235105514526367\n",
      "2.4371700286865234\n",
      "2.4340784549713135\n",
      "2.4111783504486084\n",
      "2.470684289932251\n",
      "2.4151952266693115\n",
      "2.427184820175171\n",
      "2.4019343852996826\n",
      "2.404191017150879\n",
      "2.405393600463867\n",
      "2.4144129753112793\n",
      "2.4457545280456543\n",
      "2.3884522914886475\n",
      "2.3947970867156982\n",
      "2.428267002105713\n",
      "2.474119186401367\n",
      "2.462219715118408\n",
      "2.415684938430786\n",
      "2.4378256797790527\n",
      "2.4336090087890625\n",
      "2.409146785736084\n",
      "2.3620848655700684\n",
      "2.4276604652404785\n",
      "2.3840980529785156\n",
      "2.394925117492676\n",
      "2.3946805000305176\n",
      "2.404019355773926\n",
      "2.3854615688323975\n",
      "2.4226067066192627\n",
      "2.395172595977783\n",
      "2.3792574405670166\n",
      "2.400027275085449\n",
      "2.394318103790283\n",
      "2.4232006072998047\n",
      "2.438912868499756\n",
      "2.3739981651306152\n",
      "2.405921220779419\n",
      "2.370169162750244\n",
      "2.402348041534424\n",
      "2.3931775093078613\n",
      "2.39436936378479\n",
      "2.404813766479492\n",
      "2.395235776901245\n",
      "2.3704140186309814\n",
      "2.421020746231079\n",
      "2.371690511703491\n",
      "2.392263412475586\n",
      "2.3752529621124268\n",
      "2.378436803817749\n",
      "2.376497268676758\n",
      "2.385249614715576\n",
      "2.399538993835449\n",
      "2.403679370880127\n",
      "2.380214214324951\n",
      "2.3773412704467773\n",
      "2.3566389083862305\n",
      "2.3491640090942383\n",
      "2.3988583087921143\n",
      "2.3801109790802\n",
      "2.3781661987304688\n",
      "2.38326358795166\n",
      "2.3473968505859375\n",
      "2.3897218704223633\n",
      "2.349942207336426\n",
      "2.355410575866699\n",
      "2.383436918258667\n",
      "2.360048294067383\n",
      "2.3677899837493896\n",
      "2.3275699615478516\n",
      "2.3682756423950195\n",
      "2.344593048095703\n",
      "2.3256402015686035\n",
      "2.385593891143799\n",
      "2.3442330360412598\n",
      "2.3469347953796387\n",
      "2.357431411743164\n",
      "2.328625202178955\n",
      "2.3399658203125\n",
      "2.4019341468811035\n",
      "2.3678736686706543\n",
      "2.349547863006592\n",
      "2.3554563522338867\n",
      "2.358154296875\n",
      "2.333829402923584\n",
      "2.3524973392486572\n",
      "2.3418819904327393\n",
      "2.3550305366516113\n",
      "2.362919330596924\n",
      "2.3586292266845703\n",
      "2.374293088912964\n",
      "2.361931324005127\n",
      "2.3394126892089844\n",
      "2.317626953125\n",
      "2.350921869277954\n",
      "2.366328239440918\n",
      "2.392599582672119\n",
      "2.377593517303467\n",
      "2.387159824371338\n",
      "2.3601365089416504\n",
      "2.3190197944641113\n",
      "2.339764356613159\n",
      "2.3420748710632324\n",
      "2.321031093597412\n",
      "2.3347911834716797\n",
      "2.318063259124756\n",
      "2.3558292388916016\n",
      "2.3270912170410156\n",
      "2.3391873836517334\n",
      "2.3452954292297363\n",
      "2.326341390609741\n",
      "2.3332366943359375\n",
      "2.3498106002807617\n",
      "2.31620717048645\n",
      "2.3423848152160645\n",
      "2.3444786071777344\n",
      "2.3023061752319336\n",
      "2.34169864654541\n",
      "2.302443027496338\n",
      "2.339535713195801\n",
      "2.313530445098877\n",
      "2.3083763122558594\n",
      "2.3174829483032227\n",
      "2.346029758453369\n",
      "2.3227834701538086\n",
      "2.3554913997650146\n",
      "2.308368682861328\n",
      "2.3192644119262695\n",
      "2.306706666946411\n",
      "2.2915966510772705\n",
      "2.3079686164855957\n",
      "2.3749794960021973\n",
      "2.280148506164551\n",
      "2.3193578720092773\n",
      "2.309939384460449\n",
      "2.297987222671509\n",
      "2.3163704872131348\n",
      "2.3111722469329834\n",
      "2.3025665283203125\n",
      "2.294065475463867\n",
      "2.3391642570495605\n",
      "2.314833879470825\n",
      "2.312252998352051\n",
      "2.295522689819336\n",
      "2.295743465423584\n",
      "2.292231559753418\n",
      "2.3275063037872314\n",
      "2.302945137023926\n",
      "2.296915054321289\n",
      "2.3262293338775635\n",
      "2.3081319332122803\n",
      "2.333238363265991\n",
      "2.2849769592285156\n",
      "2.2822022438049316\n",
      "2.319307804107666\n",
      "2.28132700920105\n",
      "2.2856202125549316\n",
      "2.3029017448425293\n",
      "2.2544045448303223\n",
      "2.2657361030578613\n",
      "2.319350242614746\n",
      "2.2854387760162354\n",
      "2.3232421875\n",
      "2.32940936088562\n",
      "2.3149290084838867\n",
      "2.283717632293701\n",
      "2.28745698928833\n",
      "2.256220817565918\n",
      "2.2756125926971436\n",
      "2.3085544109344482\n",
      "2.296745777130127\n",
      "2.2685234546661377\n",
      "2.274280071258545\n",
      "2.3053746223449707\n",
      "2.3145151138305664\n",
      "2.315490484237671\n",
      "2.2836408615112305\n",
      "2.2535271644592285\n",
      "2.238615036010742\n",
      "2.2611029148101807\n",
      "2.24765944480896\n",
      "2.2466001510620117\n",
      "2.267486572265625\n",
      "2.280031204223633\n",
      "2.287992477416992\n",
      "2.3149895668029785\n",
      "2.3245174884796143\n",
      "2.2538866996765137\n",
      "2.2580695152282715\n",
      "2.2764904499053955\n",
      "2.2301104068756104\n",
      "2.288506031036377\n",
      "2.2849299907684326\n",
      "2.292573928833008\n",
      "2.220181941986084\n",
      "2.2790911197662354\n",
      "2.2744193077087402\n",
      "2.315674304962158\n",
      "2.2614569664001465\n",
      "2.286045789718628\n",
      "2.2779946327209473\n",
      "2.307426691055298\n",
      "2.274322032928467\n",
      "2.31223201751709\n",
      "2.287985324859619\n",
      "2.241377353668213\n",
      "2.2638063430786133\n",
      "2.2533490657806396\n",
      "2.262556552886963\n",
      "2.2680039405822754\n",
      "2.2890310287475586\n",
      "2.2848494052886963\n",
      "2.271909713745117\n",
      "2.233088493347168\n",
      "2.2849373817443848\n",
      "2.2346057891845703\n",
      "2.279752492904663\n",
      "2.235628128051758\n",
      "2.2873096466064453\n",
      "2.2420120239257812\n",
      "2.25618314743042\n",
      "2.308387279510498\n",
      "2.2810161113739014\n",
      "2.270598888397217\n",
      "2.266216516494751\n",
      "2.263495445251465\n",
      "2.2897157669067383\n",
      "2.25327730178833\n",
      "2.2049920558929443\n",
      "2.2603559494018555\n",
      "2.254868507385254\n",
      "2.2505173683166504\n",
      "2.2778289318084717\n",
      "2.273045539855957\n",
      "2.246110677719116\n",
      "2.2441458702087402\n",
      "2.327603816986084\n",
      "2.2866055965423584\n",
      "2.283224105834961\n",
      "2.2316946983337402\n",
      "2.2472715377807617\n",
      "2.2688231468200684\n",
      "2.2307674884796143\n",
      "2.234412670135498\n",
      "2.231074571609497\n",
      "2.271250009536743\n",
      "2.290256977081299\n",
      "2.229539394378662\n",
      "2.262523651123047\n",
      "2.2591819763183594\n",
      "2.2195253372192383\n",
      "2.2634544372558594\n",
      "2.204824686050415\n",
      "2.2397263050079346\n",
      "2.2320616245269775\n",
      "2.2608747482299805\n",
      "2.2803356647491455\n",
      "2.2248799800872803\n",
      "2.2812142372131348\n",
      "2.252776622772217\n",
      "2.194479465484619\n",
      "2.201978921890259\n",
      "2.2585692405700684\n",
      "2.248875617980957\n",
      "2.219778060913086\n",
      "2.2734649181365967\n",
      "2.2139739990234375\n",
      "2.2786834239959717\n",
      "2.1910386085510254\n",
      "2.2594375610351562\n",
      "2.220782518386841\n",
      "2.2214696407318115\n",
      "2.2224459648132324\n",
      "2.270982265472412\n",
      "2.252377510070801\n",
      "2.2185840606689453\n",
      "2.2539782524108887\n",
      "2.2245423793792725\n",
      "2.253751039505005\n",
      "2.2322731018066406\n",
      "2.216749906539917\n",
      "2.2401413917541504\n",
      "2.26668643951416\n",
      "2.2382216453552246\n",
      "2.2022132873535156\n",
      "2.252716541290283\n",
      "2.2701668739318848\n",
      "2.2277708053588867\n",
      "2.228364944458008\n",
      "2.2126736640930176\n",
      "2.2281010150909424\n",
      "2.23175048828125\n",
      "2.2386770248413086\n",
      "2.257495880126953\n",
      "2.2269420623779297\n",
      "2.2400097846984863\n",
      "2.226856231689453\n",
      "2.229719638824463\n",
      "2.169962167739868\n",
      "2.2307589054107666\n",
      "2.2368974685668945\n",
      "2.234039306640625\n",
      "2.189455509185791\n",
      "2.265265464782715\n",
      "2.226754903793335\n",
      "2.242058038711548\n",
      "2.2251858711242676\n",
      "2.2064812183380127\n",
      "2.1464014053344727\n",
      "2.2191505432128906\n",
      "2.253629684448242\n",
      "2.163909435272217\n",
      "2.1709296703338623\n",
      "2.2175631523132324\n",
      "2.2141025066375732\n",
      "2.1971001625061035\n",
      "2.216628074645996\n",
      "2.152190685272217\n",
      "2.2447967529296875\n",
      "2.233677864074707\n",
      "2.2075462341308594\n",
      "2.2431771755218506\n",
      "2.191181182861328\n",
      "2.2333834171295166\n",
      "2.2043161392211914\n",
      "2.173215866088867\n",
      "2.2231364250183105\n",
      "2.2117130756378174\n",
      "2.240959644317627\n",
      "2.1999285221099854\n",
      "2.25039005279541\n",
      "2.2329277992248535\n",
      "2.2471563816070557\n",
      "2.207082986831665\n",
      "2.2384543418884277\n",
      "2.200078248977661\n",
      "2.2333521842956543\n",
      "2.216125011444092\n",
      "2.192021608352661\n",
      "2.255155324935913\n",
      "2.2109549045562744\n",
      "2.191328525543213\n",
      "2.182509660720825\n",
      "2.211388349533081\n",
      "2.176135301589966\n",
      "2.210212230682373\n",
      "2.184732437133789\n",
      "2.256941556930542\n",
      "2.1963491439819336\n",
      "2.1988000869750977\n",
      "2.1677706241607666\n",
      "2.1590323448181152\n",
      "2.1791346073150635\n",
      "2.216399908065796\n",
      "2.2242679595947266\n",
      "2.228764533996582\n",
      "2.1875038146972656\n",
      "2.2104430198669434\n",
      "2.2129812240600586\n",
      "2.1907548904418945\n",
      "2.1901705265045166\n",
      "2.1618354320526123\n",
      "2.18367862701416\n",
      "2.1939287185668945\n",
      "2.1894893646240234\n",
      "2.165414333343506\n",
      "2.1986143589019775\n",
      "2.2238073348999023\n",
      "2.2251431941986084\n",
      "2.159912347793579\n",
      "2.216360092163086\n",
      "2.167043685913086\n",
      "2.1581735610961914\n",
      "2.1858458518981934\n",
      "2.1573963165283203\n",
      "2.217160224914551\n",
      "2.2194905281066895\n",
      "2.2057790756225586\n",
      "2.2360453605651855\n",
      "2.2025022506713867\n",
      "2.1437511444091797\n",
      "2.190200090408325\n",
      "2.208638906478882\n",
      "2.19002628326416\n",
      "2.1913962364196777\n",
      "2.2105798721313477\n",
      "2.2461395263671875\n",
      "2.1688191890716553\n",
      "2.191411018371582\n",
      "2.174900531768799\n",
      "2.1532437801361084\n",
      "2.1700174808502197\n",
      "2.1818761825561523\n",
      "2.138455390930176\n",
      "2.1541097164154053\n",
      "2.1812796592712402\n",
      "2.143176555633545\n",
      "2.213221311569214\n",
      "2.2058522701263428\n",
      "2.192978858947754\n",
      "2.191741466522217\n",
      "2.2101187705993652\n",
      "2.2206592559814453\n",
      "2.17734432220459\n",
      "2.1714465618133545\n",
      "2.1985652446746826\n",
      "2.2165489196777344\n",
      "2.1718204021453857\n",
      "2.168454170227051\n",
      "2.1556339263916016\n",
      "2.1836938858032227\n",
      "2.181908369064331\n",
      "2.189828872680664\n",
      "2.134676933288574\n",
      "2.195510149002075\n",
      "2.203857660293579\n",
      "2.1649184226989746\n",
      "2.189631700515747\n",
      "2.14479660987854\n",
      "2.1735634803771973\n",
      "2.193915367126465\n",
      "2.2010388374328613\n",
      "2.195704936981201\n",
      "2.1351847648620605\n",
      "2.1503233909606934\n",
      "2.134044647216797\n",
      "2.0958609580993652\n",
      "2.15773344039917\n",
      "2.1596970558166504\n",
      "2.2388744354248047\n",
      "2.129293441772461\n",
      "2.152798891067505\n",
      "2.147582769393921\n",
      "2.1437625885009766\n",
      "2.235024929046631\n",
      "2.213984966278076\n",
      "2.1681394577026367\n",
      "2.1792235374450684\n",
      "2.142444133758545\n",
      "2.111114501953125\n",
      "2.18404483795166\n",
      "2.1572861671447754\n",
      "2.1465871334075928\n",
      "2.2235846519470215\n",
      "2.1514792442321777\n",
      "2.164532423019409\n",
      "2.150678873062134\n",
      "2.201664924621582\n",
      "2.17789888381958\n",
      "2.2080025672912598\n",
      "2.105740547180176\n",
      "2.174561023712158\n",
      "2.1938538551330566\n",
      "2.1511006355285645\n",
      "2.1385953426361084\n",
      "2.18904447555542\n",
      "2.1900739669799805\n",
      "2.173501491546631\n",
      "2.180816888809204\n",
      "2.1283860206604004\n",
      "2.180124521255493\n",
      "2.182371139526367\n",
      "2.11018705368042\n",
      "2.144192695617676\n",
      "2.1670422554016113\n",
      "2.140517234802246\n",
      "2.163231134414673\n",
      "2.1176347732543945\n",
      "2.179861068725586\n",
      "2.163966655731201\n",
      "2.1610169410705566\n",
      "2.207798957824707\n",
      "2.129150390625\n",
      "2.193403482437134\n",
      "2.201122999191284\n",
      "2.0986311435699463\n",
      "2.165055274963379\n",
      "2.1614110469818115\n",
      "2.1729040145874023\n",
      "2.168909788131714\n",
      "2.1058835983276367\n",
      "2.1935348510742188\n",
      "2.1524012088775635\n",
      "2.196995973587036\n",
      "2.1718339920043945\n",
      "2.1420397758483887\n",
      "2.1234054565429688\n",
      "2.1587464809417725\n",
      "2.1443979740142822\n",
      "2.136162519454956\n",
      "2.169508695602417\n",
      "2.156393051147461\n",
      "2.2034006118774414\n",
      "2.123595714569092\n",
      "2.0896830558776855\n",
      "2.152496337890625\n",
      "2.1678967475891113\n",
      "2.125791549682617\n",
      "2.078359603881836\n",
      "2.1353039741516113\n",
      "2.12913179397583\n",
      "2.098621129989624\n",
      "2.133498430252075\n",
      "2.120607614517212\n",
      "2.1266307830810547\n",
      "2.1769087314605713\n",
      "2.0887374877929688\n",
      "2.088017463684082\n",
      "2.150197982788086\n",
      "2.122831106185913\n",
      "2.127080202102661\n",
      "2.0754144191741943\n",
      "2.137387752532959\n",
      "2.1386730670928955\n",
      "2.12094783782959\n",
      "2.1209068298339844\n",
      "2.128588914871216\n",
      "2.1551809310913086\n",
      "2.1127068996429443\n",
      "2.1403493881225586\n",
      "2.2019383907318115\n",
      "2.1500346660614014\n",
      "2.1027698516845703\n",
      "2.1688895225524902\n",
      "2.1790292263031006\n",
      "2.1226954460144043\n",
      "2.0867490768432617\n",
      "2.1565818786621094\n",
      "2.1856775283813477\n",
      "2.161198377609253\n",
      "2.162810802459717\n",
      "2.1617605686187744\n",
      "2.195389747619629\n",
      "2.129404306411743\n",
      "2.127918243408203\n",
      "2.1548357009887695\n",
      "2.1312756538391113\n",
      "2.1410112380981445\n",
      "2.1190567016601562\n",
      "2.1439547538757324\n",
      "2.1460328102111816\n",
      "2.1153740882873535\n",
      "2.1723296642303467\n",
      "2.104524612426758\n",
      "2.11008882522583\n",
      "2.0941197872161865\n",
      "2.1419811248779297\n",
      "2.1236910820007324\n",
      "2.082719564437866\n",
      "2.1505250930786133\n",
      "2.1157612800598145\n",
      "2.1029257774353027\n",
      "2.1118810176849365\n",
      "2.1021547317504883\n",
      "2.1149628162384033\n",
      "2.1527462005615234\n",
      "2.1390442848205566\n",
      "2.1034038066864014\n",
      "2.138453722000122\n",
      "2.0665225982666016\n",
      "2.1259822845458984\n",
      "2.1250290870666504\n",
      "2.1416664123535156\n",
      "2.132188320159912\n",
      "2.1402413845062256\n",
      "2.1314008235931396\n",
      "2.1375722885131836\n",
      "2.1044793128967285\n",
      "2.1143031120300293\n",
      "2.0738260746002197\n",
      "2.1610989570617676\n",
      "2.1712331771850586\n",
      "2.1343343257904053\n",
      "2.1471776962280273\n",
      "2.1501474380493164\n",
      "2.1340012550354004\n",
      "2.1054821014404297\n",
      "2.141913652420044\n",
      "2.0776867866516113\n",
      "2.1290283203125\n",
      "2.1102442741394043\n",
      "2.1107177734375\n",
      "2.133319616317749\n",
      "2.1224660873413086\n",
      "2.0847485065460205\n",
      "2.1380834579467773\n",
      "2.078627586364746\n",
      "2.106856107711792\n",
      "2.097794532775879\n",
      "2.1113412380218506\n",
      "2.0831780433654785\n",
      "2.132934331893921\n",
      "2.117920398712158\n",
      "2.1615219116210938\n",
      "2.1271309852600098\n",
      "2.1117196083068848\n",
      "2.113004684448242\n",
      "2.1243138313293457\n",
      "2.1328864097595215\n",
      "2.125668525695801\n",
      "2.0883524417877197\n",
      "2.133812427520752\n",
      "2.09503173828125\n",
      "2.1141653060913086\n",
      "2.1456522941589355\n",
      "2.145711898803711\n",
      "2.145737409591675\n",
      "2.050077438354492\n",
      "2.120354652404785\n",
      "2.1022486686706543\n",
      "2.076263427734375\n",
      "2.1287102699279785\n",
      "2.142148494720459\n",
      "2.0951340198516846\n",
      "2.151467800140381\n",
      "2.1100382804870605\n",
      "2.1055808067321777\n",
      "2.091132402420044\n",
      "2.1363048553466797\n",
      "2.1059844493865967\n",
      "2.106842517852783\n",
      "2.0903074741363525\n",
      "2.1055493354797363\n",
      "2.1177115440368652\n",
      "2.0941996574401855\n",
      "2.1138477325439453\n",
      "2.1256651878356934\n",
      "2.112466335296631\n",
      "2.092312812805176\n",
      "2.0977938175201416\n",
      "2.1551694869995117\n",
      "2.1388003826141357\n",
      "2.117152690887451\n",
      "2.127535820007324\n",
      "2.117415189743042\n",
      "2.0929906368255615\n",
      "2.101712226867676\n",
      "2.0465087890625\n",
      "2.082566738128662\n",
      "2.1184659004211426\n",
      "2.0899040699005127\n",
      "2.1164536476135254\n",
      "2.107509136199951\n",
      "2.0667576789855957\n",
      "2.132045269012451\n",
      "2.0941929817199707\n",
      "2.066706657409668\n",
      "2.0543437004089355\n",
      "2.0939393043518066\n",
      "2.0720129013061523\n",
      "2.06948184967041\n",
      "2.0935311317443848\n",
      "2.092977523803711\n",
      "2.0975236892700195\n",
      "2.072417974472046\n",
      "2.128342628479004\n",
      "2.105283737182617\n",
      "2.037106990814209\n",
      "2.0997071266174316\n",
      "2.097005844116211\n",
      "2.1093130111694336\n",
      "2.119114637374878\n",
      "2.083631753921509\n",
      "2.1235461235046387\n",
      "2.098775863647461\n",
      "2.107138156890869\n",
      "2.085681676864624\n",
      "2.033677577972412\n",
      "2.079145908355713\n",
      "2.0927066802978516\n",
      "2.050182819366455\n",
      "2.0134501457214355\n",
      "2.0895161628723145\n",
      "2.072089672088623\n",
      "2.1191349029541016\n",
      "2.101482391357422\n",
      "2.119434118270874\n",
      "2.1020700931549072\n",
      "2.0973691940307617\n",
      "2.0711774826049805\n",
      "2.0988430976867676\n",
      "2.0800108909606934\n",
      "2.098708152770996\n",
      "2.1307578086853027\n",
      "2.05694842338562\n",
      "2.1218514442443848\n",
      "2.105962038040161\n",
      "2.0752737522125244\n",
      "2.045073986053467\n",
      "2.1019344329833984\n",
      "2.1105382442474365\n",
      "2.0719757080078125\n",
      "2.036961078643799\n",
      "2.1478495597839355\n",
      "2.087374687194824\n",
      "2.0785725116729736\n",
      "2.100429058074951\n",
      "2.090095281600952\n",
      "2.076639175415039\n",
      "2.1302266120910645\n",
      "2.0948054790496826\n",
      "2.0738179683685303\n",
      "2.1130948066711426\n",
      "2.093938112258911\n",
      "2.120562791824341\n",
      "2.0882554054260254\n",
      "2.058485984802246\n",
      "2.1006600856781006\n",
      "2.1094932556152344\n",
      "2.1165525913238525\n",
      "2.063314914703369\n",
      "2.095524787902832\n",
      "2.1294243335723877\n",
      "2.089958667755127\n",
      "2.0755865573883057\n",
      "2.114083766937256\n",
      "2.114694356918335\n",
      "2.1258301734924316\n",
      "2.090703010559082\n",
      "2.024553060531616\n",
      "2.0501508712768555\n",
      "2.035970687866211\n",
      "2.0848004817962646\n",
      "2.0459001064300537\n",
      "2.0540807247161865\n",
      "2.065528631210327\n",
      "2.0661141872406006\n",
      "2.08903431892395\n",
      "2.037827968597412\n",
      "2.1108202934265137\n",
      "2.0823099613189697\n",
      "2.1040761470794678\n",
      "2.0581533908843994\n",
      "2.0695109367370605\n",
      "2.0967864990234375\n",
      "2.0925302505493164\n",
      "1.9991776943206787\n",
      "2.1141960620880127\n",
      "2.0394647121429443\n",
      "2.094758987426758\n",
      "2.070514440536499\n",
      "2.0643839836120605\n",
      "2.0742647647857666\n",
      "2.087022304534912\n",
      "2.008808135986328\n",
      "2.081921339035034\n",
      "2.057734966278076\n",
      "2.0717544555664062\n",
      "2.0548253059387207\n",
      "2.0744259357452393\n",
      "2.049684762954712\n",
      "2.063448429107666\n",
      "2.1188082695007324\n",
      "2.025221824645996\n",
      "2.078457832336426\n",
      "2.0423030853271484\n",
      "2.0571084022521973\n",
      "2.0832624435424805\n",
      "2.094722032546997\n",
      "2.104071617126465\n",
      "2.1184568405151367\n",
      "2.062312126159668\n",
      "2.038172960281372\n",
      "2.0585532188415527\n",
      "2.103437900543213\n",
      "2.1140787601470947\n",
      "2.0688347816467285\n",
      "2.0789849758148193\n",
      "2.063638210296631\n",
      "2.0853261947631836\n",
      "2.0851879119873047\n",
      "2.087179660797119\n",
      "2.071455478668213\n",
      "2.0426151752471924\n",
      "2.085627555847168\n",
      "2.0914483070373535\n",
      "2.0548672676086426\n",
      "2.0676321983337402\n",
      "2.0539956092834473\n",
      "2.0314295291900635\n",
      "2.073856830596924\n",
      "2.068568229675293\n",
      "2.043262243270874\n",
      "2.0898404121398926\n",
      "2.02175235748291\n",
      "2.0512659549713135\n",
      "2.0644619464874268\n",
      "1.982065200805664\n",
      "2.0432639122009277\n",
      "2.0369274616241455\n",
      "2.018117666244507\n",
      "2.0415267944335938\n",
      "2.0929923057556152\n",
      "2.0354292392730713\n",
      "2.0115365982055664\n",
      "2.0738754272460938\n",
      "2.054964780807495\n",
      "2.094099760055542\n",
      "2.075063943862915\n",
      "2.0448899269104004\n",
      "2.0247080326080322\n",
      "2.065894842147827\n",
      "2.087045907974243\n",
      "2.0268430709838867\n",
      "2.039517402648926\n",
      "2.076444149017334\n",
      "2.086392402648926\n",
      "2.0651962757110596\n",
      "2.0661752223968506\n",
      "2.0247364044189453\n",
      "2.049348831176758\n",
      "2.022467851638794\n",
      "2.099592924118042\n",
      "2.055027961730957\n",
      "2.01324462890625\n",
      "2.0520453453063965\n",
      "2.0687530040740967\n",
      "2.0658645629882812\n",
      "2.0533628463745117\n",
      "2.0745351314544678\n",
      "2.113718032836914\n",
      "2.0325212478637695\n",
      "2.018191337585449\n",
      "2.0584158897399902\n",
      "2.0533528327941895\n",
      "2.037447929382324\n",
      "2.044677495956421\n",
      "2.088174819946289\n",
      "2.0556130409240723\n",
      "2.0063955783843994\n",
      "2.0640039443969727\n",
      "2.0683443546295166\n",
      "2.0554447174072266\n",
      "2.0440521240234375\n",
      "2.061641216278076\n",
      "2.0665106773376465\n",
      "2.101311683654785\n",
      "2.0655837059020996\n",
      "2.077901840209961\n",
      "2.0518832206726074\n",
      "2.0258188247680664\n",
      "2.033517360687256\n",
      "2.090482711791992\n",
      "2.0037074089050293\n",
      "2.0255417823791504\n",
      "2.0499019622802734\n",
      "2.067636728286743\n",
      "2.041468620300293\n",
      "2.095299482345581\n",
      "2.0618703365325928\n",
      "2.1044068336486816\n",
      "2.076681137084961\n",
      "2.0788822174072266\n",
      "2.0732316970825195\n",
      "2.0649280548095703\n",
      "2.02150297164917\n",
      "2.0632433891296387\n",
      "1.9610824584960938\n",
      "2.012755870819092\n",
      "2.012836217880249\n",
      "2.0192251205444336\n",
      "2.071474075317383\n",
      "2.1036109924316406\n",
      "2.042426109313965\n",
      "2.0529956817626953\n",
      "2.048959970474243\n",
      "2.0087697505950928\n",
      "2.044060230255127\n",
      "2.0441315174102783\n",
      "2.0815792083740234\n",
      "2.0493698120117188\n",
      "2.0519802570343018\n",
      "2.023472309112549\n",
      "2.0116562843322754\n",
      "2.0431551933288574\n",
      "2.0598530769348145\n",
      "2.0803260803222656\n",
      "2.0194711685180664\n",
      "2.0819430351257324\n",
      "2.0305302143096924\n",
      "2.0961928367614746\n",
      "2.045227289199829\n",
      "2.0641252994537354\n",
      "2.0605392456054688\n",
      "2.033083438873291\n",
      "2.0393080711364746\n",
      "2.032029151916504\n",
      "2.088778018951416\n",
      "2.0318446159362793\n",
      "2.063383102416992\n",
      "2.042789936065674\n",
      "2.0671889781951904\n",
      "2.069328784942627\n",
      "2.005913734436035\n",
      "2.0124502182006836\n",
      "2.0378546714782715\n",
      "2.0972461700439453\n",
      "2.0651659965515137\n",
      "2.041710376739502\n",
      "1.981377124786377\n",
      "2.0776424407958984\n",
      "2.0176186561584473\n",
      "2.013698101043701\n",
      "2.069129467010498\n",
      "2.0361921787261963\n",
      "2.056781768798828\n",
      "2.047334671020508\n",
      "2.0766966342926025\n",
      "2.0068857669830322\n",
      "2.0789644718170166\n",
      "2.009141206741333\n",
      "2.0142152309417725\n",
      "2.0484323501586914\n",
      "2.064438819885254\n",
      "2.069415807723999\n",
      "1.932098150253296\n",
      "2.0496749877929688\n",
      "2.022984743118286\n",
      "2.0088906288146973\n",
      "2.023460865020752\n",
      "2.005411148071289\n",
      "2.0413830280303955\n",
      "2.0679984092712402\n",
      "1.9941577911376953\n",
      "2.0418763160705566\n",
      "2.006671667098999\n",
      "2.038426399230957\n",
      "2.0315651893615723\n",
      "2.1073803901672363\n",
      "2.0683093070983887\n",
      "2.0492966175079346\n",
      "1.9877374172210693\n",
      "2.0364108085632324\n",
      "2.015303611755371\n",
      "2.050466299057007\n",
      "2.0501158237457275\n",
      "2.0544660091400146\n",
      "2.0388259887695312\n",
      "2.004136562347412\n",
      "2.045745611190796\n",
      "2.0451953411102295\n",
      "2.016596794128418\n",
      "2.032031536102295\n",
      "2.0383639335632324\n",
      "2.013031244277954\n",
      "2.0807418823242188\n",
      "2.0499625205993652\n",
      "2.011101007461548\n",
      "2.0251855850219727\n",
      "2.0893349647521973\n",
      "2.07755708694458\n",
      "2.0340189933776855\n",
      "2.068300724029541\n",
      "2.0391616821289062\n",
      "2.0058770179748535\n",
      "1.9874932765960693\n",
      "2.0689990520477295\n",
      "2.0642659664154053\n",
      "2.046135663986206\n",
      "2.0767414569854736\n",
      "1.9951872825622559\n",
      "1.9984450340270996\n",
      "2.1109981536865234\n",
      "2.0325746536254883\n",
      "2.002485752105713\n",
      "2.048095226287842\n",
      "2.0701842308044434\n",
      "2.0359628200531006\n",
      "2.0217275619506836\n",
      "1.9971439838409424\n",
      "2.0401883125305176\n",
      "2.0311663150787354\n",
      "2.0413761138916016\n",
      "2.0428149700164795\n",
      "2.0570621490478516\n",
      "2.062751293182373\n",
      "2.0407252311706543\n",
      "2.0489823818206787\n",
      "2.030510663986206\n",
      "2.0579192638397217\n",
      "2.029235601425171\n",
      "1.9936574697494507\n",
      "2.0274605751037598\n",
      "1.9881129264831543\n",
      "2.025514841079712\n",
      "1.9698452949523926\n",
      "1.9824397563934326\n",
      "2.0504589080810547\n",
      "2.017350673675537\n",
      "1.9849694967269897\n",
      "1.9901714324951172\n",
      "2.0089917182922363\n",
      "2.018800735473633\n",
      "2.0249271392822266\n",
      "1.975145697593689\n",
      "2.0153703689575195\n",
      "2.0275321006774902\n",
      "2.049098014831543\n",
      "1.9716719388961792\n",
      "1.9851880073547363\n",
      "2.053788661956787\n",
      "2.0166420936584473\n",
      "2.0283420085906982\n",
      "1.9919887781143188\n",
      "2.010089635848999\n",
      "1.945847511291504\n",
      "2.0104241371154785\n",
      "1.9887505769729614\n",
      "2.002028703689575\n",
      "2.0126302242279053\n",
      "2.0320870876312256\n",
      "2.0225448608398438\n",
      "2.006010055541992\n",
      "2.055568218231201\n",
      "2.0580639839172363\n",
      "2.0177412033081055\n",
      "2.0031991004943848\n",
      "2.0448620319366455\n",
      "2.066979169845581\n",
      "2.036973476409912\n",
      "2.0021228790283203\n",
      "2.045989513397217\n",
      "2.03004789352417\n",
      "2.036098003387451\n",
      "1.9889931678771973\n",
      "2.0187056064605713\n",
      "2.027799606323242\n",
      "2.0270774364471436\n",
      "2.0472426414489746\n",
      "2.0121891498565674\n",
      "2.0226049423217773\n",
      "2.052593231201172\n",
      "1.9938290119171143\n",
      "1.9908777475357056\n",
      "2.03413724899292\n",
      "1.9869011640548706\n",
      "2.037503480911255\n",
      "2.0009331703186035\n",
      "1.998680830001831\n",
      "2.0234274864196777\n",
      "1.9860132932662964\n",
      "1.9965524673461914\n",
      "2.0374813079833984\n",
      "1.9981051683425903\n",
      "2.001707077026367\n",
      "2.0271711349487305\n",
      "2.007148504257202\n",
      "2.0443286895751953\n",
      "2.0362162590026855\n",
      "2.0246357917785645\n",
      "2.025998115539551\n",
      "2.0374488830566406\n",
      "2.014716625213623\n",
      "2.009113311767578\n",
      "2.0454635620117188\n",
      "2.004260301589966\n",
      "2.0160441398620605\n",
      "2.05941104888916\n",
      "2.0303401947021484\n",
      "2.02608060836792\n",
      "2.019920587539673\n",
      "2.0230026245117188\n",
      "2.0331668853759766\n",
      "2.026909112930298\n",
      "2.015505313873291\n",
      "2.018632650375366\n",
      "1.9941824674606323\n",
      "2.0031492710113525\n",
      "2.0128848552703857\n",
      "2.064627170562744\n",
      "2.033796787261963\n",
      "2.017425060272217\n",
      "1.9282073974609375\n",
      "2.0364785194396973\n",
      "2.038099765777588\n",
      "1.996739149093628\n",
      "1.9862349033355713\n",
      "2.022599220275879\n",
      "1.994144320487976\n",
      "1.9981849193572998\n",
      "1.9858604669570923\n",
      "1.9926223754882812\n",
      "1.9875259399414062\n",
      "2.0077407360076904\n",
      "2.0599687099456787\n",
      "1.998427391052246\n",
      "1.9903820753097534\n",
      "2.0151216983795166\n",
      "1.9765511751174927\n",
      "2.003486156463623\n",
      "2.053859233856201\n",
      "2.011657953262329\n",
      "1.9826557636260986\n",
      "2.0080814361572266\n",
      "1.9757899045944214\n",
      "1.9878768920898438\n",
      "2.040874719619751\n",
      "2.013969898223877\n",
      "1.9917421340942383\n",
      "2.040411949157715\n",
      "2.0244483947753906\n",
      "1.9995968341827393\n",
      "2.0198569297790527\n",
      "1.9993782043457031\n",
      "1.9890401363372803\n",
      "1.9703021049499512\n",
      "2.0481014251708984\n",
      "1.9915266036987305\n",
      "1.9799721240997314\n",
      "1.993971824645996\n",
      "1.9969191551208496\n",
      "2.0402047634124756\n",
      "2.0513100624084473\n",
      "2.010793685913086\n",
      "2.01829195022583\n",
      "2.020754337310791\n",
      "2.0017483234405518\n",
      "1.9883573055267334\n",
      "2.0354397296905518\n",
      "2.0102381706237793\n",
      "2.003634452819824\n",
      "2.0241122245788574\n",
      "1.9448680877685547\n",
      "1.986826777458191\n",
      "2.0368072986602783\n",
      "1.993557095527649\n",
      "2.0328903198242188\n",
      "1.9930133819580078\n",
      "2.0140600204467773\n",
      "2.0185489654541016\n",
      "2.0348567962646484\n",
      "1.994739055633545\n",
      "1.97727370262146\n",
      "1.9927077293395996\n",
      "1.9726529121398926\n",
      "1.9990270137786865\n",
      "1.9874169826507568\n",
      "1.9888843297958374\n",
      "2.0069398880004883\n",
      "1.972860336303711\n",
      "2.015838146209717\n",
      "2.0368752479553223\n",
      "2.009467124938965\n",
      "1.9953248500823975\n",
      "1.9631283283233643\n",
      "1.985498309135437\n",
      "1.9950083494186401\n",
      "1.961058259010315\n",
      "1.9888074398040771\n",
      "1.9856328964233398\n",
      "2.008685827255249\n",
      "2.018134593963623\n",
      "1.9834809303283691\n",
      "1.9806767702102661\n",
      "2.011814832687378\n",
      "2.013429641723633\n",
      "2.0378971099853516\n",
      "2.0091142654418945\n",
      "2.009321689605713\n",
      "2.0358545780181885\n",
      "1.9883873462677002\n",
      "1.991310954093933\n",
      "2.0173959732055664\n",
      "1.9535415172576904\n",
      "1.9969455003738403\n",
      "1.9841605424880981\n",
      "1.9395185708999634\n",
      "1.983689308166504\n",
      "2.0218052864074707\n",
      "1.973842740058899\n",
      "1.9599465131759644\n",
      "1.9605724811553955\n",
      "1.991544485092163\n",
      "1.975609302520752\n",
      "1.979384183883667\n",
      "2.005675792694092\n",
      "1.9768807888031006\n",
      "1.9115387201309204\n",
      "1.9339158535003662\n",
      "1.9490668773651123\n",
      "1.9644999504089355\n",
      "2.033686637878418\n",
      "1.9535973072052002\n",
      "1.9815014600753784\n",
      "2.0123844146728516\n",
      "2.0179805755615234\n",
      "1.9629257917404175\n",
      "2.0061957836151123\n",
      "1.9934508800506592\n",
      "1.9655112028121948\n",
      "2.024353504180908\n",
      "2.027434825897217\n",
      "2.002073287963867\n",
      "1.9873652458190918\n",
      "2.002056360244751\n",
      "1.942178726196289\n",
      "1.9820332527160645\n",
      "2.0050253868103027\n",
      "2.0080668926239014\n",
      "1.9685324430465698\n",
      "2.043485164642334\n",
      "1.9692325592041016\n",
      "1.9544037580490112\n",
      "2.0214896202087402\n",
      "1.9838643074035645\n",
      "2.0031373500823975\n",
      "1.9956871271133423\n",
      "2.011491298675537\n",
      "1.9961011409759521\n",
      "1.9959765672683716\n",
      "1.9927725791931152\n",
      "1.984717607498169\n",
      "2.0066733360290527\n",
      "2.042597532272339\n",
      "2.03088116645813\n",
      "2.0403833389282227\n",
      "1.9767550230026245\n",
      "2.0017595291137695\n",
      "2.018826723098755\n",
      "1.9990198612213135\n",
      "1.9408456087112427\n",
      "1.958051085472107\n",
      "2.013744831085205\n",
      "2.031585454940796\n",
      "1.9870489835739136\n",
      "1.9651827812194824\n",
      "2.026115894317627\n",
      "1.9814445972442627\n",
      "1.9900637865066528\n",
      "2.0418407917022705\n",
      "1.9859983921051025\n",
      "2.0187506675720215\n",
      "1.9971877336502075\n",
      "1.9228723049163818\n",
      "1.9592761993408203\n",
      "1.9636335372924805\n",
      "2.036541700363159\n",
      "1.9904835224151611\n",
      "1.9867985248565674\n",
      "1.954363226890564\n",
      "1.9570765495300293\n",
      "2.0174522399902344\n",
      "1.9747869968414307\n",
      "1.9527777433395386\n",
      "1.9539049863815308\n",
      "2.0110549926757812\n",
      "1.9894073009490967\n",
      "1.9850914478302002\n",
      "2.0121591091156006\n",
      "1.9865118265151978\n",
      "2.0075595378875732\n",
      "1.9972004890441895\n",
      "2.005878448486328\n",
      "1.9697811603546143\n",
      "2.0015883445739746\n",
      "1.9797903299331665\n",
      "2.003420352935791\n",
      "2.0037460327148438\n",
      "1.9943535327911377\n",
      "1.9678490161895752\n",
      "1.992368221282959\n",
      "1.982686996459961\n",
      "1.9755501747131348\n",
      "1.9166332483291626\n",
      "1.9611139297485352\n",
      "2.051631450653076\n",
      "2.0082216262817383\n",
      "1.9161109924316406\n",
      "1.9189951419830322\n",
      "1.9956352710723877\n",
      "2.0138397216796875\n",
      "2.012652635574341\n",
      "1.9824401140213013\n",
      "1.9819588661193848\n",
      "2.0267765522003174\n",
      "1.97679603099823\n",
      "1.9550859928131104\n",
      "1.959733009338379\n",
      "1.9797217845916748\n",
      "2.0061802864074707\n",
      "1.9550020694732666\n",
      "1.9716079235076904\n",
      "1.9810738563537598\n",
      "1.9649853706359863\n",
      "1.957918405532837\n",
      "1.9817054271697998\n",
      "2.0086112022399902\n",
      "1.9939972162246704\n",
      "1.985905647277832\n",
      "2.0303797721862793\n",
      "1.9671934843063354\n",
      "1.947115421295166\n",
      "1.9683841466903687\n",
      "1.9793531894683838\n",
      "1.969374179840088\n",
      "2.016423225402832\n",
      "2.012753963470459\n",
      "1.9451171159744263\n",
      "2.001519203186035\n",
      "1.996703863143921\n",
      "2.0072526931762695\n",
      "1.9521832466125488\n",
      "1.9608221054077148\n",
      "1.9681788682937622\n",
      "1.9769742488861084\n",
      "2.012418270111084\n",
      "1.958386778831482\n",
      "1.9952226877212524\n",
      "1.9546616077423096\n",
      "1.97836172580719\n",
      "1.9812878370285034\n",
      "1.9813580513000488\n",
      "1.9703099727630615\n",
      "2.0336108207702637\n",
      "1.9813989400863647\n",
      "2.0040416717529297\n",
      "1.9941599369049072\n",
      "2.0185022354125977\n",
      "1.9634621143341064\n",
      "1.9706575870513916\n",
      "1.9591248035430908\n",
      "1.962997555732727\n",
      "1.9387047290802002\n",
      "2.020808219909668\n",
      "1.9846547842025757\n",
      "1.9750561714172363\n",
      "1.9090579748153687\n",
      "1.9813575744628906\n",
      "1.9869441986083984\n",
      "1.9619474411010742\n",
      "1.9927293062210083\n",
      "1.9451544284820557\n",
      "1.9824625253677368\n",
      "2.004991292953491\n",
      "1.9779658317565918\n",
      "1.9921631813049316\n",
      "1.9440838098526\n",
      "1.9845376014709473\n",
      "1.9578919410705566\n",
      "1.9198166131973267\n",
      "2.008300304412842\n",
      "1.9778521060943604\n",
      "1.9535517692565918\n",
      "1.9787604808807373\n",
      "1.9351942539215088\n",
      "1.971954584121704\n",
      "1.9445557594299316\n",
      "1.9711346626281738\n",
      "1.9828240871429443\n",
      "1.9611576795578003\n",
      "1.981076717376709\n",
      "1.9578437805175781\n",
      "1.9979426860809326\n",
      "1.9841638803482056\n",
      "1.938723087310791\n",
      "1.9671028852462769\n",
      "1.9422860145568848\n",
      "1.9707956314086914\n",
      "1.9653785228729248\n",
      "1.8987674713134766\n",
      "1.9441800117492676\n",
      "1.9230306148529053\n",
      "1.9684665203094482\n",
      "2.0165371894836426\n",
      "1.9643149375915527\n",
      "2.011183261871338\n",
      "1.9795211553573608\n",
      "1.9888745546340942\n",
      "1.9801417589187622\n",
      "1.9539072513580322\n",
      "1.978128433227539\n",
      "1.9317630529403687\n",
      "1.9999796152114868\n",
      "1.9312069416046143\n",
      "1.9365733861923218\n",
      "1.9602675437927246\n",
      "1.9602106809616089\n",
      "1.9401074647903442\n",
      "1.985912799835205\n",
      "1.9773119688034058\n",
      "1.975867748260498\n",
      "1.965526819229126\n",
      "2.001295566558838\n",
      "1.9928569793701172\n",
      "2.003660202026367\n",
      "1.9825501441955566\n",
      "1.9924640655517578\n",
      "1.9964628219604492\n",
      "1.9469375610351562\n",
      "1.9284725189208984\n",
      "1.9631099700927734\n",
      "1.9674274921417236\n",
      "1.9625357389450073\n",
      "2.007194757461548\n",
      "1.9836292266845703\n",
      "1.9178451299667358\n",
      "1.9586586952209473\n",
      "1.9704749584197998\n",
      "2.008791446685791\n",
      "1.9452252388000488\n",
      "1.9594743251800537\n",
      "1.9896900653839111\n",
      "2.0048341751098633\n",
      "1.8973479270935059\n",
      "1.9845566749572754\n",
      "1.9633610248565674\n",
      "1.937217354774475\n",
      "1.9710955619812012\n",
      "1.9457905292510986\n",
      "1.9757626056671143\n",
      "1.992192268371582\n",
      "1.948852777481079\n",
      "1.9859201908111572\n",
      "1.9778339862823486\n",
      "2.0153889656066895\n",
      "1.9555561542510986\n",
      "2.0125694274902344\n",
      "1.9854247570037842\n",
      "1.9547529220581055\n",
      "1.9824950695037842\n",
      "1.9781460762023926\n",
      "1.943195104598999\n",
      "1.9781672954559326\n",
      "1.9914007186889648\n",
      "1.9450325965881348\n",
      "1.988297939300537\n",
      "2.002270221710205\n",
      "1.9476900100708008\n",
      "2.0040106773376465\n",
      "1.934022307395935\n",
      "1.9325631856918335\n",
      "1.9545013904571533\n",
      "1.9337573051452637\n",
      "1.9745497703552246\n",
      "1.973533272743225\n",
      "1.9664239883422852\n",
      "1.9721386432647705\n",
      "1.9622012376785278\n",
      "1.9350062608718872\n",
      "1.9638640880584717\n",
      "1.9849894046783447\n",
      "1.884587049484253\n",
      "1.9808015823364258\n",
      "1.9623479843139648\n",
      "1.9431737661361694\n",
      "1.933083415031433\n",
      "1.9633554220199585\n",
      "1.965092658996582\n",
      "1.9971859455108643\n",
      "2.0052411556243896\n",
      "1.9517805576324463\n",
      "1.9450905323028564\n",
      "2.0020365715026855\n",
      "1.97633957862854\n",
      "1.94944429397583\n",
      "1.940657138824463\n",
      "1.9359183311462402\n",
      "1.9661850929260254\n",
      "1.9031295776367188\n",
      "1.9106136560440063\n",
      "1.8670848608016968\n",
      "1.9811080694198608\n",
      "1.9774001836776733\n",
      "1.9126317501068115\n",
      "1.9344065189361572\n",
      "2.0074262619018555\n",
      "1.9977060556411743\n",
      "1.9292006492614746\n",
      "1.9717309474945068\n",
      "1.9593870639801025\n",
      "2.0134265422821045\n",
      "1.9597570896148682\n",
      "1.9432203769683838\n",
      "1.9568259716033936\n",
      "1.951779842376709\n",
      "1.9954023361206055\n",
      "1.9458634853363037\n",
      "1.9293231964111328\n",
      "1.9663565158843994\n",
      "1.956904411315918\n",
      "1.9395732879638672\n",
      "1.9263319969177246\n",
      "1.9910029172897339\n",
      "1.988569736480713\n",
      "1.9373549222946167\n",
      "1.9277852773666382\n",
      "1.9788153171539307\n",
      "1.9253424406051636\n",
      "1.9626402854919434\n",
      "1.9841554164886475\n",
      "1.9623442888259888\n",
      "1.9502182006835938\n",
      "1.9829685688018799\n",
      "1.9526731967926025\n",
      "1.9509769678115845\n",
      "1.983223557472229\n",
      "1.963828206062317\n",
      "2.016221523284912\n",
      "1.9619214534759521\n",
      "1.9881869554519653\n",
      "1.958651065826416\n",
      "1.9318904876708984\n",
      "1.954167127609253\n",
      "1.9248429536819458\n",
      "1.9048285484313965\n",
      "1.935868740081787\n",
      "1.9377999305725098\n",
      "1.9689122438430786\n",
      "1.9787375926971436\n",
      "1.9631798267364502\n",
      "1.9580402374267578\n",
      "1.9586033821105957\n",
      "1.9591069221496582\n",
      "1.9023622274398804\n",
      "1.9648220539093018\n",
      "1.9387755393981934\n",
      "1.9804296493530273\n",
      "1.8975211381912231\n",
      "1.9644429683685303\n",
      "1.9744725227355957\n",
      "1.9649953842163086\n",
      "1.9550204277038574\n",
      "1.9397106170654297\n",
      "1.946349859237671\n",
      "1.9604905843734741\n",
      "1.9483380317687988\n",
      "1.9614402055740356\n",
      "1.9683823585510254\n",
      "1.990578293800354\n",
      "1.9959603548049927\n",
      "2.01522159576416\n",
      "1.9205247163772583\n",
      "1.98824143409729\n",
      "1.9669326543807983\n",
      "1.948258399963379\n",
      "1.9397048950195312\n",
      "1.9766749143600464\n",
      "1.9763147830963135\n",
      "1.9874541759490967\n",
      "1.9401695728302002\n",
      "1.9438683986663818\n",
      "1.9568336009979248\n",
      "1.962581992149353\n",
      "1.9599113464355469\n",
      "1.9222556352615356\n",
      "1.983892798423767\n",
      "1.940345287322998\n",
      "1.9393646717071533\n",
      "1.9269107580184937\n",
      "1.9784419536590576\n",
      "1.9235165119171143\n",
      "1.9357377290725708\n",
      "1.9881980419158936\n",
      "1.9601536989212036\n",
      "1.9918227195739746\n",
      "1.9647762775421143\n",
      "1.9372926950454712\n",
      "1.9130808115005493\n",
      "1.9540672302246094\n",
      "1.9623197317123413\n",
      "1.9997344017028809\n",
      "1.8561303615570068\n",
      "1.9192181825637817\n",
      "1.933508276939392\n",
      "1.9268691539764404\n",
      "1.9129575490951538\n",
      "1.99311101436615\n",
      "1.9014537334442139\n",
      "1.937739610671997\n",
      "1.898928165435791\n",
      "1.9616494178771973\n",
      "1.9402108192443848\n",
      "1.9655828475952148\n",
      "1.9719053506851196\n",
      "1.9772320985794067\n",
      "1.9816535711288452\n",
      "1.9179089069366455\n",
      "1.973131775856018\n",
      "1.952754259109497\n",
      "1.957322120666504\n",
      "1.9157090187072754\n",
      "1.9573609828948975\n",
      "1.933800220489502\n",
      "1.9137320518493652\n",
      "1.9470486640930176\n",
      "1.9360467195510864\n",
      "1.9268498420715332\n",
      "1.9566694498062134\n",
      "1.958080768585205\n",
      "1.979853630065918\n",
      "1.9704315662384033\n",
      "1.9156420230865479\n",
      "1.886410117149353\n",
      "1.8820650577545166\n",
      "1.9310173988342285\n",
      "1.9742240905761719\n",
      "1.9588687419891357\n",
      "1.951249122619629\n",
      "1.933332085609436\n",
      "1.984956979751587\n",
      "1.8671365976333618\n",
      "1.9152650833129883\n",
      "1.9440944194793701\n",
      "1.9791104793548584\n",
      "1.9624197483062744\n",
      "1.9804741144180298\n",
      "1.949998140335083\n",
      "1.9557284116744995\n",
      "1.9207813739776611\n",
      "1.9208428859710693\n",
      "1.9033958911895752\n",
      "1.9162790775299072\n",
      "1.924477458000183\n",
      "1.8814728260040283\n",
      "1.919062852859497\n",
      "1.9863972663879395\n",
      "1.9767532348632812\n",
      "1.9237005710601807\n",
      "1.936940312385559\n",
      "1.8896019458770752\n",
      "1.9251163005828857\n",
      "1.9629652500152588\n",
      "1.9924356937408447\n",
      "1.9294207096099854\n",
      "1.9430994987487793\n",
      "1.9533348083496094\n",
      "1.9297988414764404\n",
      "1.9913684129714966\n",
      "1.945335030555725\n",
      "1.9603819847106934\n",
      "1.9039685726165771\n",
      "1.9083762168884277\n",
      "1.9156956672668457\n",
      "1.955991268157959\n",
      "1.9285039901733398\n",
      "1.9243571758270264\n",
      "1.951888084411621\n",
      "1.9684884548187256\n",
      "1.9248039722442627\n",
      "1.9234182834625244\n",
      "1.9438517093658447\n",
      "1.992732048034668\n",
      "1.9376220703125\n",
      "1.9211339950561523\n",
      "1.9106123447418213\n",
      "1.9218559265136719\n",
      "1.91244637966156\n",
      "1.947317123413086\n",
      "1.903874158859253\n",
      "1.9720747470855713\n",
      "1.9477053880691528\n",
      "1.9406274557113647\n",
      "1.945927381515503\n",
      "1.9054372310638428\n",
      "1.9418301582336426\n",
      "1.9213114976882935\n",
      "1.9509124755859375\n",
      "1.9318116903305054\n",
      "1.938974380493164\n",
      "1.9268121719360352\n",
      "1.92670476436615\n",
      "1.9169117212295532\n",
      "1.9108850955963135\n",
      "1.8820455074310303\n",
      "1.9115345478057861\n",
      "1.9500118494033813\n",
      "1.966983675956726\n",
      "1.9822750091552734\n",
      "1.945537805557251\n",
      "1.9414258003234863\n",
      "1.9722174406051636\n",
      "1.9392004013061523\n",
      "1.951660394668579\n",
      "1.9605940580368042\n",
      "1.9728572368621826\n",
      "1.9637033939361572\n",
      "1.9759149551391602\n",
      "1.9476590156555176\n",
      "1.9410568475723267\n",
      "1.9070333242416382\n",
      "1.9222078323364258\n",
      "1.9864317178726196\n",
      "1.9469038248062134\n",
      "1.967409610748291\n",
      "1.9777793884277344\n",
      "1.9396233558654785\n",
      "1.9789310693740845\n",
      "1.935591220855713\n",
      "2.000605344772339\n",
      "1.9648692607879639\n",
      "1.9084105491638184\n",
      "1.9036672115325928\n",
      "1.927478313446045\n",
      "1.9952739477157593\n",
      "1.9393088817596436\n",
      "1.9281929731369019\n",
      "1.9499521255493164\n",
      "1.9001619815826416\n",
      "1.967118263244629\n",
      "1.9091506004333496\n",
      "1.8830642700195312\n",
      "1.9840279817581177\n",
      "1.8771542310714722\n",
      "1.941060185432434\n",
      "1.9327609539031982\n",
      "1.8875453472137451\n",
      "1.9085713624954224\n",
      "1.9724552631378174\n",
      "1.925893783569336\n",
      "1.9924819469451904\n",
      "1.840122103691101\n",
      "1.9186427593231201\n",
      "1.8912047147750854\n",
      "1.9521030187606812\n",
      "1.9511795043945312\n",
      "1.9330010414123535\n",
      "1.959801435470581\n",
      "1.9461784362792969\n",
      "1.9167085886001587\n",
      "1.9298059940338135\n",
      "1.9196935892105103\n",
      "1.9323139190673828\n",
      "1.9812086820602417\n",
      "1.936924695968628\n",
      "1.9114927053451538\n",
      "1.927037000656128\n",
      "1.909621000289917\n",
      "1.971381425857544\n",
      "1.951136827468872\n",
      "1.9506444931030273\n",
      "1.9632898569107056\n",
      "1.9242130517959595\n",
      "1.9628872871398926\n",
      "1.9385079145431519\n",
      "1.8966064453125\n",
      "1.9465067386627197\n",
      "1.8980026245117188\n",
      "1.91768479347229\n",
      "1.9392744302749634\n",
      "1.8852070569992065\n",
      "1.9293272495269775\n",
      "1.9835149049758911\n",
      "1.9077835083007812\n",
      "1.9306986331939697\n",
      "1.9370753765106201\n",
      "1.8965139389038086\n",
      "1.9296741485595703\n",
      "1.94520103931427\n",
      "1.9226229190826416\n",
      "1.9625098705291748\n",
      "1.9423010349273682\n",
      "1.9014854431152344\n",
      "1.9271082878112793\n",
      "1.883611798286438\n",
      "1.8561464548110962\n",
      "1.9523398876190186\n",
      "1.914919376373291\n",
      "1.9702672958374023\n",
      "1.9270694255828857\n",
      "1.9210141897201538\n",
      "1.9009490013122559\n",
      "1.9671958684921265\n",
      "1.942345142364502\n",
      "1.936661958694458\n",
      "1.942628264427185\n",
      "1.9373314380645752\n",
      "1.9763782024383545\n",
      "1.972543716430664\n",
      "1.9743492603302002\n",
      "1.8944826126098633\n",
      "1.9175653457641602\n",
      "1.9254941940307617\n",
      "1.9106881618499756\n",
      "1.9266072511672974\n",
      "1.9485666751861572\n",
      "1.9092319011688232\n",
      "1.9353861808776855\n",
      "1.906776785850525\n",
      "1.8962697982788086\n",
      "1.9415287971496582\n",
      "1.9744043350219727\n",
      "1.8930745124816895\n",
      "1.9231081008911133\n",
      "1.9405348300933838\n",
      "1.9803178310394287\n",
      "1.9534790515899658\n",
      "1.8922109603881836\n",
      "1.940929651260376\n",
      "1.8959603309631348\n",
      "1.9027674198150635\n",
      "1.9707945585250854\n",
      "1.9223239421844482\n",
      "1.9440510272979736\n",
      "1.922055721282959\n",
      "1.9626598358154297\n",
      "1.9350166320800781\n",
      "1.896904706954956\n",
      "1.9064304828643799\n",
      "1.911073923110962\n",
      "1.9068384170532227\n",
      "1.9210814237594604\n",
      "1.9379866123199463\n",
      "1.9178907871246338\n",
      "1.9535372257232666\n",
      "1.9004034996032715\n",
      "1.8736766576766968\n",
      "1.8811805248260498\n",
      "1.9473817348480225\n",
      "1.9496080875396729\n",
      "1.9565593004226685\n",
      "1.9574244022369385\n",
      "1.9361876249313354\n",
      "1.8914271593093872\n",
      "1.9479981660842896\n",
      "1.9361202716827393\n",
      "1.8839519023895264\n",
      "1.8944441080093384\n",
      "1.9061592817306519\n",
      "1.8908740282058716\n",
      "1.9145431518554688\n",
      "1.935433030128479\n",
      "1.914939522743225\n",
      "1.9683880805969238\n",
      "1.9127918481826782\n",
      "1.9053981304168701\n",
      "1.9307153224945068\n",
      "1.9384808540344238\n",
      "1.9535973072052002\n",
      "1.942478895187378\n",
      "1.9370313882827759\n",
      "1.9560847282409668\n",
      "1.9506022930145264\n",
      "1.9333821535110474\n",
      "1.8700989484786987\n",
      "1.9712790250778198\n",
      "1.950803518295288\n",
      "1.9202237129211426\n",
      "1.8991117477416992\n",
      "1.8937705755233765\n",
      "1.951685905456543\n",
      "1.901552438735962\n",
      "1.8943243026733398\n",
      "1.9080774784088135\n",
      "1.8695032596588135\n",
      "1.9397696256637573\n",
      "1.8673558235168457\n",
      "1.9216279983520508\n",
      "1.9139647483825684\n",
      "1.9456465244293213\n",
      "1.941725492477417\n",
      "1.8851001262664795\n",
      "1.9241549968719482\n",
      "1.9029844999313354\n",
      "1.940293312072754\n",
      "1.924386739730835\n",
      "1.912670612335205\n",
      "1.9257066249847412\n",
      "1.9431805610656738\n",
      "1.9363818168640137\n",
      "1.9311435222625732\n",
      "1.9091546535491943\n",
      "1.95527982711792\n",
      "1.8976950645446777\n",
      "1.9105738401412964\n",
      "1.9048107862472534\n",
      "1.9224473237991333\n",
      "1.899420976638794\n",
      "1.934875249862671\n",
      "1.909292459487915\n",
      "1.8843210935592651\n",
      "1.9674856662750244\n",
      "1.9184446334838867\n",
      "1.933065414428711\n",
      "1.9255658388137817\n",
      "1.9525516033172607\n",
      "1.8909807205200195\n",
      "1.945432424545288\n",
      "1.9084800481796265\n",
      "1.9217748641967773\n",
      "1.8945097923278809\n",
      "1.9823884963989258\n",
      "1.936614990234375\n",
      "1.9377272129058838\n",
      "1.938909888267517\n",
      "1.906959056854248\n",
      "1.9382997751235962\n",
      "1.9557397365570068\n",
      "1.8992631435394287\n",
      "1.9036871194839478\n",
      "1.9341354370117188\n",
      "1.8989895582199097\n",
      "1.8544126749038696\n",
      "1.9268779754638672\n",
      "1.875841736793518\n",
      "1.9343150854110718\n",
      "1.957459568977356\n",
      "1.9032502174377441\n",
      "1.9105230569839478\n",
      "1.9709548950195312\n",
      "1.9430359601974487\n",
      "1.9249746799468994\n",
      "1.912302017211914\n",
      "1.881687045097351\n",
      "1.8868334293365479\n",
      "1.9561636447906494\n",
      "1.8777046203613281\n",
      "1.877945899963379\n",
      "1.9372644424438477\n",
      "1.9113816022872925\n",
      "1.9027940034866333\n",
      "1.855724811553955\n",
      "1.928326964378357\n",
      "1.9579678773880005\n",
      "1.8821821212768555\n",
      "1.9023597240447998\n",
      "1.9178712368011475\n",
      "1.891431212425232\n",
      "1.9417672157287598\n",
      "1.9194867610931396\n",
      "1.9198628664016724\n",
      "1.9317069053649902\n",
      "1.916490077972412\n",
      "1.9419556856155396\n",
      "1.890756607055664\n",
      "1.9745521545410156\n",
      "1.9230616092681885\n",
      "1.9138166904449463\n",
      "1.9409477710723877\n",
      "1.9106335639953613\n",
      "1.9024028778076172\n",
      "1.9071791172027588\n",
      "1.921381950378418\n",
      "1.937528371810913\n",
      "1.962010145187378\n",
      "1.8895699977874756\n",
      "1.9435675144195557\n",
      "1.9346531629562378\n",
      "1.876657485961914\n",
      "1.9468475580215454\n",
      "1.967169165611267\n",
      "1.9087998867034912\n",
      "1.896332025527954\n",
      "1.9265071153640747\n",
      "1.8925514221191406\n",
      "1.9184017181396484\n",
      "1.9359205961227417\n",
      "1.9423730373382568\n",
      "1.9493919610977173\n",
      "1.9112753868103027\n",
      "1.9409383535385132\n",
      "1.9179296493530273\n",
      "1.8925824165344238\n",
      "1.931715488433838\n",
      "1.943045735359192\n",
      "1.9144203662872314\n",
      "1.8847975730895996\n",
      "1.911490559577942\n",
      "1.9057741165161133\n",
      "1.9076731204986572\n",
      "1.9180577993392944\n",
      "1.9428995847702026\n",
      "1.950305461883545\n",
      "1.9065418243408203\n",
      "1.9359827041625977\n",
      "1.9211325645446777\n",
      "1.9305286407470703\n",
      "1.9290413856506348\n",
      "1.9123504161834717\n",
      "1.891075849533081\n",
      "1.9511092901229858\n",
      "1.97063410282135\n",
      "1.954940915107727\n",
      "1.9895267486572266\n",
      "1.9428788423538208\n",
      "1.898697853088379\n",
      "1.8874318599700928\n",
      "1.879812240600586\n",
      "1.9359533786773682\n",
      "1.862735629081726\n",
      "1.9532368183135986\n",
      "1.911048173904419\n",
      "1.911717414855957\n",
      "1.9308838844299316\n",
      "1.9091075658798218\n",
      "1.8950533866882324\n",
      "1.9201524257659912\n",
      "1.9373995065689087\n",
      "1.9098042249679565\n",
      "1.8898484706878662\n",
      "1.9137734174728394\n",
      "1.8686516284942627\n",
      "1.9562199115753174\n",
      "1.9010756015777588\n",
      "1.9331623315811157\n",
      "1.9101526737213135\n",
      "1.9494683742523193\n",
      "1.946669340133667\n",
      "1.9468073844909668\n",
      "1.923677921295166\n",
      "1.9483685493469238\n",
      "1.9330639839172363\n",
      "1.933876633644104\n",
      "1.8834357261657715\n",
      "1.8975073099136353\n",
      "1.9347645044326782\n",
      "1.9499043226242065\n",
      "1.8803483247756958\n",
      "1.8290424346923828\n",
      "1.8989243507385254\n",
      "1.9198517799377441\n",
      "1.891157627105713\n",
      "1.8984038829803467\n",
      "1.8719079494476318\n",
      "1.8952674865722656\n",
      "1.8995712995529175\n",
      "1.9180030822753906\n",
      "1.8543471097946167\n",
      "1.8817335367202759\n",
      "1.9165313243865967\n",
      "1.898909330368042\n",
      "1.9275085926055908\n",
      "1.9080173969268799\n",
      "1.909052848815918\n",
      "1.9357573986053467\n",
      "1.9267263412475586\n",
      "1.8995285034179688\n",
      "1.9216561317443848\n",
      "1.9223964214324951\n",
      "1.8371875286102295\n",
      "1.907259464263916\n",
      "1.8926202058792114\n",
      "1.9660613536834717\n",
      "1.8666132688522339\n",
      "1.8622820377349854\n",
      "1.8636529445648193\n",
      "1.9533376693725586\n",
      "1.9385298490524292\n",
      "1.9010252952575684\n",
      "1.9144048690795898\n",
      "1.906442403793335\n",
      "1.9093214273452759\n",
      "1.9238641262054443\n",
      "1.8589041233062744\n",
      "1.9144587516784668\n",
      "1.89415442943573\n",
      "1.9167840480804443\n",
      "1.944016695022583\n",
      "1.930330514907837\n",
      "1.8848083019256592\n",
      "1.9088873863220215\n",
      "1.9092042446136475\n",
      "1.848629117012024\n",
      "1.9092096090316772\n",
      "1.9165096282958984\n",
      "1.9030430316925049\n",
      "1.8773834705352783\n",
      "1.9134197235107422\n",
      "1.8915369510650635\n",
      "1.9268196821212769\n",
      "1.8776224851608276\n",
      "1.90897536277771\n",
      "1.8955681324005127\n",
      "1.93296217918396\n",
      "1.9101307392120361\n",
      "1.88308584690094\n",
      "1.9012844562530518\n",
      "1.9075615406036377\n",
      "1.898031234741211\n",
      "1.9014123678207397\n",
      "1.9157098531723022\n",
      "1.89806067943573\n",
      "1.9047987461090088\n",
      "1.9151815176010132\n",
      "1.9325733184814453\n",
      "1.871889352798462\n",
      "1.9166264533996582\n",
      "1.9632877111434937\n",
      "1.9015475511550903\n",
      "1.9274513721466064\n",
      "1.9187039136886597\n",
      "1.9095433950424194\n",
      "1.9438798427581787\n",
      "1.8837001323699951\n",
      "1.9248915910720825\n",
      "1.886317491531372\n",
      "1.8903861045837402\n",
      "1.8985185623168945\n",
      "1.9254653453826904\n",
      "1.9280247688293457\n",
      "1.8970510959625244\n",
      "1.9132568836212158\n",
      "1.9356427192687988\n",
      "1.8855540752410889\n",
      "1.878001093864441\n",
      "1.9346604347229004\n",
      "1.9076740741729736\n",
      "1.9229023456573486\n",
      "1.9068629741668701\n",
      "1.8288549184799194\n",
      "1.8810405731201172\n",
      "1.9241015911102295\n",
      "1.9465755224227905\n",
      "1.8937268257141113\n",
      "1.9216492176055908\n",
      "1.8976624011993408\n",
      "1.9522838592529297\n",
      "1.8781226873397827\n",
      "1.885854959487915\n",
      "1.8809514045715332\n",
      "1.9698412418365479\n",
      "1.8824325799942017\n",
      "1.873486876487732\n",
      "1.9088313579559326\n",
      "1.8842248916625977\n",
      "1.8591190576553345\n",
      "1.8899815082550049\n",
      "1.8903543949127197\n",
      "1.9208717346191406\n",
      "1.8728752136230469\n",
      "1.9386296272277832\n",
      "1.8838368654251099\n",
      "1.9175043106079102\n",
      "1.893563985824585\n",
      "1.9172775745391846\n",
      "1.9368174076080322\n",
      "1.9441187381744385\n",
      "1.8941795825958252\n",
      "1.9154819250106812\n",
      "1.8989441394805908\n",
      "1.8415486812591553\n",
      "1.8989523649215698\n",
      "1.905529260635376\n",
      "1.919493556022644\n",
      "1.910201907157898\n",
      "1.8305330276489258\n",
      "1.8659672737121582\n",
      "1.9091570377349854\n",
      "1.873342752456665\n",
      "1.899512529373169\n",
      "1.9255080223083496\n",
      "1.8983612060546875\n",
      "1.8512001037597656\n",
      "1.9292882680892944\n",
      "1.825019359588623\n",
      "1.9023412466049194\n",
      "1.8808364868164062\n",
      "1.9306645393371582\n",
      "1.8922780752182007\n",
      "1.874821424484253\n",
      "1.9167581796646118\n",
      "1.8326334953308105\n",
      "1.8618853092193604\n",
      "1.8492683172225952\n",
      "1.9277451038360596\n",
      "1.9299249649047852\n",
      "1.9580082893371582\n",
      "1.8940470218658447\n",
      "1.8276783227920532\n",
      "1.9260976314544678\n",
      "1.931779384613037\n",
      "1.940711498260498\n",
      "1.8823546171188354\n",
      "1.9330450296401978\n",
      "1.9095573425292969\n",
      "1.9006166458129883\n",
      "1.8833884000778198\n",
      "1.9164499044418335\n",
      "1.9203085899353027\n",
      "1.920194387435913\n",
      "1.9012833833694458\n",
      "1.9229342937469482\n",
      "1.9308300018310547\n",
      "1.8840277194976807\n",
      "1.9045320749282837\n",
      "1.8584105968475342\n",
      "1.9433443546295166\n",
      "1.9082796573638916\n",
      "1.8926458358764648\n",
      "1.9234508275985718\n",
      "1.9599661827087402\n",
      "1.8790457248687744\n",
      "1.9178658723831177\n",
      "1.8942396640777588\n",
      "1.913017988204956\n",
      "1.884830117225647\n",
      "1.90052330493927\n",
      "1.8650317192077637\n",
      "1.8611345291137695\n",
      "1.912658929824829\n",
      "1.9169301986694336\n",
      "1.920208215713501\n",
      "1.8760004043579102\n",
      "1.896785855293274\n",
      "1.8710283041000366\n",
      "1.8882369995117188\n",
      "1.892689824104309\n",
      "1.8814456462860107\n",
      "1.8892159461975098\n",
      "1.901329517364502\n",
      "1.8948605060577393\n",
      "1.8710635900497437\n",
      "1.9056354761123657\n",
      "1.8495819568634033\n",
      "1.9025788307189941\n",
      "1.8610377311706543\n",
      "1.9117655754089355\n",
      "1.8813670873641968\n",
      "1.9323525428771973\n",
      "1.8739701509475708\n",
      "1.9112272262573242\n",
      "1.877265214920044\n",
      "1.8879449367523193\n",
      "1.8614230155944824\n",
      "1.9322118759155273\n",
      "1.9235055446624756\n",
      "1.895223617553711\n",
      "1.9390677213668823\n",
      "1.8784401416778564\n",
      "1.908726692199707\n",
      "1.941021203994751\n",
      "1.886185884475708\n",
      "1.9227283000946045\n",
      "1.8454999923706055\n",
      "1.9017469882965088\n",
      "1.8835208415985107\n",
      "1.889847755432129\n",
      "1.9045298099517822\n",
      "1.9185552597045898\n",
      "1.8716846704483032\n",
      "1.8937304019927979\n",
      "1.878799319267273\n",
      "1.8857371807098389\n",
      "1.8667099475860596\n",
      "1.9253811836242676\n",
      "1.8856656551361084\n",
      "1.8810787200927734\n",
      "1.911280632019043\n",
      "1.8913657665252686\n",
      "1.9391822814941406\n",
      "1.894019603729248\n",
      "1.875708818435669\n",
      "1.918853998184204\n",
      "1.8705132007598877\n",
      "1.8599811792373657\n",
      "1.8793846368789673\n",
      "1.9139683246612549\n",
      "1.8529201745986938\n",
      "1.9258198738098145\n",
      "1.8641984462738037\n",
      "1.8804411888122559\n",
      "1.8875077962875366\n",
      "1.8441691398620605\n",
      "1.9225058555603027\n",
      "1.9046965837478638\n",
      "1.8777987957000732\n",
      "1.9198077917099\n",
      "1.890774130821228\n",
      "1.9186149835586548\n",
      "1.8783512115478516\n",
      "1.8653401136398315\n",
      "1.8987162113189697\n",
      "1.9017137289047241\n",
      "1.9242491722106934\n",
      "1.9179749488830566\n",
      "1.8936455249786377\n",
      "1.8993552923202515\n",
      "1.914330244064331\n",
      "1.9558597803115845\n",
      "1.9259048700332642\n",
      "1.894895315170288\n",
      "1.8833897113800049\n",
      "1.8929239511489868\n",
      "1.9108259677886963\n",
      "1.9198963642120361\n",
      "1.887277603149414\n",
      "1.8836755752563477\n",
      "1.9115707874298096\n",
      "1.902831792831421\n",
      "1.882071852684021\n",
      "1.8671808242797852\n",
      "1.880552053451538\n",
      "1.8730535507202148\n",
      "1.8559770584106445\n",
      "1.8992719650268555\n",
      "1.9344508647918701\n",
      "1.9271490573883057\n",
      "1.9105708599090576\n",
      "1.8625972270965576\n",
      "1.8796782493591309\n",
      "1.8577487468719482\n",
      "1.8887910842895508\n",
      "1.8898346424102783\n",
      "1.866079568862915\n",
      "1.8812402486801147\n",
      "1.8558577299118042\n",
      "1.8031067848205566\n",
      "1.8439258337020874\n",
      "1.8573335409164429\n",
      "1.874072551727295\n",
      "1.9252521991729736\n",
      "1.8809123039245605\n",
      "1.9012947082519531\n",
      "1.9052214622497559\n",
      "1.8757507801055908\n",
      "1.8909941911697388\n",
      "1.8671975135803223\n",
      "1.844876766204834\n",
      "1.8625314235687256\n",
      "1.9324264526367188\n",
      "1.901265025138855\n",
      "1.8932942152023315\n",
      "1.8555866479873657\n",
      "1.8702211380004883\n",
      "1.8768336772918701\n",
      "1.863451361656189\n",
      "1.873627781867981\n",
      "1.89471435546875\n",
      "1.894920825958252\n",
      "1.8381468057632446\n",
      "1.8927428722381592\n",
      "1.9213945865631104\n",
      "1.8837921619415283\n",
      "1.892514944076538\n",
      "1.8402948379516602\n",
      "1.9257912635803223\n",
      "1.8042125701904297\n",
      "1.8773646354675293\n",
      "1.8353931903839111\n",
      "1.9250444173812866\n",
      "1.860081672668457\n",
      "1.8973665237426758\n",
      "1.854661464691162\n",
      "1.896646499633789\n",
      "1.8582680225372314\n",
      "1.9196593761444092\n",
      "1.8819177150726318\n",
      "1.8683326244354248\n",
      "1.8700891733169556\n",
      "1.8975152969360352\n",
      "1.8725658655166626\n",
      "1.9367165565490723\n",
      "1.913165807723999\n",
      "1.8666599988937378\n",
      "1.873237133026123\n",
      "1.862972378730774\n",
      "1.8605823516845703\n",
      "1.9053914546966553\n",
      "1.8773281574249268\n",
      "1.8910635709762573\n",
      "1.883034110069275\n",
      "1.9123725891113281\n",
      "1.9250670671463013\n",
      "1.8742647171020508\n",
      "1.913954734802246\n",
      "1.9278931617736816\n",
      "1.8381937742233276\n",
      "1.9065139293670654\n",
      "1.8690857887268066\n",
      "1.928980827331543\n",
      "1.839251160621643\n",
      "1.9026201963424683\n",
      "1.8697922229766846\n",
      "1.8927381038665771\n",
      "1.9401119947433472\n",
      "1.7917238473892212\n",
      "1.8786579370498657\n",
      "1.8979662656784058\n",
      "1.9291574954986572\n",
      "1.8822021484375\n",
      "1.9250094890594482\n",
      "1.9407908916473389\n",
      "1.8865392208099365\n",
      "1.8613052368164062\n",
      "1.8497576713562012\n",
      "1.841094732284546\n",
      "1.907853126525879\n",
      "1.807976484298706\n",
      "1.8397294282913208\n",
      "1.898991584777832\n",
      "1.894801378250122\n",
      "1.9418621063232422\n",
      "1.8099910020828247\n",
      "1.8835840225219727\n",
      "1.9346575736999512\n",
      "1.8610360622406006\n",
      "1.9005647897720337\n",
      "1.8437917232513428\n",
      "1.8720526695251465\n",
      "1.8884166479110718\n",
      "1.9546515941619873\n",
      "1.8651247024536133\n",
      "1.883847713470459\n",
      "1.9014471769332886\n",
      "1.8772165775299072\n",
      "1.8814666271209717\n",
      "1.8905282020568848\n",
      "1.9098668098449707\n",
      "1.8785362243652344\n",
      "1.8403551578521729\n",
      "1.8395531177520752\n",
      "1.920114517211914\n",
      "1.8441717624664307\n",
      "1.8647830486297607\n",
      "1.8664990663528442\n",
      "1.8768630027770996\n",
      "1.917750597000122\n",
      "1.852492094039917\n",
      "1.8359344005584717\n",
      "1.8821337223052979\n",
      "1.8997125625610352\n",
      "1.9258136749267578\n",
      "1.9331778287887573\n",
      "1.8841909170150757\n",
      "1.880434513092041\n",
      "1.894448161125183\n",
      "1.8559935092926025\n",
      "1.8456857204437256\n",
      "1.9204227924346924\n",
      "1.8516321182250977\n",
      "1.8729679584503174\n",
      "1.880742073059082\n",
      "1.917336344718933\n",
      "1.882994532585144\n",
      "1.9225234985351562\n",
      "1.8583037853240967\n",
      "1.8732749223709106\n",
      "1.8987613916397095\n",
      "1.9189138412475586\n",
      "1.8523800373077393\n",
      "1.8857135772705078\n",
      "1.9265269041061401\n",
      "1.8940742015838623\n",
      "1.9085326194763184\n",
      "1.885296106338501\n",
      "1.8932855129241943\n",
      "1.9154261350631714\n",
      "1.8925971984863281\n",
      "1.9302973747253418\n",
      "1.862809181213379\n",
      "1.8503895998001099\n",
      "1.8621567487716675\n",
      "1.8513083457946777\n",
      "1.85642409324646\n",
      "1.891526699066162\n",
      "1.8946011066436768\n",
      "1.8696253299713135\n",
      "1.8711621761322021\n",
      "1.8956811428070068\n",
      "1.8541841506958008\n",
      "1.8686535358428955\n",
      "1.9146511554718018\n",
      "1.9196852445602417\n",
      "1.8969541788101196\n",
      "1.8715944290161133\n",
      "1.8697329759597778\n",
      "1.9124820232391357\n",
      "1.9028292894363403\n",
      "1.8721258640289307\n",
      "1.873070240020752\n",
      "1.8863961696624756\n",
      "1.8938744068145752\n",
      "1.8403842449188232\n",
      "1.8701921701431274\n",
      "1.9038878679275513\n",
      "1.8653062582015991\n",
      "1.8745101690292358\n",
      "1.886979579925537\n",
      "1.9055964946746826\n",
      "1.9176030158996582\n",
      "1.8830453157424927\n",
      "1.9197747707366943\n",
      "1.9185609817504883\n",
      "1.9084404706954956\n",
      "1.9272093772888184\n",
      "1.8859889507293701\n",
      "1.8756600618362427\n",
      "1.9000697135925293\n",
      "1.8738352060317993\n",
      "1.9394384622573853\n",
      "1.8677494525909424\n",
      "1.8640708923339844\n",
      "1.8652939796447754\n",
      "1.8479259014129639\n",
      "1.9002944231033325\n",
      "1.9115991592407227\n",
      "1.96041738986969\n",
      "1.8734568357467651\n",
      "1.8377704620361328\n",
      "1.883872628211975\n",
      "1.8876348733901978\n",
      "1.8868801593780518\n",
      "1.9439070224761963\n",
      "1.9123530387878418\n",
      "1.9525232315063477\n",
      "1.8372735977172852\n",
      "1.9015499353408813\n",
      "1.9413368701934814\n",
      "1.9082186222076416\n",
      "1.851865291595459\n",
      "1.835831642150879\n",
      "1.845069408416748\n",
      "1.8615983724594116\n",
      "1.8796106576919556\n",
      "1.8461096286773682\n",
      "1.8753353357315063\n",
      "1.853137493133545\n",
      "1.8884936571121216\n",
      "1.8504482507705688\n",
      "1.8569560050964355\n",
      "1.881270408630371\n",
      "1.8758461475372314\n",
      "1.874105453491211\n",
      "1.8899009227752686\n",
      "1.8453891277313232\n",
      "1.8490948677062988\n",
      "1.8796197175979614\n",
      "1.8926862478256226\n",
      "1.8866493701934814\n",
      "1.8828506469726562\n",
      "1.8499393463134766\n",
      "1.900857925415039\n",
      "1.885565996170044\n",
      "1.905432105064392\n",
      "1.884228229522705\n",
      "1.904806137084961\n",
      "1.8608942031860352\n",
      "1.8532538414001465\n",
      "1.8640806674957275\n",
      "1.8615350723266602\n",
      "1.8882980346679688\n",
      "1.8857008218765259\n",
      "1.8541038036346436\n",
      "1.8861327171325684\n",
      "1.857943058013916\n",
      "1.890099048614502\n",
      "1.8804144859313965\n",
      "1.8242082595825195\n",
      "1.8621344566345215\n",
      "1.8778283596038818\n",
      "1.8971915245056152\n",
      "1.8360357284545898\n",
      "1.8461048603057861\n",
      "1.8423027992248535\n",
      "1.8744795322418213\n",
      "1.8637843132019043\n",
      "1.9304916858673096\n",
      "1.8646042346954346\n",
      "1.96578049659729\n",
      "1.8916419744491577\n",
      "1.8815040588378906\n",
      "1.935675859451294\n",
      "1.8453880548477173\n",
      "1.8655149936676025\n",
      "1.8324835300445557\n",
      "1.8421306610107422\n",
      "1.8685455322265625\n",
      "1.9135284423828125\n",
      "1.920750379562378\n",
      "1.8777251243591309\n",
      "1.8881549835205078\n",
      "1.865999460220337\n",
      "1.8989545106887817\n",
      "1.9073750972747803\n",
      "1.9115040302276611\n",
      "1.8838716745376587\n",
      "1.9018917083740234\n",
      "1.8643889427185059\n",
      "1.8683668375015259\n",
      "1.8718833923339844\n",
      "1.8439749479293823\n",
      "1.8792415857315063\n",
      "1.8974610567092896\n",
      "1.9057083129882812\n",
      "1.836591362953186\n",
      "1.8273043632507324\n",
      "1.8457094430923462\n",
      "1.8960866928100586\n",
      "1.8409106731414795\n",
      "1.884181022644043\n",
      "1.8479499816894531\n",
      "1.8660540580749512\n",
      "1.8922998905181885\n",
      "1.8425277471542358\n",
      "1.8833162784576416\n",
      "1.8162870407104492\n",
      "1.8401190042495728\n",
      "1.8568565845489502\n",
      "1.9139671325683594\n",
      "1.9089041948318481\n",
      "1.92778742313385\n",
      "1.8762800693511963\n",
      "1.8773943185806274\n",
      "1.874037742614746\n",
      "1.873847246170044\n",
      "1.8864598274230957\n",
      "1.8886078596115112\n",
      "1.9106101989746094\n",
      "1.9371123313903809\n",
      "1.8945610523223877\n",
      "1.8296855688095093\n",
      "1.8859480619430542\n",
      "1.8627179861068726\n",
      "1.8738914728164673\n",
      "1.8762967586517334\n",
      "1.828361988067627\n",
      "1.871653437614441\n",
      "1.965458869934082\n",
      "1.9356735944747925\n",
      "1.8152281045913696\n",
      "1.9093899726867676\n",
      "1.8788057565689087\n",
      "1.9312007427215576\n",
      "1.8677833080291748\n",
      "1.8636054992675781\n",
      "1.8851048946380615\n",
      "1.8132611513137817\n",
      "1.931391716003418\n",
      "1.8862707614898682\n",
      "1.862867832183838\n",
      "1.9003188610076904\n",
      "1.8433912992477417\n",
      "1.8576602935791016\n",
      "1.9081308841705322\n",
      "1.8683931827545166\n",
      "1.8891472816467285\n",
      "1.8757026195526123\n",
      "1.8362417221069336\n",
      "1.9024438858032227\n",
      "1.8811208009719849\n",
      "1.90506911277771\n",
      "1.9144461154937744\n",
      "1.8398500680923462\n",
      "1.8728405237197876\n",
      "1.8883652687072754\n",
      "1.898524522781372\n",
      "1.8858128786087036\n",
      "1.9109406471252441\n",
      "1.9229726791381836\n",
      "1.869328260421753\n",
      "1.88224458694458\n",
      "1.807384729385376\n",
      "1.858030080795288\n",
      "1.8027610778808594\n",
      "1.9112128019332886\n",
      "1.861295461654663\n",
      "1.8610599040985107\n",
      "1.8315457105636597\n",
      "1.8465251922607422\n",
      "1.917543649673462\n",
      "1.8539084196090698\n",
      "1.8277121782302856\n",
      "1.872438669204712\n",
      "1.8520355224609375\n",
      "1.8373188972473145\n",
      "1.888839840888977\n",
      "1.8861660957336426\n",
      "1.8103039264678955\n",
      "1.9243884086608887\n",
      "1.9055609703063965\n",
      "1.928450584411621\n",
      "1.8479728698730469\n",
      "1.8649334907531738\n",
      "1.8879852294921875\n",
      "1.857311725616455\n",
      "1.8912296295166016\n",
      "1.8484747409820557\n",
      "1.9069349765777588\n",
      "1.831355094909668\n",
      "1.915098786354065\n",
      "1.9399199485778809\n",
      "1.8492755889892578\n",
      "1.8067877292633057\n",
      "1.9347965717315674\n",
      "1.857471227645874\n",
      "1.8502132892608643\n",
      "1.8427343368530273\n",
      "1.8832104206085205\n",
      "1.8228082656860352\n",
      "1.8879369497299194\n",
      "1.8419582843780518\n",
      "1.8344228267669678\n",
      "1.8996105194091797\n",
      "1.86888587474823\n",
      "1.8804165124893188\n",
      "1.8845174312591553\n",
      "1.8565082550048828\n",
      "1.8976798057556152\n",
      "1.867941975593567\n",
      "1.886420488357544\n",
      "1.8768340349197388\n",
      "1.8386006355285645\n",
      "1.908576250076294\n",
      "1.8870251178741455\n",
      "1.874360203742981\n",
      "1.8619630336761475\n",
      "1.8558779954910278\n",
      "1.8927373886108398\n",
      "1.9056532382965088\n",
      "1.8641401529312134\n",
      "1.847233772277832\n",
      "1.878953218460083\n",
      "1.8056142330169678\n",
      "1.869429588317871\n",
      "1.8325965404510498\n",
      "1.8596059083938599\n",
      "1.858198881149292\n",
      "1.8806225061416626\n",
      "1.874934196472168\n",
      "1.8434044122695923\n",
      "1.8746819496154785\n",
      "1.8888143301010132\n",
      "1.891770601272583\n",
      "1.8743085861206055\n",
      "1.871596336364746\n",
      "1.8526012897491455\n",
      "1.8920718431472778\n",
      "1.869473934173584\n",
      "1.8119196891784668\n",
      "1.8833882808685303\n",
      "1.8378612995147705\n",
      "1.7764368057250977\n",
      "1.8530595302581787\n",
      "1.8355767726898193\n",
      "1.794531226158142\n",
      "1.808445930480957\n",
      "1.8646258115768433\n",
      "1.8726370334625244\n",
      "1.824098825454712\n",
      "1.8269580602645874\n",
      "1.8854961395263672\n",
      "1.898506760597229\n",
      "1.8788173198699951\n",
      "1.8241767883300781\n",
      "1.928812026977539\n",
      "1.8421590328216553\n",
      "1.8423511981964111\n",
      "1.8567402362823486\n",
      "1.8778440952301025\n",
      "1.89517343044281\n",
      "1.8882766962051392\n",
      "1.836137056350708\n",
      "1.8344190120697021\n",
      "1.7762055397033691\n",
      "1.8737460374832153\n",
      "1.8446636199951172\n",
      "1.8682284355163574\n",
      "1.8650791645050049\n",
      "1.9089103937149048\n",
      "1.786702036857605\n",
      "1.8946130275726318\n",
      "1.892385721206665\n",
      "1.8729209899902344\n",
      "1.8511631488800049\n",
      "1.8998253345489502\n",
      "1.9187979698181152\n",
      "1.8976476192474365\n",
      "1.8917831182479858\n",
      "1.8943817615509033\n",
      "1.8992977142333984\n",
      "1.9101505279541016\n",
      "1.9100315570831299\n",
      "1.8846237659454346\n",
      "1.8393802642822266\n",
      "1.839944839477539\n",
      "1.881908655166626\n",
      "1.8568527698516846\n",
      "1.8405449390411377\n",
      "1.8530505895614624\n",
      "1.848341941833496\n",
      "1.8745940923690796\n",
      "1.8630642890930176\n",
      "1.8119860887527466\n",
      "1.8671460151672363\n",
      "1.8746460676193237\n",
      "1.876560091972351\n",
      "1.9204998016357422\n",
      "1.8217387199401855\n",
      "1.875182867050171\n",
      "1.8214964866638184\n",
      "1.8958642482757568\n",
      "1.861242413520813\n",
      "1.8713734149932861\n",
      "1.8970615863800049\n",
      "1.8852437734603882\n",
      "1.8821983337402344\n",
      "1.8425350189208984\n",
      "1.897402048110962\n",
      "1.873936653137207\n",
      "1.8594577312469482\n",
      "1.8299260139465332\n",
      "1.8830056190490723\n",
      "1.8573310375213623\n",
      "1.8875430822372437\n",
      "1.8603612184524536\n",
      "1.8590322732925415\n",
      "1.9023303985595703\n",
      "1.8514211177825928\n",
      "1.8890411853790283\n",
      "1.8744051456451416\n",
      "1.8959112167358398\n",
      "1.8782134056091309\n",
      "1.8530538082122803\n",
      "1.8383656740188599\n",
      "1.8744333982467651\n",
      "1.8838000297546387\n",
      "1.9020729064941406\n",
      "1.8901681900024414\n",
      "1.827649474143982\n",
      "1.8161741495132446\n",
      "1.8548290729522705\n",
      "1.9160391092300415\n",
      "1.8664209842681885\n",
      "1.8610894680023193\n",
      "1.9051504135131836\n",
      "1.8603031635284424\n",
      "1.8624942302703857\n",
      "1.8878121376037598\n",
      "1.8684344291687012\n",
      "1.8406000137329102\n",
      "1.8268513679504395\n",
      "1.8869237899780273\n",
      "1.8933058977127075\n",
      "1.9177170991897583\n",
      "1.8230458498001099\n",
      "1.860755205154419\n",
      "1.8956210613250732\n",
      "1.9047386646270752\n",
      "1.8663499355316162\n",
      "1.8958945274353027\n",
      "1.8456544876098633\n",
      "1.8429008722305298\n",
      "1.7991217374801636\n",
      "1.836435317993164\n",
      "1.870011806488037\n",
      "1.8613603115081787\n",
      "1.8367130756378174\n",
      "1.8792698383331299\n",
      "1.8910164833068848\n",
      "1.8230574131011963\n",
      "1.8792489767074585\n",
      "1.8885674476623535\n",
      "1.8641357421875\n",
      "1.806546926498413\n",
      "1.85542893409729\n",
      "1.9102981090545654\n",
      "1.83664071559906\n",
      "1.8545801639556885\n",
      "1.8294765949249268\n",
      "1.8136060237884521\n",
      "1.8603562116622925\n",
      "1.8943063020706177\n",
      "1.8306221961975098\n",
      "1.8745369911193848\n",
      "1.8504071235656738\n",
      "1.8774147033691406\n",
      "1.8831639289855957\n",
      "1.8731296062469482\n",
      "1.8186781406402588\n",
      "1.857017159461975\n",
      "1.874840259552002\n",
      "1.8240759372711182\n",
      "1.866607427597046\n",
      "1.8466933965682983\n",
      "1.8570561408996582\n",
      "1.848961353302002\n",
      "1.8853257894515991\n",
      "1.8335752487182617\n",
      "1.914050579071045\n",
      "1.8638997077941895\n",
      "1.8587102890014648\n",
      "1.8641388416290283\n",
      "1.871415138244629\n",
      "1.8793063163757324\n",
      "1.8668556213378906\n",
      "1.839707374572754\n",
      "1.8363564014434814\n",
      "1.8826395273208618\n",
      "1.850182056427002\n",
      "1.8768198490142822\n",
      "1.8530267477035522\n",
      "1.9150856733322144\n",
      "1.8227732181549072\n",
      "1.8438518047332764\n",
      "1.8735677003860474\n",
      "1.828392505645752\n",
      "1.849914789199829\n",
      "1.8485990762710571\n",
      "1.8469321727752686\n",
      "1.9156208038330078\n",
      "1.870582103729248\n",
      "1.8752472400665283\n",
      "1.8680182695388794\n",
      "1.8800525665283203\n",
      "1.8239483833312988\n",
      "1.8621277809143066\n",
      "1.8651533126831055\n",
      "1.8794549703598022\n",
      "1.7697267532348633\n",
      "1.8216156959533691\n",
      "1.8296399116516113\n",
      "1.8439300060272217\n",
      "1.8201781511306763\n",
      "1.8924376964569092\n",
      "1.8533284664154053\n",
      "1.8635464906692505\n",
      "1.836249589920044\n",
      "1.8546254634857178\n",
      "1.892045259475708\n",
      "1.8885633945465088\n",
      "1.865628719329834\n",
      "1.8489469289779663\n",
      "1.8509650230407715\n",
      "1.8525054454803467\n",
      "1.9272923469543457\n",
      "1.8790576457977295\n",
      "1.8470393419265747\n",
      "1.8567333221435547\n",
      "1.8189655542373657\n",
      "1.906510591506958\n",
      "1.821480393409729\n",
      "1.8460679054260254\n",
      "1.87501859664917\n",
      "1.8405330181121826\n",
      "1.8616511821746826\n",
      "1.8373509645462036\n",
      "1.8918285369873047\n",
      "1.8722891807556152\n",
      "1.8538436889648438\n",
      "1.8461880683898926\n",
      "1.822092056274414\n",
      "1.8382728099822998\n",
      "1.8562637567520142\n",
      "1.8603460788726807\n",
      "1.819395661354065\n",
      "1.8263911008834839\n",
      "1.860361933708191\n",
      "1.8882087469100952\n",
      "1.8933343887329102\n",
      "1.8677443265914917\n",
      "1.8358187675476074\n",
      "1.8617596626281738\n",
      "1.8824478387832642\n",
      "1.8381662368774414\n",
      "1.8215997219085693\n",
      "1.825014591217041\n",
      "1.8626885414123535\n",
      "1.8177149295806885\n",
      "1.8685269355773926\n",
      "1.886033535003662\n",
      "1.8108835220336914\n",
      "1.8797621726989746\n",
      "1.86444091796875\n",
      "1.8424052000045776\n",
      "1.8973278999328613\n",
      "1.870875358581543\n",
      "1.8622896671295166\n",
      "1.8542486429214478\n",
      "1.8817212581634521\n",
      "1.818210244178772\n",
      "1.855651617050171\n",
      "1.8825604915618896\n",
      "1.8723570108413696\n",
      "1.8398637771606445\n",
      "1.8870012760162354\n",
      "1.8466441631317139\n",
      "1.933027744293213\n",
      "1.891014814376831\n",
      "1.8965710401535034\n",
      "1.8550934791564941\n",
      "1.8516480922698975\n",
      "1.8707947731018066\n",
      "1.8249810934066772\n",
      "1.8576252460479736\n",
      "1.916935920715332\n",
      "1.8325726985931396\n",
      "1.845522403717041\n",
      "1.8402347564697266\n",
      "1.8396222591400146\n",
      "1.8588052988052368\n",
      "1.8572282791137695\n",
      "1.8818093538284302\n",
      "1.8816885948181152\n",
      "1.8423998355865479\n",
      "1.8561434745788574\n",
      "1.8807570934295654\n",
      "1.8572163581848145\n",
      "1.8652586936950684\n",
      "1.8808344602584839\n",
      "1.8572971820831299\n",
      "1.8812638521194458\n",
      "1.814975380897522\n",
      "1.8074651956558228\n",
      "1.864757776260376\n",
      "1.8935481309890747\n",
      "1.8205902576446533\n",
      "1.8534801006317139\n",
      "1.8282010555267334\n",
      "1.8435547351837158\n",
      "1.8568390607833862\n",
      "1.899194598197937\n",
      "1.8359125852584839\n",
      "1.849676489830017\n",
      "1.8300331830978394\n",
      "1.8909733295440674\n",
      "1.890416145324707\n",
      "1.8312309980392456\n",
      "1.830296277999878\n",
      "1.873881459236145\n",
      "1.8525755405426025\n",
      "1.8589401245117188\n",
      "1.8295587301254272\n",
      "1.8393570184707642\n",
      "1.881934404373169\n",
      "1.8593177795410156\n",
      "1.8440237045288086\n",
      "1.8515170812606812\n",
      "1.8091528415679932\n",
      "1.7813040018081665\n",
      "1.868302583694458\n",
      "1.8486469984054565\n",
      "1.878061294555664\n",
      "1.851525068283081\n",
      "1.7974263429641724\n",
      "1.8486906290054321\n",
      "1.8317980766296387\n",
      "1.8624958992004395\n",
      "1.863765835762024\n",
      "1.8703463077545166\n",
      "1.8660637140274048\n",
      "1.8741955757141113\n",
      "1.8624303340911865\n",
      "1.8523228168487549\n",
      "1.843544363975525\n",
      "1.8693634271621704\n",
      "1.8841891288757324\n",
      "1.87477445602417\n",
      "1.8784737586975098\n",
      "1.848833680152893\n",
      "1.864344596862793\n",
      "1.8277710676193237\n",
      "1.776841163635254\n",
      "1.9122412204742432\n",
      "1.8212664127349854\n",
      "1.8605237007141113\n",
      "1.819724202156067\n",
      "1.8847792148590088\n",
      "1.8599871397018433\n",
      "1.828293800354004\n",
      "1.8551077842712402\n",
      "1.8444422483444214\n",
      "1.852565050125122\n",
      "1.841428279876709\n",
      "1.8443119525909424\n",
      "1.8160353899002075\n",
      "1.8769466876983643\n",
      "1.8830825090408325\n",
      "1.833918571472168\n",
      "1.874036192893982\n",
      "1.8570046424865723\n",
      "1.8730733394622803\n",
      "1.87772798538208\n",
      "1.8449621200561523\n",
      "1.831155776977539\n",
      "1.8478666543960571\n",
      "1.8239972591400146\n",
      "1.8883330821990967\n",
      "1.8824446201324463\n",
      "1.889605164527893\n",
      "1.874565601348877\n",
      "1.8471920490264893\n",
      "1.8519511222839355\n",
      "1.8261648416519165\n",
      "1.8381918668746948\n",
      "1.7981586456298828\n",
      "1.8562819957733154\n",
      "1.8652644157409668\n",
      "1.836575984954834\n",
      "1.8652563095092773\n",
      "1.838897943496704\n",
      "1.8415859937667847\n",
      "1.8322577476501465\n",
      "1.865128993988037\n",
      "1.8126177787780762\n",
      "1.8828458786010742\n",
      "1.8435287475585938\n",
      "1.8347041606903076\n",
      "1.8074040412902832\n",
      "1.8814321756362915\n",
      "1.873713731765747\n",
      "1.8329124450683594\n",
      "1.8999882936477661\n",
      "1.8590259552001953\n",
      "1.8059039115905762\n",
      "1.8397611379623413\n",
      "1.868424892425537\n",
      "1.8352584838867188\n",
      "1.8536484241485596\n",
      "1.8431878089904785\n",
      "1.868088722229004\n",
      "1.911142349243164\n",
      "1.8650299310684204\n",
      "1.8267744779586792\n",
      "1.7912509441375732\n",
      "1.8895646333694458\n",
      "1.847697138786316\n",
      "1.8848718404769897\n",
      "1.8428727388381958\n",
      "1.873305082321167\n",
      "1.8534643650054932\n",
      "1.8275291919708252\n",
      "1.8192784786224365\n",
      "1.8164801597595215\n",
      "1.8166385889053345\n",
      "1.812784194946289\n",
      "1.8529343605041504\n",
      "1.8251055479049683\n",
      "1.8266191482543945\n",
      "1.8227641582489014\n",
      "1.8388054370880127\n",
      "1.8308979272842407\n",
      "1.8107799291610718\n",
      "1.815327763557434\n",
      "1.8248519897460938\n",
      "1.8371787071228027\n",
      "1.8235821723937988\n",
      "1.8289666175842285\n",
      "1.870467185974121\n",
      "1.819161057472229\n",
      "1.8587226867675781\n",
      "1.889205813407898\n",
      "1.8906617164611816\n",
      "1.839442253112793\n",
      "1.8973352909088135\n",
      "1.8161072731018066\n",
      "1.847434639930725\n",
      "1.8055973052978516\n",
      "1.8067474365234375\n",
      "1.8483734130859375\n",
      "1.7993581295013428\n",
      "1.8137187957763672\n",
      "1.8419697284698486\n",
      "1.936835527420044\n",
      "1.8495784997940063\n",
      "1.8077901601791382\n",
      "1.787116289138794\n",
      "1.837083339691162\n",
      "1.833846092224121\n",
      "1.845555305480957\n",
      "1.8689377307891846\n",
      "1.8878364562988281\n",
      "1.7997093200683594\n",
      "1.821554183959961\n",
      "1.8386104106903076\n",
      "1.8570976257324219\n",
      "1.8354501724243164\n",
      "1.8298966884613037\n",
      "1.8440117835998535\n",
      "1.834322214126587\n",
      "1.8459416627883911\n",
      "1.8602769374847412\n",
      "1.8674662113189697\n",
      "1.8199021816253662\n",
      "1.7982978820800781\n",
      "1.8678828477859497\n",
      "1.8010667562484741\n",
      "1.8397657871246338\n",
      "1.8544577360153198\n",
      "1.8595142364501953\n",
      "1.7780427932739258\n",
      "1.8707056045532227\n",
      "1.8485320806503296\n",
      "1.829232931137085\n",
      "1.8735671043395996\n",
      "1.8998217582702637\n",
      "1.8782615661621094\n",
      "1.857340693473816\n",
      "1.7996553182601929\n",
      "1.8633257150650024\n",
      "1.7649834156036377\n",
      "1.8741202354431152\n",
      "1.8596818447113037\n",
      "1.8883305788040161\n",
      "1.8332456350326538\n",
      "1.862966537475586\n",
      "1.810429573059082\n",
      "1.8829214572906494\n",
      "1.8727831840515137\n",
      "1.8772406578063965\n",
      "1.8174835443496704\n",
      "1.848564624786377\n",
      "1.9199261665344238\n",
      "1.8612321615219116\n",
      "1.8303160667419434\n",
      "1.8751072883605957\n",
      "1.8743788003921509\n",
      "1.840507984161377\n",
      "1.8334580659866333\n",
      "1.863917589187622\n",
      "1.8543049097061157\n",
      "1.8440911769866943\n",
      "1.8883745670318604\n",
      "1.8799694776535034\n",
      "1.8269516229629517\n",
      "1.84794020652771\n",
      "1.774145245552063\n",
      "1.8244138956069946\n",
      "1.8675591945648193\n",
      "1.8718382120132446\n",
      "1.7888593673706055\n",
      "1.8224127292633057\n",
      "1.8756208419799805\n",
      "1.8389170169830322\n",
      "1.8662368059158325\n",
      "1.8049827814102173\n",
      "1.7959320545196533\n",
      "1.8154842853546143\n",
      "1.8094416856765747\n",
      "1.8645281791687012\n",
      "1.859763741493225\n",
      "1.8560688495635986\n",
      "1.858368992805481\n",
      "1.8404326438903809\n",
      "1.8462016582489014\n",
      "1.8178675174713135\n",
      "1.8933216333389282\n",
      "1.8627946376800537\n",
      "1.8257861137390137\n",
      "1.8424899578094482\n",
      "1.8721733093261719\n",
      "1.8438745737075806\n",
      "1.8557500839233398\n",
      "1.918922781944275\n",
      "1.8155440092086792\n",
      "1.8194636106491089\n",
      "1.8453800678253174\n",
      "1.8127491474151611\n",
      "1.7870979309082031\n",
      "1.8969755172729492\n",
      "1.8890738487243652\n",
      "1.8473225831985474\n",
      "1.8421003818511963\n",
      "1.8084475994110107\n",
      "1.8433972597122192\n",
      "1.8178821802139282\n",
      "1.829694390296936\n",
      "1.8415443897247314\n",
      "1.8043742179870605\n",
      "1.8715782165527344\n",
      "1.8680309057235718\n",
      "1.792738676071167\n",
      "1.7912571430206299\n",
      "1.846440315246582\n",
      "1.8854740858078003\n",
      "1.860636830329895\n",
      "1.8018524646759033\n",
      "1.756556510925293\n",
      "1.8687794208526611\n",
      "1.812718391418457\n",
      "1.832068681716919\n",
      "1.8291237354278564\n",
      "1.8850772380828857\n",
      "1.8314759731292725\n",
      "1.8315705060958862\n",
      "1.8578672409057617\n",
      "1.852500557899475\n",
      "1.8544437885284424\n",
      "1.8108609914779663\n",
      "1.890328049659729\n",
      "1.8707395792007446\n",
      "1.861769437789917\n",
      "1.8543990850448608\n",
      "1.8820266723632812\n",
      "1.8213927745819092\n",
      "1.881259799003601\n",
      "1.79393470287323\n",
      "1.861021876335144\n",
      "1.8476574420928955\n",
      "1.8532309532165527\n",
      "1.8548818826675415\n",
      "1.8475979566574097\n",
      "1.8352711200714111\n",
      "1.8783349990844727\n",
      "1.8576769828796387\n",
      "1.9091315269470215\n",
      "1.836310863494873\n",
      "1.8576574325561523\n",
      "1.8597733974456787\n",
      "1.8903119564056396\n",
      "1.8292880058288574\n",
      "1.824704647064209\n",
      "1.8367760181427002\n",
      "1.882589340209961\n",
      "1.886584758758545\n",
      "1.9106528759002686\n",
      "1.8645086288452148\n",
      "1.8885422945022583\n",
      "1.8336067199707031\n",
      "1.8441250324249268\n",
      "1.7923729419708252\n",
      "1.8034417629241943\n",
      "1.8246712684631348\n",
      "1.7970874309539795\n",
      "1.8160147666931152\n",
      "1.8639984130859375\n",
      "1.774327278137207\n",
      "1.780683994293213\n",
      "1.8532707691192627\n",
      "1.8311948776245117\n",
      "1.8783262968063354\n",
      "1.893274188041687\n",
      "1.8694052696228027\n",
      "1.809678077697754\n",
      "1.8422399759292603\n",
      "1.8513041734695435\n",
      "1.85989511013031\n",
      "1.8205136060714722\n",
      "1.8979790210723877\n",
      "1.8338513374328613\n",
      "1.770693063735962\n",
      "1.879805564880371\n",
      "1.8618359565734863\n",
      "1.8408424854278564\n",
      "1.8452192544937134\n",
      "1.8152434825897217\n",
      "1.8298698663711548\n",
      "1.8665279150009155\n",
      "1.8685052394866943\n",
      "1.7780791521072388\n",
      "1.8637382984161377\n",
      "1.8378386497497559\n",
      "1.8614749908447266\n",
      "1.8295788764953613\n",
      "1.8261744976043701\n",
      "1.7990126609802246\n",
      "1.8511524200439453\n",
      "1.8497028350830078\n",
      "1.849714994430542\n",
      "1.8807034492492676\n",
      "1.888502836227417\n",
      "1.8365437984466553\n",
      "1.8730038404464722\n",
      "1.8829600811004639\n",
      "1.8134372234344482\n",
      "1.8759312629699707\n",
      "1.8555772304534912\n",
      "1.877308964729309\n",
      "1.8089945316314697\n",
      "1.8480360507965088\n",
      "1.7662405967712402\n",
      "1.8778072595596313\n",
      "1.8880507946014404\n",
      "1.845296859741211\n",
      "1.8534233570098877\n",
      "1.8490080833435059\n",
      "1.8541306257247925\n",
      "1.815704584121704\n",
      "1.8564045429229736\n",
      "1.8258113861083984\n",
      "1.8615989685058594\n",
      "1.8586230278015137\n",
      "1.8762116432189941\n",
      "1.8268451690673828\n",
      "1.8470262289047241\n",
      "1.814687967300415\n",
      "1.841294288635254\n",
      "1.8626571893692017\n",
      "1.8124754428863525\n",
      "1.758476972579956\n",
      "1.8218581676483154\n",
      "1.7835237979888916\n",
      "1.8240094184875488\n",
      "1.8305447101593018\n",
      "1.77754807472229\n",
      "1.8175690174102783\n",
      "1.8858236074447632\n",
      "1.8804209232330322\n",
      "1.8443598747253418\n",
      "1.821945309638977\n",
      "1.8677268028259277\n",
      "1.7764923572540283\n",
      "1.82425856590271\n",
      "1.8302512168884277\n",
      "1.784198522567749\n",
      "1.8466973304748535\n",
      "1.807570219039917\n",
      "1.8515185117721558\n",
      "1.823453664779663\n",
      "1.8301348686218262\n",
      "1.8562678098678589\n",
      "1.8080384731292725\n",
      "1.7937545776367188\n",
      "1.8179630041122437\n",
      "1.8701205253601074\n",
      "1.846497654914856\n",
      "1.8666927814483643\n",
      "1.8404500484466553\n",
      "1.9071170091629028\n",
      "1.8217296600341797\n",
      "1.8751542568206787\n",
      "1.829092264175415\n",
      "1.806436538696289\n",
      "1.8499290943145752\n",
      "1.8158516883850098\n",
      "1.871253252029419\n",
      "1.8421785831451416\n",
      "1.864581823348999\n",
      "1.858890175819397\n",
      "1.8093938827514648\n",
      "1.8665647506713867\n",
      "1.830811858177185\n",
      "1.8837952613830566\n",
      "1.8436647653579712\n",
      "1.8595327138900757\n",
      "1.8454293012619019\n",
      "1.8866219520568848\n",
      "1.8559627532958984\n",
      "1.8863005638122559\n",
      "1.810534119606018\n",
      "1.7957565784454346\n",
      "1.8576961755752563\n",
      "1.8541638851165771\n",
      "1.8178764581680298\n",
      "1.8067853450775146\n",
      "1.8503942489624023\n",
      "1.8434134721755981\n",
      "1.7848734855651855\n",
      "1.7933393716812134\n",
      "1.8774529695510864\n",
      "1.8579416275024414\n",
      "1.8871026039123535\n",
      "1.8884257078170776\n",
      "1.8706579208374023\n",
      "1.7921252250671387\n",
      "1.8226163387298584\n",
      "1.8318067789077759\n",
      "1.8344223499298096\n",
      "1.827017068862915\n",
      "1.8576319217681885\n",
      "1.8580431938171387\n",
      "1.8576632738113403\n",
      "1.8450572490692139\n",
      "1.849010705947876\n",
      "1.8627398014068604\n",
      "1.8735089302062988\n",
      "1.846980333328247\n",
      "1.8475980758666992\n",
      "1.816382884979248\n",
      "1.8155369758605957\n",
      "1.8350731134414673\n",
      "1.8906230926513672\n",
      "1.8349428176879883\n",
      "1.7973765134811401\n",
      "1.8023864030838013\n",
      "1.8604528903961182\n",
      "1.8684959411621094\n",
      "1.850243330001831\n",
      "1.8428699970245361\n",
      "1.8236732482910156\n",
      "1.8112735748291016\n",
      "1.8457057476043701\n",
      "1.8404003381729126\n",
      "1.7972135543823242\n",
      "1.8891444206237793\n",
      "1.87608003616333\n",
      "1.7851992845535278\n",
      "1.8376384973526\n",
      "1.8550676107406616\n",
      "1.8591792583465576\n",
      "1.8550903797149658\n",
      "1.8926324844360352\n",
      "1.854447603225708\n",
      "1.8294346332550049\n",
      "1.8746588230133057\n",
      "1.865066409111023\n",
      "1.848904013633728\n",
      "1.8728264570236206\n",
      "1.8357049226760864\n",
      "1.8628473281860352\n",
      "1.8129611015319824\n",
      "1.8370740413665771\n",
      "1.830909013748169\n",
      "1.857865571975708\n",
      "1.8413264751434326\n",
      "1.8090434074401855\n",
      "1.880286693572998\n",
      "1.8773616552352905\n",
      "1.8220261335372925\n",
      "1.858577013015747\n",
      "1.8149467706680298\n",
      "1.8071675300598145\n",
      "1.786341667175293\n",
      "1.7939865589141846\n",
      "1.8832805156707764\n",
      "1.875950574874878\n",
      "1.8194173574447632\n",
      "1.8292264938354492\n",
      "1.833949089050293\n",
      "1.8132075071334839\n",
      "1.8716896772384644\n",
      "1.8670423030853271\n",
      "1.866591215133667\n",
      "1.772810459136963\n",
      "1.8342998027801514\n",
      "1.8022292852401733\n",
      "1.856050729751587\n",
      "1.8285675048828125\n",
      "1.82782781124115\n",
      "1.8372483253479004\n",
      "1.8106610774993896\n",
      "1.8279168605804443\n",
      "1.8460959196090698\n",
      "1.7987751960754395\n",
      "1.8449604511260986\n",
      "1.7996019124984741\n",
      "1.8071660995483398\n",
      "1.8254919052124023\n",
      "1.876692533493042\n",
      "1.8175017833709717\n",
      "1.8568751811981201\n",
      "1.8161239624023438\n",
      "1.778543472290039\n",
      "1.8475075960159302\n",
      "1.81313157081604\n",
      "1.820483684539795\n",
      "1.8864505290985107\n",
      "1.8885743618011475\n",
      "1.7983300685882568\n",
      "1.8217496871948242\n",
      "1.8702566623687744\n",
      "1.7728030681610107\n",
      "1.8104873895645142\n",
      "1.8324388265609741\n",
      "1.8323333263397217\n",
      "1.8133342266082764\n",
      "1.7848879098892212\n",
      "1.80782151222229\n",
      "1.820998191833496\n",
      "1.8238863945007324\n",
      "1.811745524406433\n",
      "1.8059074878692627\n",
      "1.8492026329040527\n",
      "1.847834825515747\n",
      "1.7850658893585205\n",
      "1.8503285646438599\n",
      "1.8356118202209473\n",
      "1.824965476989746\n",
      "1.8021419048309326\n",
      "1.8067835569381714\n",
      "1.794783353805542\n",
      "1.8143961429595947\n",
      "1.8364577293395996\n",
      "1.8259704113006592\n",
      "1.8198535442352295\n",
      "1.8337950706481934\n",
      "1.832006812095642\n",
      "1.8783273696899414\n",
      "1.860865831375122\n",
      "1.8466567993164062\n",
      "1.8217482566833496\n",
      "1.8443771600723267\n",
      "1.8231935501098633\n",
      "1.806333065032959\n",
      "1.8939160108566284\n",
      "1.833463430404663\n",
      "1.8125693798065186\n",
      "1.7834957838058472\n",
      "1.8084778785705566\n",
      "1.836698055267334\n",
      "1.865917682647705\n",
      "1.82062566280365\n",
      "1.835438847541809\n",
      "1.8673455715179443\n",
      "1.8124213218688965\n",
      "1.84065842628479\n",
      "1.8287055492401123\n",
      "1.837479591369629\n",
      "1.8198676109313965\n",
      "1.8879112005233765\n",
      "1.8318923711776733\n",
      "1.837310552597046\n",
      "1.8365947008132935\n",
      "1.8196896314620972\n",
      "1.7904198169708252\n",
      "1.8913246393203735\n",
      "1.827406644821167\n",
      "1.8688105344772339\n",
      "1.7924461364746094\n",
      "1.8326319456100464\n",
      "1.872523546218872\n",
      "1.8358168601989746\n",
      "1.784936785697937\n",
      "1.7936400175094604\n",
      "1.7811057567596436\n",
      "1.87412428855896\n",
      "1.8338476419448853\n",
      "1.8177605867385864\n",
      "1.8366036415100098\n",
      "1.848319172859192\n",
      "1.8454654216766357\n",
      "1.8637510538101196\n",
      "1.8556325435638428\n",
      "1.8602733612060547\n",
      "1.8535864353179932\n",
      "1.846301794052124\n",
      "1.817857265472412\n",
      "1.8574788570404053\n",
      "1.798719882965088\n",
      "1.8562536239624023\n",
      "1.829838514328003\n",
      "1.8516368865966797\n",
      "1.8219515085220337\n",
      "1.8105920553207397\n",
      "1.886880874633789\n",
      "1.793670654296875\n",
      "1.8244740962982178\n",
      "1.8127269744873047\n",
      "1.8706637620925903\n",
      "1.7689855098724365\n",
      "1.7984397411346436\n",
      "1.8453660011291504\n",
      "1.8524552583694458\n",
      "1.887871503829956\n",
      "1.8349692821502686\n",
      "1.840116262435913\n",
      "1.835139274597168\n",
      "1.843537449836731\n",
      "1.7927663326263428\n",
      "1.8746881484985352\n",
      "1.8223036527633667\n",
      "1.8678288459777832\n",
      "1.8866405487060547\n",
      "1.8573331832885742\n",
      "1.8580043315887451\n",
      "1.8225822448730469\n",
      "1.8609375953674316\n",
      "1.8129714727401733\n",
      "1.827043056488037\n",
      "1.857391595840454\n",
      "1.7793092727661133\n",
      "1.8223202228546143\n",
      "1.8305301666259766\n",
      "1.8551472425460815\n",
      "1.884939432144165\n",
      "1.8360979557037354\n",
      "1.8720152378082275\n",
      "1.8264827728271484\n",
      "1.7724010944366455\n",
      "1.8281075954437256\n",
      "1.8322758674621582\n",
      "1.8137872219085693\n",
      "1.8793728351593018\n",
      "1.8066718578338623\n",
      "1.8018606901168823\n",
      "1.8600859642028809\n",
      "1.7985727787017822\n",
      "1.7966644763946533\n",
      "1.8026928901672363\n",
      "1.8149380683898926\n",
      "1.8178409337997437\n",
      "1.8636077642440796\n",
      "1.8100979328155518\n",
      "1.8474630117416382\n",
      "1.8214964866638184\n",
      "1.8246350288391113\n",
      "1.8044564723968506\n",
      "1.8617854118347168\n",
      "1.8202602863311768\n",
      "1.808793067932129\n",
      "1.8082597255706787\n",
      "1.8139055967330933\n",
      "1.8458367586135864\n",
      "1.8341350555419922\n",
      "1.8090193271636963\n",
      "1.8308780193328857\n",
      "1.7726449966430664\n",
      "1.8580951690673828\n",
      "1.7935254573822021\n",
      "1.8319244384765625\n",
      "1.830791711807251\n",
      "1.8562734127044678\n",
      "1.8451989889144897\n",
      "1.8239080905914307\n",
      "1.8849183320999146\n",
      "1.8066966533660889\n",
      "1.7805349826812744\n",
      "1.8006865978240967\n",
      "1.8108694553375244\n",
      "1.8182120323181152\n",
      "1.7709218263626099\n",
      "1.8029956817626953\n",
      "1.8728160858154297\n",
      "1.8762080669403076\n",
      "1.8260890245437622\n",
      "1.787022352218628\n",
      "1.7880802154541016\n",
      "1.837315559387207\n",
      "1.8424574136734009\n",
      "1.8739275932312012\n",
      "1.81441068649292\n",
      "1.8608770370483398\n",
      "1.7821547985076904\n",
      "1.8296676874160767\n",
      "1.839536428451538\n",
      "1.8557041883468628\n",
      "1.862760066986084\n",
      "1.7846587896347046\n",
      "1.8728770017623901\n",
      "1.8185882568359375\n",
      "1.8494936227798462\n",
      "1.8170700073242188\n",
      "1.8528776168823242\n",
      "1.8693718910217285\n",
      "1.8335930109024048\n",
      "1.8826981782913208\n",
      "1.8151447772979736\n",
      "1.8518435955047607\n",
      "1.8036417961120605\n",
      "1.839026689529419\n",
      "1.8676402568817139\n",
      "1.877357840538025\n",
      "1.8171043395996094\n",
      "1.8294556140899658\n",
      "1.7695960998535156\n",
      "1.7548928260803223\n",
      "1.7808663845062256\n",
      "1.8426480293273926\n",
      "1.8465121984481812\n",
      "1.8339844942092896\n",
      "1.8745536804199219\n",
      "1.8130345344543457\n",
      "1.7893415689468384\n",
      "1.7413707971572876\n",
      "1.8783740997314453\n",
      "1.8055644035339355\n",
      "1.845177173614502\n",
      "1.836779236793518\n",
      "1.7992830276489258\n",
      "1.8481769561767578\n",
      "1.8147318363189697\n",
      "1.8831660747528076\n",
      "1.7630645036697388\n",
      "1.8259819746017456\n",
      "1.787832260131836\n",
      "1.844357967376709\n",
      "1.8103963136672974\n",
      "1.8035993576049805\n",
      "1.8705856800079346\n",
      "1.8482158184051514\n",
      "1.8465120792388916\n",
      "1.8178222179412842\n",
      "1.8337676525115967\n",
      "1.820166826248169\n",
      "1.847186803817749\n",
      "1.7746777534484863\n",
      "1.8320813179016113\n",
      "1.8090133666992188\n",
      "1.8436508178710938\n",
      "1.7979761362075806\n",
      "1.7469170093536377\n",
      "1.8632919788360596\n",
      "1.8698360919952393\n",
      "1.8236408233642578\n",
      "1.8188756704330444\n",
      "1.857450246810913\n",
      "1.8028109073638916\n",
      "1.8259789943695068\n",
      "1.787360668182373\n",
      "1.8281276226043701\n",
      "1.81842041015625\n",
      "1.8659377098083496\n",
      "1.7711522579193115\n",
      "1.7995562553405762\n",
      "1.862871527671814\n",
      "1.7922941446304321\n",
      "1.8727891445159912\n",
      "1.850069522857666\n",
      "1.8651384115219116\n",
      "1.849330186843872\n",
      "1.8501479625701904\n",
      "1.835017204284668\n",
      "1.862844467163086\n",
      "1.7855077981948853\n",
      "1.8390130996704102\n",
      "1.804782509803772\n",
      "1.860677719116211\n",
      "1.7765412330627441\n",
      "1.8649390935897827\n",
      "1.8565586805343628\n",
      "1.843186378479004\n",
      "1.833099126815796\n",
      "1.8482084274291992\n",
      "1.72782301902771\n",
      "1.8167554140090942\n",
      "1.8306467533111572\n",
      "1.7845462560653687\n",
      "1.8664350509643555\n",
      "1.8016656637191772\n",
      "1.8657116889953613\n",
      "1.801194190979004\n",
      "1.8423696756362915\n",
      "1.865034818649292\n",
      "1.8337137699127197\n",
      "1.8093260526657104\n",
      "1.7857162952423096\n",
      "1.8321728706359863\n",
      "1.8523328304290771\n",
      "1.8290235996246338\n",
      "1.7833359241485596\n",
      "1.8045903444290161\n",
      "1.8125404119491577\n",
      "1.8299908638000488\n",
      "1.8539137840270996\n",
      "1.8326482772827148\n",
      "1.7890336513519287\n",
      "1.8338912725448608\n",
      "1.7636817693710327\n",
      "1.8274190425872803\n",
      "1.751509189605713\n",
      "1.8198487758636475\n",
      "1.8232229948043823\n",
      "1.841734766960144\n",
      "1.8063260316848755\n",
      "1.8159350156784058\n",
      "1.8108866214752197\n",
      "1.8353497982025146\n",
      "1.7948923110961914\n",
      "1.8358397483825684\n",
      "1.8265923261642456\n",
      "1.8274260759353638\n",
      "1.8504530191421509\n",
      "1.8056951761245728\n",
      "1.8456302881240845\n",
      "1.8233319520950317\n",
      "1.85713791847229\n",
      "1.8182181119918823\n",
      "1.8299837112426758\n",
      "1.8417224884033203\n",
      "1.805370807647705\n",
      "1.8052492141723633\n",
      "1.829027533531189\n",
      "1.7743901014328003\n",
      "1.868366003036499\n",
      "1.8599848747253418\n",
      "1.856441855430603\n",
      "1.754805088043213\n",
      "1.8097493648529053\n",
      "1.8362815380096436\n",
      "1.8520658016204834\n",
      "1.7956435680389404\n",
      "1.7967722415924072\n",
      "1.8421109914779663\n",
      "1.7888617515563965\n",
      "1.879921317100525\n",
      "1.8107222318649292\n",
      "1.8037874698638916\n",
      "1.8243917226791382\n",
      "1.8376352787017822\n",
      "1.787238597869873\n",
      "1.8064374923706055\n",
      "1.8133115768432617\n",
      "1.8771049976348877\n",
      "1.8073680400848389\n",
      "1.810410737991333\n",
      "1.8815231323242188\n",
      "1.8891990184783936\n",
      "1.8318815231323242\n",
      "1.8585859537124634\n",
      "1.8235421180725098\n",
      "1.8226088285446167\n",
      "1.8180246353149414\n",
      "1.8503382205963135\n",
      "1.8198826313018799\n",
      "1.8074766397476196\n",
      "1.825775146484375\n",
      "1.8513435125350952\n",
      "1.7934982776641846\n",
      "1.8080649375915527\n",
      "1.8014914989471436\n",
      "1.833639144897461\n",
      "1.836425542831421\n",
      "1.7839081287384033\n",
      "1.8101791143417358\n",
      "1.8667209148406982\n",
      "1.8169891834259033\n",
      "1.8388673067092896\n",
      "1.8249778747558594\n",
      "1.8237169981002808\n",
      "1.825742483139038\n",
      "1.8166134357452393\n",
      "1.846429705619812\n",
      "1.789273738861084\n",
      "1.796088695526123\n",
      "1.811807632446289\n",
      "1.8310699462890625\n",
      "1.8317875862121582\n",
      "1.8124433755874634\n",
      "1.8369473218917847\n",
      "1.8240966796875\n",
      "1.7931740283966064\n",
      "1.8157596588134766\n",
      "1.7548147439956665\n",
      "1.7844176292419434\n",
      "1.8191869258880615\n",
      "1.8150492906570435\n",
      "1.8527941703796387\n",
      "1.7949554920196533\n",
      "1.8356053829193115\n",
      "1.8586444854736328\n",
      "1.8092159032821655\n",
      "1.776737093925476\n",
      "1.8075833320617676\n",
      "1.856793761253357\n",
      "1.7995320558547974\n",
      "1.8210011720657349\n",
      "1.7959189414978027\n",
      "1.7933588027954102\n",
      "1.8034570217132568\n",
      "1.84381103515625\n",
      "1.7987048625946045\n",
      "1.8600294589996338\n",
      "1.8265690803527832\n",
      "1.8164658546447754\n",
      "1.8460065126419067\n",
      "1.8152145147323608\n",
      "1.8115254640579224\n",
      "1.7796874046325684\n",
      "1.8648958206176758\n",
      "1.7802165746688843\n",
      "1.813187837600708\n",
      "1.7705726623535156\n",
      "1.7990961074829102\n",
      "1.8129088878631592\n",
      "1.7985700368881226\n",
      "1.8474867343902588\n",
      "1.8609766960144043\n",
      "1.8009984493255615\n",
      "1.8185232877731323\n",
      "1.817291021347046\n",
      "1.820920467376709\n",
      "1.8360114097595215\n",
      "1.7925593852996826\n",
      "1.867006778717041\n",
      "1.7992029190063477\n",
      "1.8342154026031494\n",
      "1.7847981452941895\n",
      "1.776881217956543\n",
      "1.8305909633636475\n",
      "1.8395717144012451\n",
      "1.787935495376587\n",
      "1.8327717781066895\n",
      "1.8397983312606812\n",
      "1.7924995422363281\n",
      "1.7978551387786865\n",
      "1.8426107168197632\n",
      "1.7407546043395996\n",
      "1.7968347072601318\n",
      "1.8397026062011719\n",
      "1.8314018249511719\n",
      "1.7917414903640747\n",
      "1.8432602882385254\n",
      "1.805354356765747\n",
      "1.8655072450637817\n",
      "1.844537615776062\n",
      "1.829221487045288\n",
      "1.810705304145813\n",
      "1.8214752674102783\n",
      "1.7992225885391235\n",
      "1.8456172943115234\n",
      "1.7957278490066528\n",
      "1.7853059768676758\n",
      "1.817643165588379\n",
      "1.8096269369125366\n",
      "1.8139832019805908\n",
      "1.8230361938476562\n",
      "1.8828338384628296\n",
      "1.8634037971496582\n",
      "1.8231525421142578\n",
      "1.8230724334716797\n",
      "1.8216363191604614\n",
      "1.7905857563018799\n",
      "1.8298747539520264\n",
      "1.86897873878479\n",
      "1.8373069763183594\n",
      "1.7697559595108032\n",
      "1.7840543985366821\n",
      "1.8199691772460938\n",
      "1.768683910369873\n",
      "1.8919233083724976\n",
      "1.822046160697937\n",
      "1.7909694910049438\n",
      "1.8922092914581299\n",
      "1.8536843061447144\n",
      "1.7532877922058105\n",
      "1.8457698822021484\n",
      "1.8544435501098633\n",
      "1.7855396270751953\n",
      "1.7555729150772095\n",
      "1.9060746431350708\n",
      "1.8217073678970337\n",
      "1.7966833114624023\n",
      "1.8115161657333374\n",
      "1.8265891075134277\n",
      "1.7727625370025635\n",
      "1.7804595232009888\n",
      "1.83616304397583\n",
      "1.8127551078796387\n",
      "1.8195011615753174\n",
      "1.7988405227661133\n",
      "1.8404780626296997\n",
      "1.8126540184020996\n",
      "1.846866488456726\n",
      "1.8333220481872559\n",
      "1.8288843631744385\n",
      "1.8613967895507812\n",
      "1.8352327346801758\n",
      "1.862550973892212\n",
      "1.8176779747009277\n",
      "1.8394970893859863\n",
      "1.846773624420166\n",
      "1.7843133211135864\n",
      "1.8518952131271362\n",
      "1.784240484237671\n",
      "1.843536376953125\n",
      "1.8535006046295166\n",
      "1.8372914791107178\n",
      "1.8303139209747314\n",
      "1.8412508964538574\n",
      "1.8070616722106934\n",
      "1.8184800148010254\n",
      "1.8353604078292847\n",
      "1.8686647415161133\n",
      "1.7985541820526123\n",
      "1.8337820768356323\n",
      "1.8146560192108154\n",
      "1.8243752717971802\n",
      "1.7933919429779053\n",
      "1.8570054769515991\n",
      "1.8503841161727905\n",
      "1.8269319534301758\n",
      "1.8175143003463745\n",
      "1.8630821704864502\n",
      "1.8044319152832031\n",
      "1.8080416917800903\n",
      "1.835418939590454\n",
      "1.8460536003112793\n",
      "1.830564022064209\n",
      "1.8297340869903564\n",
      "1.7854256629943848\n",
      "1.8090873956680298\n",
      "1.7715312242507935\n",
      "1.8488410711288452\n",
      "1.8702365159988403\n",
      "1.8402131795883179\n",
      "1.7824491262435913\n",
      "1.7726235389709473\n",
      "1.8381223678588867\n",
      "1.8035908937454224\n",
      "1.8032796382904053\n",
      "1.7818071842193604\n",
      "1.828681230545044\n",
      "1.8083757162094116\n",
      "1.8059327602386475\n",
      "1.880626916885376\n",
      "1.81419038772583\n",
      "1.788600206375122\n",
      "1.8031948804855347\n",
      "1.8780791759490967\n",
      "1.786209225654602\n",
      "1.814833402633667\n",
      "1.8020578622817993\n",
      "1.804119348526001\n",
      "1.8101985454559326\n",
      "1.858354926109314\n",
      "1.8437235355377197\n",
      "1.8029417991638184\n",
      "1.8073744773864746\n",
      "1.824120283126831\n",
      "1.8154884576797485\n",
      "1.8516967296600342\n",
      "1.8360662460327148\n",
      "1.8059685230255127\n",
      "1.8275785446166992\n",
      "1.811509132385254\n",
      "1.8605154752731323\n",
      "1.852402687072754\n",
      "1.7687569856643677\n",
      "1.8180944919586182\n",
      "1.788321614265442\n",
      "1.7801707983016968\n",
      "1.819541573524475\n",
      "1.8375390768051147\n",
      "1.8280835151672363\n",
      "1.812905192375183\n",
      "1.8179762363433838\n",
      "1.8788166046142578\n",
      "1.8070037364959717\n",
      "1.8435375690460205\n",
      "1.8331599235534668\n",
      "1.8195286989212036\n",
      "1.8230605125427246\n",
      "1.8518271446228027\n",
      "1.79481041431427\n",
      "1.7466732263565063\n",
      "1.7870211601257324\n",
      "1.8437097072601318\n",
      "1.7994873523712158\n",
      "1.8416416645050049\n",
      "1.8072082996368408\n",
      "1.8168433904647827\n",
      "1.7931236028671265\n",
      "1.7922632694244385\n",
      "1.8251426219940186\n",
      "1.8187187910079956\n",
      "1.8106451034545898\n",
      "1.7350616455078125\n",
      "1.8434934616088867\n",
      "1.859119176864624\n",
      "1.7857861518859863\n",
      "1.8013956546783447\n",
      "1.8507307767868042\n",
      "1.8509445190429688\n",
      "1.7851383686065674\n",
      "1.791951298713684\n",
      "1.7703301906585693\n",
      "1.8169769048690796\n",
      "1.831104040145874\n",
      "1.8697590827941895\n",
      "1.7433812618255615\n",
      "1.8682808876037598\n",
      "1.8029322624206543\n",
      "1.807583212852478\n",
      "1.78824782371521\n",
      "1.8216662406921387\n",
      "1.821967601776123\n",
      "1.7988206148147583\n",
      "1.7988932132720947\n",
      "1.8264381885528564\n",
      "1.7868369817733765\n",
      "1.7981436252593994\n",
      "1.814607858657837\n",
      "1.8156651258468628\n",
      "1.8244956731796265\n",
      "1.8023438453674316\n",
      "1.7812533378601074\n",
      "1.8344498872756958\n",
      "1.818015694618225\n",
      "1.8314287662506104\n",
      "1.8414762020111084\n",
      "1.753469467163086\n",
      "1.8136966228485107\n",
      "1.8204309940338135\n",
      "1.8424699306488037\n",
      "1.7261121273040771\n",
      "1.796700119972229\n",
      "1.8363404273986816\n",
      "1.7916150093078613\n",
      "1.763275384902954\n",
      "1.8262070417404175\n",
      "1.8432461023330688\n",
      "1.742719292640686\n",
      "1.8032639026641846\n",
      "1.8231515884399414\n",
      "1.836595058441162\n",
      "1.791843295097351\n",
      "1.7980047464370728\n",
      "1.7867627143859863\n",
      "1.8318020105361938\n",
      "1.8478312492370605\n",
      "1.8097639083862305\n",
      "1.839957594871521\n",
      "1.8124092817306519\n",
      "1.7622638940811157\n",
      "1.825717568397522\n",
      "1.7651959657669067\n",
      "1.7798465490341187\n",
      "1.8563510179519653\n",
      "1.803186058998108\n",
      "1.8210957050323486\n",
      "1.821626901626587\n",
      "1.8241674900054932\n",
      "1.804488182067871\n",
      "1.839094638824463\n",
      "1.7874308824539185\n",
      "1.7730820178985596\n",
      "1.8213363885879517\n",
      "1.7893931865692139\n",
      "1.8681836128234863\n",
      "1.7976564168930054\n",
      "1.817834496498108\n",
      "1.8147603273391724\n",
      "1.7656636238098145\n",
      "1.7882850170135498\n",
      "1.8263306617736816\n",
      "1.7802188396453857\n",
      "1.7611680030822754\n",
      "1.8511499166488647\n",
      "1.8433327674865723\n",
      "1.784561276435852\n",
      "1.8077350854873657\n",
      "1.831169843673706\n",
      "1.7737972736358643\n",
      "1.8036353588104248\n",
      "1.8421964645385742\n",
      "1.826947569847107\n",
      "1.7991693019866943\n",
      "1.8222202062606812\n",
      "1.8282740116119385\n",
      "1.8610303401947021\n",
      "1.8622312545776367\n",
      "1.84163498878479\n",
      "1.8400169610977173\n",
      "1.8331201076507568\n",
      "1.8467392921447754\n",
      "1.8167338371276855\n",
      "1.784351110458374\n",
      "1.8309826850891113\n",
      "1.7903549671173096\n",
      "1.8103251457214355\n",
      "1.7904632091522217\n",
      "1.7729138135910034\n",
      "1.7846035957336426\n",
      "1.8151025772094727\n",
      "1.8308892250061035\n",
      "1.815940022468567\n",
      "1.8105559349060059\n",
      "1.8137438297271729\n",
      "1.8053183555603027\n",
      "1.8042231798171997\n",
      "1.7736961841583252\n",
      "1.7819714546203613\n",
      "1.812159538269043\n",
      "1.7712308168411255\n",
      "1.8059101104736328\n",
      "1.8484147787094116\n",
      "1.7941675186157227\n",
      "1.8070929050445557\n",
      "1.8246629238128662\n",
      "1.7785091400146484\n",
      "1.8638185262680054\n",
      "1.8092129230499268\n",
      "1.8297547101974487\n",
      "1.7575656175613403\n",
      "1.789530873298645\n",
      "1.8358263969421387\n",
      "1.8244097232818604\n",
      "1.81635320186615\n",
      "1.8115813732147217\n",
      "1.7728031873703003\n",
      "1.7781295776367188\n",
      "1.8132665157318115\n",
      "1.7893863916397095\n",
      "1.815233588218689\n",
      "1.7961008548736572\n",
      "1.7987921237945557\n",
      "1.7868516445159912\n",
      "1.8265550136566162\n",
      "1.7703884840011597\n",
      "1.807356595993042\n",
      "1.7564698457717896\n",
      "1.7870644330978394\n",
      "1.8091827630996704\n",
      "1.8029429912567139\n",
      "1.8205697536468506\n",
      "1.782392978668213\n",
      "1.8035968542099\n",
      "1.7936367988586426\n",
      "1.7779549360275269\n",
      "1.8046954870224\n",
      "1.8132193088531494\n",
      "1.8216955661773682\n",
      "1.771458625793457\n",
      "1.8078250885009766\n",
      "1.8301470279693604\n",
      "1.8411893844604492\n",
      "1.8023278713226318\n",
      "1.8190330266952515\n",
      "1.784879207611084\n",
      "1.7978568077087402\n",
      "1.780167818069458\n",
      "1.8446779251098633\n",
      "1.7978324890136719\n",
      "1.807827115058899\n",
      "1.7757115364074707\n",
      "1.863027811050415\n",
      "1.7980620861053467\n",
      "1.8697724342346191\n",
      "1.8484739065170288\n",
      "1.8156784772872925\n",
      "1.8616161346435547\n",
      "1.82955002784729\n",
      "1.775672197341919\n",
      "1.8130149841308594\n",
      "1.7857722043991089\n",
      "1.8300650119781494\n",
      "1.807580828666687\n",
      "1.8166983127593994\n",
      "1.828932762145996\n",
      "1.8475569486618042\n",
      "1.8138693571090698\n",
      "1.7854968309402466\n",
      "1.7975420951843262\n",
      "1.7853357791900635\n",
      "1.7864773273468018\n",
      "1.7673150300979614\n",
      "1.8401241302490234\n",
      "1.828364610671997\n",
      "1.8320739269256592\n",
      "1.8460043668746948\n",
      "1.7943007946014404\n",
      "1.838983416557312\n",
      "1.7541224956512451\n",
      "1.8141062259674072\n",
      "1.8045306205749512\n",
      "1.8587470054626465\n",
      "1.810239315032959\n",
      "1.7959907054901123\n",
      "1.8184736967086792\n",
      "1.7960563898086548\n",
      "1.8069756031036377\n",
      "1.7925422191619873\n",
      "1.8025422096252441\n",
      "1.8557915687561035\n",
      "1.8221648931503296\n",
      "1.798084020614624\n",
      "1.7726197242736816\n",
      "1.7962225675582886\n",
      "1.816309928894043\n",
      "1.8747925758361816\n",
      "1.7815663814544678\n",
      "1.7886629104614258\n",
      "1.755768060684204\n",
      "1.8308130502700806\n",
      "1.7863459587097168\n",
      "1.8430368900299072\n",
      "1.7912665605545044\n",
      "1.8122919797897339\n",
      "1.8211017847061157\n",
      "1.8111432790756226\n",
      "1.804499864578247\n",
      "1.819820761680603\n",
      "1.8059725761413574\n",
      "1.8108277320861816\n",
      "1.872930645942688\n",
      "1.810725212097168\n",
      "1.7355237007141113\n",
      "1.8130412101745605\n",
      "1.7858749628067017\n",
      "1.8348292112350464\n",
      "1.8056812286376953\n",
      "1.8201771974563599\n",
      "1.7869007587432861\n",
      "1.7896062135696411\n",
      "1.7944111824035645\n",
      "1.8468832969665527\n",
      "1.8561638593673706\n",
      "1.808504581451416\n",
      "1.7887799739837646\n",
      "1.8139536380767822\n",
      "1.8128093481063843\n",
      "1.768032193183899\n",
      "1.8517117500305176\n",
      "1.7714163064956665\n",
      "1.821582317352295\n",
      "1.7864055633544922\n",
      "1.8033126592636108\n",
      "1.7735092639923096\n",
      "1.8375155925750732\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_51219/2692915326.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# === Run Training ===\u001b[39;00m\n\u001b[32m     21\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;28;01min\u001b[39;00m range(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     train_loss = train_epoch()\n\u001b[32m     24\u001b[39m     print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")\n",
      "\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_51219/2692915326.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;28;01min\u001b[39;00m train_loader:\n\u001b[32m      7\u001b[39m           xb, yb = xb[\u001b[32m0\u001b[39m], yb[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# unwrap batch dimension\u001b[39;00m\n\u001b[32m      8\u001b[39m           optimizer.zero_grad()\n\u001b[32m      9\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m           logits, loss = model(xb, yb)\n\u001b[32m     11\u001b[39m           loss = loss\n\u001b[32m     12\u001b[39m           loss.backward()\n\u001b[32m     13\u001b[39m           torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m                 \u001b[33m\"causes undesired behavior, please try using `module.compile()`\"\u001b[39m\n\u001b[32m    372\u001b[39m                 \u001b[33m\", or use the per-module hooks instead\"\u001b[39m,\n\u001b[32m    373\u001b[39m                 stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    374\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m super().__call__(*args, **kwargs)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1769\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _wrapped_call_impl(self, *args, **kwargs):\n\u001b[32m   1770\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self._compiled_call_impl \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1771\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._call_impl(*args, **kwargs)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1780\u001b[39m         \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m         if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n\u001b[32m   1782\u001b[39m                 \u001b[38;5;28;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m                 \u001b[38;5;28;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1785\u001b[39m \n\u001b[32m   1786\u001b[39m         result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m         called_always_called_hooks = set()\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    757\u001b[39m                     set_skip_guard_eval_unsafe(prior_skip_guard_eval_unsafe)\n\u001b[32m    758\u001b[39m                     \u001b[38;5;28;01mfor\u001b[39;00m cleanup \u001b[38;5;28;01min\u001b[39;00m cleanups:\n\u001b[32m    759\u001b[39m                         cleanup()\n\u001b[32m    760\u001b[39m             \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m                 _maybe_set_eval_frame(prior)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1769\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _wrapped_call_impl(self, *args, **kwargs):\n\u001b[32m   1770\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self._compiled_call_impl \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1771\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._call_impl(*args, **kwargs)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1780\u001b[39m         \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m         if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n\u001b[32m   1782\u001b[39m                 \u001b[38;5;28;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m                 \u001b[38;5;28;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;28;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1785\u001b[39m \n\u001b[32m   1786\u001b[39m         result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m         called_always_called_hooks = set()\n",
      "\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_51219/1249727272.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, idx, targets, eprint)\u001b[39m\n\u001b[32m    430\u001b[39m         device = idx.device\n\u001b[32m    431\u001b[39m         b, t = idx.size()\n\u001b[32m    432\u001b[39m         x = self.transformer.wte(idx)\n\u001b[32m    433\u001b[39m         x = x.detach()                 \u001b[38;5;66;03m# sever any stale history just in case\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m         x.requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)         \u001b[38;5;66;03m# make x a grad leaf for œÑ at layer 0\u001b[39;00m\n\u001b[32m    435\u001b[39m \n\u001b[32m    436\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;28;01min\u001b[39;00m self.transformer.h:\n\u001b[32m    437\u001b[39m                 x= block(x)\n",
      "\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_51219/1249727272.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(___stack0, self, targets, x)\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m forward(self, idx, targets=\u001b[38;5;28;01mNone\u001b[39;00m, eprint=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    435\u001b[39m         device = idx.device\n\u001b[32m    436\u001b[39m         b, t = idx.size()\n\u001b[32m    437\u001b[39m         x = self.transformer.wte(idx)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    929\u001b[39m                     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    930\u001b[39m                 \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m                     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    932\u001b[39m             \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m                 _maybe_set_eval_frame(prior)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*runtime_args)\u001b[39m\n\u001b[32m   1237\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m forward(*runtime_args: tuple[Any]):\n\u001b[32m   1238\u001b[39m         full_args = []\n\u001b[32m   1239\u001b[39m         full_args.extend(params_flat)\n\u001b[32m   1240\u001b[39m         full_args.extend(runtime_args)\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(full_args)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    366\u001b[39m                 torch.autograd._force_original_view_tracking(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    367\u001b[39m                 torch.enable_grad(),\n\u001b[32m    368\u001b[39m             ):\n\u001b[32m    369\u001b[39m                 record_runtime_wrapper_prologue_exit(cm)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m                 all_outs = call_func_at_runtime_with_args(\n\u001b[32m    371\u001b[39m                     compiled_fn, args_, disable_amp=disable_amp, steal_args=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    372\u001b[39m                 )\n\u001b[32m    373\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    122\u001b[39m \n\u001b[32m    123\u001b[39m     context = torch._C._DisableAutocast \u001b[38;5;28;01mif\u001b[39;00m disable_amp \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m getattr(f, \u001b[33m\"_boxed_call\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m             out = normalize_as_list(f(args))\n\u001b[32m    127\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m             \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m             \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m g(args):\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m f(*args)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \n\u001b[32m    573\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    574\u001b[39m             \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    575\u001b[39m             args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m super().apply(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \n\u001b[32m    578\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    579\u001b[39m             raise RuntimeError(\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(ctx, *deduped_flat_tensor_args)\u001b[39m\n\u001b[32m   2070\u001b[39m                 \u001b[38;5;66;03m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001b[39;00m\n\u001b[32m   2071\u001b[39m                 \u001b[38;5;66;03m#   of the original view, and not the synthetic base\u001b[39;00m\n\u001b[32m   2072\u001b[39m                 \u001b[38;5;66;03m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[39;00m\n\u001b[32m   2073\u001b[39m                 \u001b[38;5;66;03m#   in the fw output order.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2074\u001b[39m                 fw_outs = call_func_at_runtime_with_args(\n\u001b[32m   2075\u001b[39m                     CompiledFunction.compiled_fw,\n\u001b[32m   2076\u001b[39m                     args,\n\u001b[32m   2077\u001b[39m                     disable_amp=disable_amp,\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    122\u001b[39m \n\u001b[32m    123\u001b[39m     context = torch._C._DisableAutocast \u001b[38;5;28;01mif\u001b[39;00m disable_amp \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m getattr(f, \u001b[33m\"_boxed_call\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m             out = normalize_as_list(f(args))\n\u001b[32m    127\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m             \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m             \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(runtime_args)\u001b[39m\n\u001b[32m    552\u001b[39m                     \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[32m    553\u001b[39m                     runtime_metadata.num_forward_returns,\n\u001b[32m    554\u001b[39m                 )\n\u001b[32m    555\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(runtime_args)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    746\u001b[39m                 old_args = args\n\u001b[32m    747\u001b[39m                 args = [*([\u001b[38;5;28;01mNone\u001b[39;00m] * num_tokens), *args]\n\u001b[32m    748\u001b[39m                 old_args.clear()\n\u001b[32m    749\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m             outs = compiled_fn(args)\n\u001b[32m    751\u001b[39m \n\u001b[32m    752\u001b[39m             \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[32m    753\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    584\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.current_callable(inputs)\n\u001b[32m    585\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m             get_runtime_metrics_context().finish()\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m             AutotuneCacheBundler.end_compile()\n",
      "\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/h6/ch62d7snnegitfe6iquqvjvqchwgad3ng7vp2evnmvc7u6twit2m.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m  10253\u001b[39m     buf137 = empty_strided_cpu((\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m2048\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m16384\u001b[39m), torch.float32)\n\u001b[32m  10254\u001b[39m     buf145 = empty_strided_cpu((\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m2048\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m16384\u001b[39m), torch.float32)\n\u001b[32m  10255\u001b[39m     buf155 = reinterpret_tensor(buf156, (\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m66\u001b[39m), (\u001b[32m1081344\u001b[39m, \u001b[32m528\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m462\u001b[39m)  \u001b[38;5;66;03m# alias\u001b[39;00m\n\u001b[32m  10256\u001b[39m     buf158 = reinterpret_tensor(buf159, (\u001b[32m8\u001b[39m, \u001b[32m2048\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m66\u001b[39m), (\u001b[32m1216512\u001b[39m, \u001b[32m594\u001b[39m, \u001b[32m66\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m66\u001b[39m)  \u001b[38;5;66;03m# alias\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m10257\u001b[39m     cpp_fused_abs_add_argmin_cat_clamp_min_clone_copy_div_linalg_vector_norm_mul_scatter_sub_sum_where_zero_zeros_like_0(primals_1, buf27, buf156, buf0, buf1, buf7, buf6, buf9, buf10, buf11, buf2, buf3, buf4, buf12, buf15, buf19, buf157, buf20, buf16, buf21, buf22, buf17, buf23, buf24, buf25, buf26, buf28, buf29, buf35, buf34, buf37, buf38, buf39, buf30, buf31, buf32, buf40, buf148, buf43, buf44, buf50, buf49, buf52, buf53, buf54, buf45, buf46, buf47, buf55, buf149, buf58, buf59, buf65, buf64, buf67, buf68, buf69, buf60, buf61, buf62, buf70, buf150, buf73, buf74, buf80, buf79, buf82, buf83, buf84, buf75, buf76, buf77, buf85, buf151, buf88, buf89, buf95, buf94, buf97, buf98, buf99, buf90, buf91, buf92, buf100, buf152, buf103, buf104, buf110, buf109, buf112, buf113, buf114, buf105, buf106, buf107, buf115, buf153, buf118, buf119, buf125, buf124, buf127, buf128, buf129, buf120, buf121, buf122, buf130, buf154, buf133, buf134, buf140, buf139, buf142, buf143, buf144, buf135, buf136, buf137, buf145, buf155, buf158)\n\u001b[32m  10258\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m buf0\n\u001b[32m  10259\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m buf1\n\u001b[32m  10260\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m buf10\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "          xb, yb = xb[0], yb[0]  # unwrap batch dimension\n",
    "          optimizer.zero_grad()\n",
    "          \n",
    "          logits, loss = model(xb, yb)\n",
    "          loss = loss\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "          optimizer.step()\n",
    "          total_loss += loss.item()\n",
    "          losses.append(loss.item())\n",
    "          print(loss.item())\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23799726366996765\n",
      "0.24302376806735992\n",
      "0.24448652565479279\n",
      "0.22456207871437073\n",
      "0.22056986391544342\n",
      "0.22748275101184845\n",
      "0.22437533736228943\n",
      "0.25013211369514465\n",
      "0.22664299607276917\n",
      "0.24867352843284607\n",
      "0.23899221420288086\n",
      "0.2559569776058197\n",
      "0.24343770742416382\n",
      "0.21184736490249634\n",
      "0.23744869232177734\n",
      "0.2307485193014145\n",
      "0.24979770183563232\n",
      "0.2477695494890213\n",
      "0.2543850541114807\n",
      "0.23928630352020264\n",
      "0.21072596311569214\n",
      "0.2659553289413452\n",
      "0.2720505893230438\n",
      "0.23155277967453003\n",
      "0.219730406999588\n",
      "0.2130618542432785\n",
      "0.22352100908756256\n",
      "0.2201700508594513\n",
      "0.21207588911056519\n",
      "0.22624489665031433\n",
      "0.23223182559013367\n",
      "0.2180122435092926\n",
      "0.2355816662311554\n",
      "0.24245502054691315\n",
      "0.23893487453460693\n",
      "0.2385486662387848\n",
      "0.21568283438682556\n",
      "0.23683160543441772\n",
      "0.20913302898406982\n",
      "0.23690089583396912\n",
      "0.21864819526672363\n",
      "0.23364387452602386\n",
      "0.23052406311035156\n",
      "0.23503457009792328\n",
      "0.2465314269065857\n",
      "0.23467037081718445\n",
      "0.23388871550559998\n",
      "0.24674451351165771\n",
      "0.24682803452014923\n",
      "0.21454191207885742\n",
      "0.24342164397239685\n",
      "0.24651575088500977\n",
      "0.2360619753599167\n",
      "0.2037607729434967\n",
      "0.22851398587226868\n",
      "0.23097074031829834\n",
      "0.23331612348556519\n",
      "0.2294669896364212\n",
      "0.23108702898025513\n",
      "0.27082836627960205\n",
      "0.24397245049476624\n",
      "0.22070586681365967\n",
      "0.21137437224388123\n",
      "0.22308681905269623\n",
      "0.23342543840408325\n",
      "0.243659108877182\n",
      "0.2251530885696411\n",
      "0.2299201637506485\n",
      "0.24165935814380646\n",
      "0.22624634206295013\n",
      "0.22657638788223267\n",
      "0.2486068308353424\n",
      "0.23190248012542725\n",
      "0.20754313468933105\n",
      "0.2224390208721161\n",
      "0.2165033221244812\n",
      "0.214363694190979\n",
      "0.21224477887153625\n",
      "0.22333326935768127\n",
      "0.21698983013629913\n",
      "0.24548901617527008\n",
      "0.22258754074573517\n",
      "0.22770635783672333\n",
      "0.23008649051189423\n",
      "0.23273319005966187\n",
      "0.2175467163324356\n",
      "0.24980716407299042\n",
      "0.21558694541454315\n",
      "0.23612990975379944\n",
      "0.25418657064437866\n",
      "0.24260976910591125\n",
      "0.22650863230228424\n",
      "0.2256382554769516\n",
      "0.23969823122024536\n",
      "0.21748438477516174\n",
      "0.23129263520240784\n",
      "0.2213270664215088\n",
      "0.21712911128997803\n",
      "0.21741817891597748\n",
      "0.2076900154352188\n",
      "0.24067828059196472\n",
      "0.21025174856185913\n",
      "0.23425711691379547\n",
      "0.23130184412002563\n",
      "0.19194601476192474\n",
      "0.24141791462898254\n",
      "0.22408461570739746\n",
      "0.2322435975074768\n",
      "0.2375776171684265\n",
      "0.2626233696937561\n",
      "0.2614002823829651\n",
      "0.22603514790534973\n",
      "0.2531248927116394\n",
      "0.23118531703948975\n",
      "0.21650974452495575\n",
      "0.2351967990398407\n",
      "0.2691309452056885\n",
      "0.2220367193222046\n",
      "0.22130344808101654\n",
      "0.2643480896949768\n",
      "0.2697032690048218\n",
      "0.2319396436214447\n",
      "0.2377701848745346\n",
      "0.22850744426250458\n",
      "0.24002300202846527\n",
      "0.23372474312782288\n",
      "0.2441144734621048\n",
      "0.22393882274627686\n",
      "0.2395758330821991\n",
      "0.23917074501514435\n",
      "0.2254166305065155\n",
      "0.2135305404663086\n",
      "0.21778884530067444\n",
      "0.2186579555273056\n",
      "0.24992871284484863\n",
      "0.21887080371379852\n",
      "0.2560500502586365\n",
      "0.19904637336730957\n",
      "0.24642349779605865\n",
      "0.2195151448249817\n",
      "0.20719999074935913\n",
      "0.21378996968269348\n",
      "0.21673297882080078\n",
      "0.20952528715133667\n",
      "0.21837115287780762\n",
      "0.23443996906280518\n",
      "0.2136509120464325\n",
      "0.24755553901195526\n",
      "0.21735361218452454\n",
      "0.25030195713043213\n",
      "0.22087633609771729\n",
      "0.24266184866428375\n",
      "0.22579215466976166\n",
      "0.21343746781349182\n",
      "0.20910726487636566\n",
      "0.239155113697052\n",
      "0.21221886575222015\n",
      "0.2258240431547165\n",
      "0.23054322600364685\n",
      "0.23070529103279114\n",
      "0.24314571917057037\n",
      "0.22735196352005005\n",
      "0.19691242277622223\n",
      "0.21087074279785156\n",
      "0.23288124799728394\n",
      "0.19473490118980408\n",
      "0.22455637156963348\n",
      "0.2287173569202423\n",
      "0.22584624588489532\n",
      "0.20875516533851624\n",
      "0.2128037065267563\n",
      "0.2306096851825714\n",
      "0.23516853153705597\n",
      "0.21805378794670105\n",
      "0.20972546935081482\n",
      "0.21696823835372925\n",
      "0.22667506337165833\n",
      "0.21959112584590912\n",
      "0.22763028740882874\n",
      "0.21531563997268677\n",
      "0.24681946635246277\n",
      "0.23762735724449158\n",
      "0.23073908686637878\n",
      "0.23451557755470276\n",
      "0.21613387763500214\n",
      "0.22960582375526428\n",
      "0.23412713408470154\n",
      "0.21156899631023407\n",
      "0.2357737272977829\n",
      "0.20219436287879944\n",
      "0.2197134643793106\n",
      "0.22419695556163788\n",
      "0.21772202849388123\n",
      "0.2223702222108841\n",
      "0.21666431427001953\n",
      "0.22165796160697937\n",
      "0.2411011904478073\n",
      "0.21009734272956848\n",
      "0.22927188873291016\n",
      "0.22069776058197021\n",
      "0.2441672682762146\n",
      "0.22060391306877136\n",
      "0.2398718297481537\n",
      "0.24414829909801483\n",
      "0.21189406514167786\n",
      "0.24062055349349976\n",
      "0.20623520016670227\n",
      "0.19752717018127441\n",
      "0.21896065771579742\n",
      "0.211897611618042\n",
      "0.2238466888666153\n",
      "0.22222018241882324\n",
      "0.23048579692840576\n",
      "0.21736788749694824\n",
      "0.20803099870681763\n",
      "0.2217470407485962\n",
      "0.21810582280158997\n",
      "0.20731930434703827\n",
      "0.23629039525985718\n",
      "0.24933432042598724\n",
      "0.22089755535125732\n",
      "0.22762584686279297\n",
      "0.21242472529411316\n",
      "0.22062061727046967\n",
      "0.21809062361717224\n",
      "0.2440350353717804\n",
      "0.21969950199127197\n",
      "0.22110676765441895\n",
      "0.20127207040786743\n",
      "0.22338880598545074\n",
      "0.19920611381530762\n",
      "0.23094938695430756\n",
      "0.2057158350944519\n",
      "0.21236586570739746\n",
      "0.21141882240772247\n",
      "0.22065696120262146\n",
      "0.223413348197937\n",
      "0.2365523874759674\n",
      "0.21517357230186462\n",
      "0.2412186861038208\n",
      "0.20393899083137512\n",
      "0.2152934968471527\n",
      "0.23897376656532288\n",
      "0.22461599111557007\n",
      "0.2233501672744751\n",
      "0.22066450119018555\n",
      "0.2105969786643982\n",
      "0.22854562103748322\n",
      "0.22231550514698029\n",
      "0.24471783638000488\n",
      "0.2322414219379425\n",
      "0.21681727468967438\n",
      "0.22616958618164062\n",
      "0.21289777755737305\n",
      "0.22322814166545868\n",
      "0.20595869421958923\n",
      "0.22390761971473694\n",
      "0.19103163480758667\n",
      "0.22535821795463562\n",
      "0.2075294703245163\n",
      "0.2250816971063614\n",
      "0.21550428867340088\n",
      "0.2129868119955063\n",
      "0.2006007432937622\n",
      "0.20433595776557922\n",
      "0.23979762196540833\n",
      "0.2378150373697281\n",
      "0.2252105474472046\n",
      "0.21896272897720337\n",
      "0.21190941333770752\n",
      "0.2144334316253662\n",
      "0.20368455350399017\n",
      "0.2409418672323227\n",
      "0.23663483560085297\n",
      "0.2534865438938141\n",
      "0.22483310103416443\n",
      "0.21629393100738525\n",
      "0.24381650984287262\n",
      "0.2105071246623993\n",
      "0.2171526849269867\n",
      "0.2140928953886032\n",
      "0.18924112617969513\n",
      "0.21307946741580963\n",
      "0.21157494187355042\n",
      "0.20627066493034363\n",
      "0.19576087594032288\n",
      "0.21818040311336517\n",
      "0.21744602918624878\n",
      "0.21888861060142517\n",
      "0.2099907398223877\n",
      "0.19955086708068848\n",
      "0.1974448561668396\n",
      "0.21103012561798096\n",
      "0.23924925923347473\n",
      "0.22460250556468964\n",
      "0.2146124690771103\n",
      "0.24903179705142975\n",
      "0.22127780318260193\n",
      "0.2162303626537323\n",
      "0.23662741482257843\n",
      "0.23684899508953094\n",
      "0.22675028443336487\n",
      "0.20554649829864502\n",
      "0.24132554233074188\n",
      "0.21477332711219788\n",
      "0.22848659753799438\n",
      "0.20097339153289795\n",
      "0.22613970935344696\n",
      "0.20871339738368988\n",
      "0.20715224742889404\n",
      "0.19899971783161163\n",
      "0.2189895510673523\n",
      "0.20440274477005005\n",
      "0.19704324007034302\n",
      "0.20391124486923218\n",
      "0.21034666895866394\n",
      "0.21982844173908234\n",
      "0.2685307264328003\n",
      "0.20952560007572174\n",
      "0.19592085480690002\n",
      "0.22559136152267456\n",
      "0.22594517469406128\n",
      "0.20266035199165344\n",
      "0.20469501614570618\n",
      "0.24054335057735443\n",
      "0.18962985277175903\n",
      "0.20319373905658722\n",
      "0.19629129767417908\n",
      "0.22254645824432373\n",
      "0.2114829421043396\n",
      "0.1907903552055359\n",
      "0.20785073935985565\n",
      "0.197091743350029\n",
      "0.23078370094299316\n",
      "0.20189273357391357\n",
      "0.21144792437553406\n",
      "0.20499008893966675\n",
      "0.2160676121711731\n",
      "0.22575922310352325\n",
      "0.20729854702949524\n",
      "0.20069420337677002\n",
      "0.20494019985198975\n",
      "0.2077980488538742\n",
      "0.2298230528831482\n",
      "0.22339314222335815\n",
      "0.19652463495731354\n",
      "0.2133731096982956\n",
      "0.22640912234783173\n",
      "0.23685820400714874\n",
      "0.2027474343776703\n",
      "0.20762324333190918\n",
      "0.21586543321609497\n",
      "0.18352894484996796\n",
      "0.231440007686615\n",
      "0.21457886695861816\n",
      "0.18245413899421692\n",
      "0.1920425295829773\n",
      "0.19442658126354218\n",
      "0.17566367983818054\n",
      "0.2280018925666809\n",
      "0.20647475123405457\n",
      "0.22789055109024048\n",
      "0.2118629366159439\n",
      "0.2234591543674469\n",
      "0.20201045274734497\n",
      "0.20267069339752197\n",
      "0.2278449684381485\n",
      "0.21515518426895142\n",
      "0.225738063454628\n",
      "0.211198091506958\n",
      "0.23507215082645416\n",
      "0.20439180731773376\n",
      "0.24400030076503754\n",
      "0.19940181076526642\n",
      "0.22258201241493225\n",
      "0.20595121383666992\n",
      "0.20231355726718903\n",
      "0.233220636844635\n",
      "0.20931589603424072\n",
      "0.2079106569290161\n",
      "0.20574435591697693\n",
      "0.19925647974014282\n",
      "0.20759592950344086\n",
      "0.21622446179389954\n",
      "0.21996550261974335\n",
      "0.2085021436214447\n",
      "0.19163334369659424\n",
      "0.20567777752876282\n",
      "0.22907593846321106\n",
      "0.22232310473918915\n",
      "0.20609050989151\n",
      "0.22564247250556946\n",
      "0.19907860457897186\n",
      "0.20902661979198456\n",
      "0.19712021946907043\n",
      "0.1932106912136078\n",
      "0.20080837607383728\n",
      "0.18917343020439148\n",
      "0.19507281482219696\n",
      "0.2024851143360138\n",
      "0.2228778451681137\n",
      "0.23153352737426758\n",
      "0.21237900853157043\n",
      "0.21624141931533813\n",
      "0.20212966203689575\n",
      "0.21019770205020905\n",
      "0.2160407304763794\n",
      "0.20935259759426117\n",
      "0.227439284324646\n",
      "0.2198980152606964\n",
      "0.20560236275196075\n",
      "0.21582046151161194\n",
      "0.19960130751132965\n",
      "0.2144249677658081\n",
      "0.21478763222694397\n",
      "0.22636432945728302\n",
      "0.1974548101425171\n",
      "0.22089125216007233\n",
      "0.18501415848731995\n",
      "0.20449954271316528\n",
      "0.19957080483436584\n",
      "0.21486300230026245\n",
      "0.19383347034454346\n",
      "0.21317094564437866\n",
      "0.20302130281925201\n",
      "0.20987382531166077\n",
      "0.1977754533290863\n",
      "0.2189377099275589\n",
      "0.20281779766082764\n",
      "0.2194468080997467\n",
      "0.21514029800891876\n",
      "0.2120625376701355\n",
      "0.19129958748817444\n",
      "0.20512577891349792\n",
      "0.19072860479354858\n",
      "0.2306303083896637\n",
      "0.19038143754005432\n",
      "0.22868838906288147\n",
      "0.19186726212501526\n",
      "0.19234441220760345\n",
      "0.19225971400737762\n",
      "0.19777394831180573\n",
      "0.1963171660900116\n",
      "0.20538955926895142\n",
      "0.21656231582164764\n",
      "0.21977554261684418\n",
      "0.2071627378463745\n",
      "0.2033870667219162\n",
      "0.2178824245929718\n",
      "0.20034819841384888\n",
      "0.1954514980316162\n",
      "0.2240242063999176\n",
      "0.22520728409290314\n",
      "0.2006160020828247\n",
      "0.2092607021331787\n",
      "0.19237187504768372\n",
      "0.21060506999492645\n",
      "0.20502784848213196\n",
      "0.22357851266860962\n",
      "0.19764558970928192\n",
      "0.22706393897533417\n",
      "0.22850482165813446\n",
      "0.16937977075576782\n",
      "0.19970588386058807\n",
      "0.20022788643836975\n",
      "0.21453478932380676\n",
      "0.19367913901805878\n",
      "0.2009080946445465\n",
      "0.2126268446445465\n",
      "0.17361894249916077\n",
      "0.2099246233701706\n",
      "0.21558353304862976\n",
      "0.20560570061206818\n",
      "0.20451761782169342\n",
      "0.22258123755455017\n",
      "0.23347820341587067\n",
      "0.20412784814834595\n",
      "0.1936933696269989\n",
      "0.21032142639160156\n",
      "0.21035294234752655\n",
      "0.21429753303527832\n",
      "0.16515858471393585\n",
      "0.17217639088630676\n",
      "0.22156432271003723\n",
      "0.22104521095752716\n",
      "0.1985134333372116\n",
      "0.22276805341243744\n",
      "0.22232243418693542\n",
      "0.2057967633008957\n",
      "0.1878698170185089\n",
      "0.20362749695777893\n",
      "0.20858542621135712\n",
      "0.19570402801036835\n",
      "0.19217020273208618\n",
      "0.20646053552627563\n",
      "0.1982094943523407\n",
      "0.1894962191581726\n",
      "0.2150818407535553\n",
      "0.21724501252174377\n",
      "0.20547452569007874\n",
      "0.21524226665496826\n",
      "0.2074471265077591\n",
      "0.18938952684402466\n",
      "0.1612868309020996\n",
      "0.22117766737937927\n",
      "0.20995551347732544\n",
      "0.19128504395484924\n",
      "0.22149527072906494\n",
      "0.22242902219295502\n",
      "0.20073916018009186\n",
      "0.1946663111448288\n",
      "0.18113528192043304\n",
      "0.2291971743106842\n",
      "0.21576040983200073\n",
      "0.19545197486877441\n",
      "0.20995327830314636\n",
      "0.21849974989891052\n",
      "0.20706550776958466\n",
      "0.19914007186889648\n",
      "0.22756066918373108\n",
      "0.22360119223594666\n",
      "0.21938329935073853\n",
      "0.20360617339611053\n",
      "0.19278231263160706\n",
      "0.19491036236286163\n",
      "0.19429905712604523\n",
      "0.19231651723384857\n",
      "0.20001715421676636\n",
      "0.2025170922279358\n",
      "0.20364388823509216\n",
      "0.1896216869354248\n",
      "0.17347535490989685\n",
      "0.2139759361743927\n",
      "0.23611411452293396\n",
      "0.18852409720420837\n",
      "0.20257800817489624\n",
      "0.1983739584684372\n",
      "0.20403483510017395\n",
      "0.17427316308021545\n",
      "0.20421259105205536\n",
      "0.21061377227306366\n",
      "0.23526981472969055\n",
      "0.23198476433753967\n",
      "0.17702394723892212\n",
      "0.2220742404460907\n",
      "0.19764377176761627\n",
      "0.22827424108982086\n",
      "0.20622342824935913\n",
      "0.18446242809295654\n",
      "0.21151158213615417\n",
      "0.17780321836471558\n",
      "0.20286965370178223\n",
      "0.2042803019285202\n",
      "0.20442278683185577\n",
      "0.19727729260921478\n",
      "0.2130151391029358\n",
      "0.22892360389232635\n",
      "0.1708676815032959\n",
      "0.1988373100757599\n",
      "0.22932156920433044\n",
      "0.20600354671478271\n",
      "0.1929806023836136\n",
      "0.20481865108013153\n",
      "0.20270226895809174\n",
      "0.19014571607112885\n",
      "0.21263515949249268\n",
      "0.19924059510231018\n",
      "0.22325551509857178\n",
      "0.18110185861587524\n",
      "0.220754936337471\n",
      "0.1895567774772644\n",
      "0.21008805930614471\n",
      "0.17169982194900513\n",
      "0.20839840173721313\n",
      "0.20514366030693054\n",
      "0.19308382272720337\n",
      "0.2028459906578064\n",
      "0.20315515995025635\n",
      "0.1690043807029724\n",
      "0.20997239649295807\n",
      "0.1841733306646347\n",
      "0.19650229811668396\n",
      "0.19947071373462677\n",
      "0.1773935854434967\n",
      "0.19001711905002594\n",
      "0.24308308959007263\n",
      "0.21176815032958984\n",
      "0.20341373980045319\n",
      "0.2136390209197998\n",
      "0.21371573209762573\n",
      "0.23145049810409546\n",
      "0.20869022607803345\n",
      "0.19157357513904572\n",
      "0.18509764969348907\n",
      "0.19813206791877747\n",
      "0.19266021251678467\n",
      "0.199855774641037\n",
      "0.2035026252269745\n",
      "0.19938614964485168\n",
      "0.1844075620174408\n",
      "0.20987917482852936\n",
      "0.2137121856212616\n",
      "0.20541393756866455\n",
      "0.19326776266098022\n",
      "0.1784195899963379\n",
      "0.192143052816391\n",
      "0.19065631926059723\n",
      "0.2054973542690277\n",
      "0.20875932276248932\n",
      "0.1766975224018097\n",
      "0.20760853588581085\n",
      "0.18688485026359558\n",
      "0.1912449300289154\n",
      "0.18888765573501587\n",
      "0.21860383450984955\n",
      "0.20246484875679016\n",
      "0.20113524794578552\n",
      "0.19372281432151794\n",
      "0.18120168149471283\n",
      "0.196974515914917\n",
      "0.20094318687915802\n",
      "0.20887082815170288\n",
      "0.17916250228881836\n",
      "0.1990624964237213\n",
      "0.19555112719535828\n",
      "0.20056626200675964\n",
      "0.20589587092399597\n",
      "0.1768111288547516\n",
      "0.18913963437080383\n",
      "0.1768789142370224\n",
      "0.17720729112625122\n",
      "0.18411511182785034\n",
      "0.17518866062164307\n",
      "0.17168667912483215\n",
      "0.17654220759868622\n",
      "0.221408411860466\n",
      "0.16299283504486084\n",
      "0.19562150537967682\n",
      "0.19219964742660522\n",
      "0.19547972083091736\n",
      "0.19403022527694702\n",
      "0.2105509340763092\n",
      "0.20369938015937805\n",
      "0.22144189476966858\n",
      "0.20928162336349487\n",
      "0.19627398252487183\n",
      "0.22784797847270966\n",
      "0.20246616005897522\n",
      "0.17125670611858368\n",
      "0.19820308685302734\n",
      "0.19260600209236145\n",
      "0.20181697607040405\n",
      "0.17819491028785706\n",
      "0.1998533010482788\n",
      "0.1881030797958374\n",
      "0.188668891787529\n",
      "0.21246880292892456\n",
      "0.19987384974956512\n",
      "0.20092341303825378\n",
      "0.20029126107692719\n",
      "0.17110027372837067\n",
      "0.19889967143535614\n",
      "0.1918048858642578\n",
      "0.20623770356178284\n",
      "0.20194010436534882\n",
      "0.20076113939285278\n",
      "0.1777346432209015\n",
      "0.18572238087654114\n",
      "0.21062549948692322\n",
      "0.20057997107505798\n",
      "0.21946552395820618\n",
      "0.21034389734268188\n",
      "0.20163793861865997\n",
      "0.1768898367881775\n",
      "0.19418084621429443\n",
      "0.21005845069885254\n",
      "0.17825347185134888\n",
      "0.19018122553825378\n",
      "0.19219203293323517\n",
      "0.21499672532081604\n",
      "0.19572558999061584\n",
      "0.21489767730236053\n",
      "0.20128005743026733\n",
      "0.19353964924812317\n",
      "0.19275854527950287\n",
      "0.19046860933303833\n",
      "0.18310847878456116\n",
      "0.21693828701972961\n",
      "0.20460541546344757\n",
      "0.18330994248390198\n",
      "0.18906815350055695\n",
      "0.21154139935970306\n",
      "0.17502517998218536\n",
      "0.18952259421348572\n",
      "0.196031391620636\n",
      "0.19552917778491974\n",
      "0.20345568656921387\n",
      "0.18767577409744263\n",
      "0.19831909239292145\n",
      "0.19187115132808685\n",
      "0.21072638034820557\n",
      "0.17210225760936737\n",
      "0.19558656215667725\n",
      "0.17842790484428406\n",
      "0.19040800631046295\n",
      "0.20214155316352844\n",
      "0.19813591241836548\n",
      "0.2178880125284195\n",
      "0.1953577995300293\n",
      "0.1889466643333435\n",
      "0.1737792193889618\n",
      "0.19273686408996582\n",
      "0.20301881432533264\n",
      "0.2161024957895279\n",
      "0.21484479308128357\n",
      "0.18728724122047424\n",
      "0.21103405952453613\n",
      "0.2000589519739151\n",
      "0.19189924001693726\n",
      "0.2156994640827179\n",
      "0.17645540833473206\n",
      "0.21607288718223572\n",
      "0.19757351279258728\n",
      "0.18589897453784943\n",
      "0.2152547389268875\n",
      "0.21088120341300964\n",
      "0.18231603503227234\n",
      "0.1917305886745453\n",
      "0.17451094090938568\n",
      "0.18975873291492462\n",
      "0.19815796613693237\n",
      "0.1913788914680481\n",
      "0.19845467805862427\n",
      "0.20872411131858826\n",
      "0.19315949082374573\n",
      "0.20880606770515442\n",
      "0.1869116723537445\n",
      "0.18999069929122925\n",
      "0.17739465832710266\n",
      "0.21642494201660156\n",
      "0.17383873462677002\n",
      "0.19905707240104675\n",
      "0.2021830677986145\n",
      "0.1826307475566864\n",
      "0.19708938896656036\n",
      "0.1806355118751526\n",
      "0.18623286485671997\n",
      "0.20861122012138367\n",
      "0.1930927038192749\n",
      "0.19663959741592407\n",
      "0.18730086088180542\n",
      "0.19627675414085388\n",
      "0.18453899025917053\n",
      "0.19975131750106812\n",
      "0.18397295475006104\n",
      "0.2167297601699829\n",
      "0.18027369678020477\n",
      "0.18446777760982513\n",
      "0.17986364662647247\n",
      "0.2076525092124939\n",
      "0.19920764863491058\n",
      "0.2066575586795807\n",
      "0.17059467732906342\n",
      "0.20312844216823578\n",
      "0.17688104510307312\n",
      "0.21234747767448425\n",
      "0.19414761662483215\n",
      "0.19704371690750122\n",
      "0.1803758144378662\n",
      "0.20924112200737\n",
      "0.17562291026115417\n",
      "0.19411715865135193\n",
      "0.19157719612121582\n",
      "0.19621138274669647\n",
      "0.19021470844745636\n",
      "0.17573249340057373\n",
      "0.19900885224342346\n",
      "0.20392028987407684\n",
      "0.19455958902835846\n",
      "0.19290365278720856\n",
      "0.1913551539182663\n",
      "0.1867523491382599\n",
      "0.21803346276283264\n",
      "0.1830447018146515\n",
      "0.18057748675346375\n",
      "0.18894508481025696\n",
      "0.19783729314804077\n",
      "0.2049284279346466\n",
      "0.16900992393493652\n",
      "0.20688042044639587\n",
      "0.25832393765449524\n",
      "0.22078010439872742\n",
      "0.20083463191986084\n",
      "0.20380407571792603\n",
      "0.21310079097747803\n",
      "0.18568044900894165\n",
      "0.20739294588565826\n",
      "0.20270249247550964\n",
      "0.2055896818637848\n",
      "0.22755548357963562\n",
      "0.2079811841249466\n",
      "0.23070502281188965\n",
      "0.18590570986270905\n",
      "0.1792784035205841\n",
      "0.19449204206466675\n",
      "0.17308469116687775\n",
      "0.2023926079273224\n",
      "0.221686452627182\n",
      "0.20563563704490662\n",
      "0.17005588114261627\n",
      "0.228743776679039\n",
      "0.21883034706115723\n",
      "0.18254435062408447\n",
      "0.1820339560508728\n",
      "0.22153910994529724\n",
      "0.18521401286125183\n",
      "0.1787499338388443\n",
      "0.2048490345478058\n",
      "0.18728944659233093\n",
      "0.17518872022628784\n",
      "0.18813206255435944\n",
      "0.18728697299957275\n",
      "0.18163900077342987\n",
      "0.17367500066757202\n",
      "0.18700577318668365\n",
      "0.18578043580055237\n",
      "0.21514803171157837\n",
      "0.18525391817092896\n",
      "0.1874324530363083\n",
      "0.2094431221485138\n",
      "0.19191214442253113\n",
      "0.19768308103084564\n",
      "0.20127499103546143\n",
      "0.19846154749393463\n",
      "0.19625355303287506\n",
      "0.19924715161323547\n",
      "0.2047085016965866\n",
      "0.16557404398918152\n",
      "0.1951182782649994\n",
      "0.1779251992702484\n",
      "0.19614440202713013\n",
      "0.17933550477027893\n",
      "0.21820469200611115\n",
      "0.21179631352424622\n",
      "0.1712818145751953\n",
      "0.16702136397361755\n",
      "0.17701436579227448\n",
      "0.17015071213245392\n",
      "0.20553401112556458\n",
      "0.20146715641021729\n",
      "0.1756633073091507\n",
      "0.17854563891887665\n",
      "0.18055549263954163\n",
      "0.1651584506034851\n",
      "0.1831914633512497\n",
      "0.17505912482738495\n",
      "0.17954570055007935\n",
      "0.1836886703968048\n",
      "0.16762353479862213\n",
      "0.21159371733665466\n",
      "0.1903296858072281\n",
      "0.20826002955436707\n",
      "0.21447354555130005\n",
      "0.17183992266654968\n",
      "0.2117898166179657\n",
      "0.19079309701919556\n",
      "0.16452203691005707\n",
      "0.19550305604934692\n",
      "0.19235292077064514\n",
      "0.1822330206632614\n",
      "0.20643162727355957\n",
      "0.21188925206661224\n",
      "0.20875878632068634\n",
      "0.19830292463302612\n",
      "0.16738037765026093\n",
      "0.19392073154449463\n",
      "0.17375952005386353\n",
      "0.2108624279499054\n",
      "0.18576636910438538\n",
      "0.1730479896068573\n",
      "0.18031305074691772\n",
      "0.20431213080883026\n",
      "0.17041510343551636\n",
      "0.17024637758731842\n",
      "0.17535878717899323\n",
      "0.18601356446743011\n",
      "0.18232975900173187\n",
      "0.19705992937088013\n",
      "0.19871775805950165\n",
      "0.18364602327346802\n",
      "0.18420657515525818\n",
      "0.18236537277698517\n",
      "0.18582487106323242\n",
      "0.17942392826080322\n",
      "0.19017869234085083\n",
      "0.18880078196525574\n",
      "0.20309707522392273\n",
      "0.1788017302751541\n",
      "0.20159392058849335\n",
      "0.19745908677577972\n",
      "0.1824941635131836\n",
      "0.1731860637664795\n",
      "0.19404280185699463\n",
      "0.19466453790664673\n",
      "0.20323359966278076\n",
      "0.16838526725769043\n",
      "0.1969454139471054\n",
      "0.17113441228866577\n",
      "0.17744414508342743\n",
      "0.20380328595638275\n",
      "0.19538834691047668\n",
      "0.18386909365653992\n",
      "0.17604473233222961\n",
      "0.20532387495040894\n",
      "0.18896105885505676\n",
      "0.16548770666122437\n",
      "0.16550582647323608\n",
      "0.164763942360878\n",
      "0.2069242149591446\n",
      "0.17872518301010132\n",
      "0.18464545905590057\n",
      "0.19614148139953613\n",
      "0.19105592370033264\n",
      "0.17516380548477173\n",
      "0.18874046206474304\n",
      "0.1953132450580597\n",
      "0.2020113170146942\n",
      "0.17787203192710876\n",
      "0.16393721103668213\n",
      "0.1914343684911728\n",
      "0.1868927776813507\n",
      "0.18895471096038818\n",
      "0.18683820962905884\n",
      "0.17869743704795837\n",
      "0.20912526547908783\n",
      "0.2095564901828766\n",
      "0.17854724824428558\n",
      "0.18003644049167633\n",
      "0.20435777306556702\n",
      "0.20731374621391296\n",
      "0.17692263424396515\n",
      "0.18772295117378235\n",
      "0.17999637126922607\n",
      "0.16274432837963104\n",
      "0.19334089756011963\n",
      "0.18933182954788208\n",
      "0.1648549884557724\n",
      "0.16682738065719604\n",
      "0.15777373313903809\n",
      "0.1980634331703186\n",
      "0.20719018578529358\n",
      "0.17070767283439636\n",
      "0.19369885325431824\n",
      "0.1732879877090454\n",
      "0.19139054417610168\n",
      "0.16369695961475372\n",
      "0.219680055975914\n",
      "0.19207793474197388\n",
      "0.18409904837608337\n",
      "0.1850028932094574\n",
      "0.15963126718997955\n",
      "0.1710837483406067\n",
      "0.17857345938682556\n",
      "0.17965930700302124\n",
      "0.1918318122625351\n",
      "0.16979727149009705\n",
      "0.20246262848377228\n",
      "0.19160139560699463\n",
      "0.1712644249200821\n",
      "0.19239136576652527\n",
      "0.1968107521533966\n",
      "0.19939452409744263\n",
      "0.1812179535627365\n",
      "0.18974633514881134\n",
      "0.19856882095336914\n",
      "0.1918606162071228\n",
      "0.1867600977420807\n",
      "0.17570005357265472\n",
      "0.18509911000728607\n",
      "0.1874585747718811\n",
      "0.19026097655296326\n",
      "0.18623435497283936\n",
      "0.2171393632888794\n",
      "0.18682177364826202\n",
      "0.16947394609451294\n",
      "0.17121879756450653\n",
      "0.1811227649450302\n",
      "0.1735665202140808\n",
      "0.1902921497821808\n",
      "0.1714073121547699\n",
      "0.18535475432872772\n",
      "0.1801813542842865\n",
      "0.17167846858501434\n",
      "0.20636802911758423\n",
      "0.15985757112503052\n",
      "0.16572274267673492\n",
      "0.17481502890586853\n",
      "0.1792399287223816\n",
      "0.1797240972518921\n",
      "0.18424975872039795\n",
      "0.19762922823429108\n",
      "0.1849062591791153\n",
      "0.19148680567741394\n",
      "0.1997973918914795\n",
      "0.1837310642004013\n",
      "0.17252495884895325\n",
      "0.17054034769535065\n",
      "0.18916460871696472\n",
      "0.15867894887924194\n",
      "0.1838570088148117\n",
      "0.20197008550167084\n",
      "0.19123268127441406\n",
      "0.1990739405155182\n",
      "0.18318834900856018\n",
      "0.18590474128723145\n",
      "0.16498273611068726\n",
      "0.20145228505134583\n",
      "0.20149073004722595\n",
      "0.1772439181804657\n",
      "0.16915912926197052\n",
      "0.17268528044223785\n",
      "0.17668531835079193\n",
      "0.17887753248214722\n",
      "0.17012278735637665\n",
      "0.1793021261692047\n",
      "0.17486509680747986\n",
      "0.1795717477798462\n",
      "0.19766317307949066\n",
      "0.17898154258728027\n",
      "0.19099119305610657\n",
      "0.20009589195251465\n",
      "0.15738192200660706\n",
      "0.16883888840675354\n",
      "0.1890202760696411\n",
      "0.19958418607711792\n",
      "0.1807289570569992\n",
      "0.1865251064300537\n",
      "0.19670560956001282\n",
      "0.18311059474945068\n",
      "0.18363165855407715\n",
      "0.15647156536579132\n",
      "0.17974162101745605\n",
      "0.1607399433851242\n",
      "0.1878422051668167\n",
      "0.19461634755134583\n",
      "0.17053312063217163\n",
      "0.16960138082504272\n",
      "0.1636979877948761\n",
      "0.19533361494541168\n",
      "0.18135377764701843\n",
      "0.17782941460609436\n",
      "0.1791146695613861\n",
      "0.184864804148674\n",
      "0.17820674180984497\n",
      "0.1886221468448639\n",
      "0.18532904982566833\n",
      "0.1802334487438202\n",
      "0.17680637538433075\n",
      "0.16879916191101074\n",
      "0.15650051832199097\n",
      "0.20140966773033142\n",
      "0.16402769088745117\n",
      "0.20068518817424774\n",
      "0.16364137828350067\n",
      "0.1863778978586197\n",
      "0.18140585720539093\n",
      "0.16470083594322205\n",
      "0.17858362197875977\n",
      "0.20286011695861816\n",
      "0.2104710191488266\n",
      "0.18243449926376343\n",
      "0.18838463723659515\n",
      "0.21311520040035248\n",
      "0.17832979559898376\n",
      "0.19653785228729248\n",
      "0.173928901553154\n",
      "0.18918564915657043\n",
      "0.1658352166414261\n",
      "0.17643466591835022\n",
      "0.16608481109142303\n",
      "0.19516520202159882\n",
      "0.20144595205783844\n",
      "0.17664432525634766\n",
      "0.16366977989673615\n",
      "0.1831868290901184\n",
      "0.17958387732505798\n",
      "0.17112401127815247\n",
      "0.16171154379844666\n",
      "0.17802581191062927\n",
      "0.1539236307144165\n",
      "0.17137566208839417\n",
      "0.17010091245174408\n",
      "0.17939826846122742\n",
      "0.18553096055984497\n",
      "0.15783584117889404\n",
      "0.17337539792060852\n",
      "0.18644839525222778\n",
      "0.1744595170021057\n",
      "0.19363777339458466\n",
      "0.1859419196844101\n",
      "0.18727102875709534\n",
      "0.18718048930168152\n",
      "0.16341331601142883\n",
      "0.18400144577026367\n",
      "0.1736779510974884\n",
      "0.17424826323986053\n",
      "0.17657659947872162\n",
      "0.20208603143692017\n",
      "0.16113920509815216\n",
      "0.1875978410243988\n",
      "0.1716732531785965\n",
      "0.17031890153884888\n",
      "0.1688961386680603\n",
      "0.18311536312103271\n",
      "0.17066995799541473\n",
      "0.19441628456115723\n",
      "0.1955665946006775\n",
      "0.16747665405273438\n",
      "0.1731124222278595\n",
      "0.18334728479385376\n",
      "0.15678700804710388\n",
      "0.17875607311725616\n",
      "0.1891423910856247\n",
      "0.1882762610912323\n",
      "0.16399607062339783\n",
      "0.17771047353744507\n",
      "0.18576011061668396\n",
      "0.1848534792661667\n",
      "0.18019847571849823\n",
      "0.19344544410705566\n",
      "0.18762530386447906\n",
      "0.18814752995967865\n",
      "0.16735300421714783\n",
      "0.1674538254737854\n",
      "0.19108881056308746\n",
      "0.15263637900352478\n",
      "0.16762879490852356\n",
      "0.17551717162132263\n",
      "0.21162082254886627\n",
      "0.16525506973266602\n",
      "0.18198460340499878\n",
      "0.19207435846328735\n",
      "0.184132382273674\n",
      "0.16609777510166168\n",
      "0.18905285000801086\n",
      "0.17407967150211334\n",
      "0.1848168969154358\n",
      "0.19779972732067108\n",
      "0.17050790786743164\n",
      "0.15416906774044037\n",
      "0.15399521589279175\n",
      "0.17383277416229248\n",
      "0.1691991686820984\n",
      "0.19016151130199432\n",
      "0.17605233192443848\n",
      "0.16388103365898132\n",
      "0.1821710616350174\n",
      "0.18688702583312988\n",
      "0.18558675050735474\n",
      "0.18150144815444946\n",
      "0.19712437689304352\n",
      "0.1774192899465561\n",
      "0.17229169607162476\n",
      "0.17180189490318298\n",
      "0.18675000965595245\n",
      "0.16956542432308197\n",
      "0.17487728595733643\n",
      "0.1775418221950531\n",
      "0.17548923194408417\n",
      "0.18600572645664215\n",
      "0.17708231508731842\n",
      "0.18224290013313293\n",
      "0.18150295317173004\n",
      "0.1828082799911499\n",
      "0.18988491594791412\n",
      "0.17502960562705994\n",
      "0.19450423121452332\n",
      "0.17055830359458923\n",
      "0.162351593375206\n",
      "0.1842138171195984\n",
      "0.19417592883110046\n",
      "0.1586616486310959\n",
      "0.18255217373371124\n",
      "0.17478367686271667\n",
      "0.1666661500930786\n",
      "0.17722919583320618\n",
      "0.2008867710828781\n",
      "0.1624591052532196\n",
      "0.1996951401233673\n",
      "0.1905084252357483\n",
      "0.18606220185756683\n",
      "0.19497044384479523\n",
      "0.17621496319770813\n",
      "0.1835361272096634\n",
      "0.17493639886379242\n",
      "0.17049559950828552\n",
      "0.18344372510910034\n",
      "0.1550517976284027\n",
      "0.1815980225801468\n",
      "0.19661276042461395\n",
      "0.1770426630973816\n",
      "0.1697940230369568\n",
      "0.19912680983543396\n",
      "0.15831905603408813\n",
      "0.1797676831483841\n",
      "0.17755717039108276\n",
      "0.17445796728134155\n",
      "0.17198388278484344\n",
      "0.17071855068206787\n",
      "0.1751755177974701\n",
      "0.17713922262191772\n",
      "0.17010506987571716\n",
      "0.19066280126571655\n",
      "0.16838112473487854\n",
      "0.17749561369419098\n",
      "0.16718120872974396\n",
      "0.172084778547287\n",
      "0.17530424892902374\n",
      "0.1752699613571167\n",
      "0.18434946238994598\n",
      "0.17067602276802063\n",
      "0.17336615920066833\n",
      "0.15730103850364685\n",
      "0.19892200827598572\n",
      "0.17529600858688354\n",
      "0.16127648949623108\n",
      "0.19012221693992615\n",
      "0.1617710441350937\n",
      "0.18275567889213562\n",
      "0.1778486967086792\n",
      "0.16146114468574524\n",
      "0.17263031005859375\n",
      "0.1686403751373291\n",
      "0.16602963209152222\n",
      "0.15876343846321106\n",
      "0.17743828892707825\n",
      "0.15154215693473816\n",
      "0.16482800245285034\n",
      "0.16063794493675232\n",
      "0.1977686583995819\n",
      "0.17657259106636047\n",
      "0.19813382625579834\n",
      "0.1593858003616333\n",
      "0.1722303032875061\n",
      "0.17667169868946075\n",
      "0.17180413007736206\n",
      "0.17477531731128693\n",
      "0.1946638524532318\n",
      "0.16244730353355408\n",
      "0.17822501063346863\n",
      "0.18603381514549255\n",
      "0.18548670411109924\n",
      "0.16063182055950165\n",
      "0.17494188249111176\n",
      "0.163248673081398\n",
      "0.1800265908241272\n",
      "0.18023279309272766\n",
      "0.17517852783203125\n",
      "0.18840181827545166\n",
      "0.17882800102233887\n",
      "0.19257548451423645\n",
      "0.18816432356834412\n",
      "0.16184338927268982\n",
      "0.18920999765396118\n",
      "0.17490829527378082\n",
      "0.18784181773662567\n",
      "0.182580828666687\n",
      "0.15397155284881592\n",
      "0.16802728176116943\n",
      "0.17661742866039276\n",
      "0.18260455131530762\n",
      "0.1926373988389969\n",
      "0.17404840886592865\n",
      "0.18220254778862\n",
      "0.19707822799682617\n",
      "0.1789960414171219\n",
      "0.18581132590770721\n",
      "0.17930632829666138\n",
      "0.15919920802116394\n",
      "0.17879880964756012\n",
      "0.16829420626163483\n",
      "0.1727309226989746\n",
      "0.17131339013576508\n",
      "0.1650978922843933\n",
      "0.15925323963165283\n",
      "0.19411128759384155\n",
      "0.16761022806167603\n",
      "0.17075566947460175\n",
      "0.16586007177829742\n",
      "0.16272202134132385\n",
      "0.17025205492973328\n",
      "0.1728375256061554\n",
      "0.17725370824337006\n",
      "0.16540773212909698\n",
      "0.17217300832271576\n",
      "0.15572082996368408\n",
      "0.19536323845386505\n",
      "0.17842379212379456\n",
      "0.17470920085906982\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[137]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m logits, loss = model(xb, yb)\n\u001b[32m     11\u001b[39m loss = loss\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     14\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py:311\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    306\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    310\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2259\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   2257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   2258\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2245\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction.backward.<locals>.impl_fn\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimpl_fn\u001b[39m(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     out = \u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_backward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _backward_epilogue_functional(\n\u001b[32m   2247\u001b[39m         CompiledFunction.metadata,\n\u001b[32m   2248\u001b[39m         CompiledFunction.maybe_subclass_metadata,\n\u001b[32m   2249\u001b[39m         out,\n\u001b[32m   2250\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:2376\u001b[39m, in \u001b[36mAOTDispatchAutograd.post_compile.<locals>.CompiledFunction._backward_impl\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2360\u001b[39m     torch._functorch.config.donated_buffer\n\u001b[32m   2361\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m saved_tensors_use_once\n\u001b[32m   2362\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m fw_metadata.bw_donated_idxs != []\n\u001b[32m   2363\u001b[39m ):\n\u001b[32m   2364\u001b[39m     torch._check(\n\u001b[32m   2365\u001b[39m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2366\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: (\n\u001b[32m   (...)\u001b[39m\u001b[32m   2373\u001b[39m         ),\n\u001b[32m   2374\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2376\u001b[39m out = \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCompiledFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompiled_bw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[39m, in \u001b[36mcall_func_at_runtime_with_args\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m_boxed_call\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m         out = normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[32m    130\u001b[39m         warnings.warn(\n\u001b[32m    131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt take boxed arguments. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    927\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py:584\u001b[39m, in \u001b[36mCompiledFxGraph.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m     get_runtime_metrics_context().finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/2b/c2btqylowdmixa2x4j4kc7rrn3en2w6yyqpx4vffabrfz5hiirtb.py:3178\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m   3176\u001b[39m buf81 = empty_strided_cpu((\u001b[32m512\u001b[39m, \u001b[32m1280\u001b[39m), (\u001b[32m1280\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n\u001b[32m   3177\u001b[39m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3178\u001b[39m \u001b[43mextern_kernels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreinterpret_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf79\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m8192\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview_76\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuf81\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3179\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m view_76\n\u001b[32m   3180\u001b[39m buf82 = empty_strided_cpu((\u001b[32m1\u001b[39m, \u001b[32m512\u001b[39m), (\u001b[32m512\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Val loss (avg over 200 steps): 1.7724 | Perplexity: 5.89\n"
     ]
    }
   ],
   "source": [
    "# === Validation loader & evaluation ===\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# NOTE: your config should use the vocab_size loaded from meta.pkl.\n",
    "# If you accidentally used len(stoi) here, switch to vocab_size.\n",
    "# config = GPTConfig(vocab_size=vocab_size, ...)\n",
    "\n",
    "# Build a deterministic-ish validation dataset:\n",
    "# - p_aligned=1.0 ensures no random jitter offsets\n",
    "# - jitter=0 disables extra offset entirely\n",
    "val_dataset = GPUBatchDataset(\n",
    "    val_ids,\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    jitter=0,\n",
    "    p_aligned=1.0,\n",
    "    pad_len=0\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "def evaluate_validation_loss(model, loader, vocab_size, max_steps=200, seed=123):\n",
    "    \"\"\"\n",
    "    Evaluate average cross-entropy loss and perplexity on a subset of the validation set.\n",
    "    - max_steps: how many batches to average over (caps runtime)\n",
    "    - seed: sets numpy's RNG so GPUBatchDataset sampling is repeatable across runs\n",
    "    \"\"\"\n",
    "    # Make dataset sampling repeatable (GPUBatchDataset uses np.random)\n",
    "    import numpy as _np\n",
    "    _np.random.seed(seed)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (X, Y) in enumerate(loader):\n",
    "            # DataLoader(batch_size=1) adds an extra leading dim -> (1, B, T)\n",
    "            # Squeeze to get (B, T)\n",
    "            X = X[0]\n",
    "            Y = Y[0]\n",
    "\n",
    "            # Forward pass. If your GPT forward returns a tuple, grab logits accordingly.\n",
    "            logits ,loss = model(X,Y)            # shape expected: (B, T, vocab_size)\n",
    "            # If your model returns (logits, loss) uncomment the next line and comment the above:\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            steps += 1\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "\n",
    "    avg_loss = total_loss / max(1, steps)\n",
    "    ppl = math.exp(avg_loss)\n",
    "    print(f\"üß™ Val loss (avg over {steps} steps): {avg_loss:.4f} | Perplexity: {ppl:.2f}\")\n",
    "    return avg_loss, ppl\n",
    "\n",
    "# === Run it ===\n",
    "val_loss, val_ppl = evaluate_validation_loss(model, val_loader, vocab_size=vocab_size, max_steps=200, seed=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses[20:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c_XtPYjAb2M",
    "outputId": "a30a6626-e1ab-4ea5-ea3d-8f6d92d2422e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def decode_sequence_char(\n",
    "    model, stoi, itos, prompt, max_new_tokens=100, block_size=256,\n",
    "    use_fenchel=False, tau=1.0, fenchel_iters=3, temperature=1.0\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    max_ctx = int(block_size)\n",
    "\n",
    "    # ?1 pad token: try space, fallback to 0\n",
    "    space_id = stoi.get(' ', 0)\n",
    "\n",
    "    # encode prompt\n",
    "    start_ids = torch.tensor([encode_chars(prompt, stoi)], dtype=torch.long, device=device)\n",
    "\n",
    "    # If prompt is longer than block, keep the last block_size chars\n",
    "    if start_ids.size(1) > max_ctx:\n",
    "        start_ids = start_ids[:, -max_ctx:]\n",
    "\n",
    "    # Left-pad to exactly block_size so the first pass sees a full window\n",
    "    pad_len = max(0, max_ctx - start_ids.size(1))\n",
    "    if pad_len > 0:\n",
    "        pad = torch.full((1, pad_len), space_id, dtype=torch.long, device=device)\n",
    "        idx = torch.cat([pad, start_ids], dim=1)  # shape [1, block_size]\n",
    "    else:\n",
    "        idx = start_ids  # already block_size or longer (truncated above)\n",
    "\n",
    "    initial_pad_len = pad_len  # for stripping at the end\n",
    "\n",
    "    # generation loop\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            context = idx[:, -max_ctx:]  # always feed the last block_size tokens\n",
    "            logits, _ = model(context, None)\n",
    "            last_logits = logits[:, -1, :]\n",
    "            if temperature != 1.0:\n",
    "                last_logits = last_logits / float(temperature)\n",
    "            probs = torch.softmax(last_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # [1,1]\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    # drop the initial left-pad when decoding to text\n",
    "    out_tokens = idx[0, initial_pad_len:].tolist()\n",
    "    return decode_chars(out_tokens, itos)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (128) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m itos = meta[\u001b[33m\"\u001b[39m\u001b[33mitos\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     73\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33mROMEO: Juliet, do you love me?  JULIET:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m generated = \u001b[43mdecode_sequence_char_rapid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstoi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstoi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mitos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mitos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\n\u001b[32m     82\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(generated)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mdecode_sequence_char_rapid\u001b[39m\u001b[34m(model, stoi, itos, prompt, max_new_tokens, block_size, temperature, space_fallback)\u001b[39m\n\u001b[32m     41\u001b[39m     x_t = model.transformer.wte(idx_t)  \u001b[38;5;66;03m# (1,C)\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m blk, st \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model.transformer.h, feat_states):\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m         x_t = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# We'll accumulate only from the true prompt onward\u001b[39;00m\n\u001b[32m     46\u001b[39m out = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 348\u001b[39m, in \u001b[36mGPTSemanticBlock.step\u001b[39m\u001b[34m(self, inp, feat_state)\u001b[39m\n\u001b[32m    345\u001b[39m x_t,global_c = torch.chunk(inp, \u001b[32m2\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    347\u001b[39m B, C = x_t.shape\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m feats_t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_state\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, K, C)\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.K > \u001b[32m0\u001b[39m:\n\u001b[32m    351\u001b[39m     \u001b[38;5;66;03m# apply each bottleneck to the corresponding (B, C) slice\u001b[39;00m\n\u001b[32m    352\u001b[39m     processed = [\u001b[38;5;28mself\u001b[39m.bottlenecks[j](feats_t[:, j, :]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.K)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 285\u001b[39m, in \u001b[36mSemanticClusterFeaturesCausal.step\u001b[39m\u001b[34m(self, x_t, state)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_t: torch.Tensor, state: CausalPyramidState) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 231\u001b[39m, in \u001b[36mCausalPyramidState.step\u001b[39m\u001b[34m(self, x_t)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# ------- token PT (read BEFORE any push) -------\u001b[39;00m\n\u001b[32m    230\u001b[39m prev_x = \u001b[38;5;28mself\u001b[39m._read_lookback(level=\u001b[32m0\u001b[39m, r=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Œº0(t-1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m d1 = \u001b[43mphase_transport_between\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m.squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.t == \u001b[32m0\u001b[39m:\n\u001b[32m    233\u001b[39m     d1.zero_()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mphase_transport_between\u001b[39m\u001b[34m(curr, prev, tau)\u001b[39m\n\u001b[32m     17\u001b[39m u = _unit(curr)\n\u001b[32m     18\u001b[39m v = _unit(prev)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m w = \u001b[43mcurr\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev\u001b[49m\n\u001b[32m     21\u001b[39m c = (u * v).sum(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# (B,T,1)\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# masks (all as (B,T))\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (64) must match the size of tensor b (128) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def decode_sequence_char_rapid(\n",
    "    model, stoi, itos, prompt,\n",
    "    max_new_tokens=100, block_size=1024,\n",
    "    temperature=1.0, space_fallback=' '\n",
    "):\n",
    "    \"\"\"\n",
    "    Fast incremental generator using cached causal features, now matching the\n",
    "    left-pad-to-block behavior of your regular forward method.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    B = 1\n",
    "\n",
    "    space_id = stoi.get(space_fallback, 0)\n",
    "    prompt_ids = encode_chars(prompt, stoi)\n",
    "    if len(prompt_ids) == 0:\n",
    "        prompt_ids = [space_id]\n",
    "\n",
    "    # --- left-pad to block_size to match your regular decode ---\n",
    "    pad_len = max(0, block_size - len(prompt_ids))\n",
    "    pad_ids = [space_id] * pad_len\n",
    "    priming_ids = pad_ids + prompt_ids  # we will NOT include pad_ids in the returned text\n",
    "\n",
    "    # --- per-block feature caches ---\n",
    "    feat_states = [\n",
    "        CausalPyramidState(\n",
    "            num_scales=model.config.n_scales,\n",
    "            C=model.config.n_embd,\n",
    "            device=device,\n",
    "            batch_size=B,\n",
    "            tau=1e-6\n",
    "        ) for _ in model.transformer.h\n",
    "    ]\n",
    "\n",
    "    # --- prime caches with padded prompt (no output accumulation for pad) ---\n",
    "    ids = torch.tensor([priming_ids], dtype=torch.long, device=device)  # (1, T0)\n",
    "    x_t = None\n",
    "    for t in range(ids.size(1)):\n",
    "        idx_t = ids[:, t]\n",
    "        x_t = model.transformer.wte(idx_t)  # (1,C)\n",
    "        for blk, st in zip(model.transformer.h, feat_states):\n",
    "            x_t = blk.step(x_t, st)\n",
    "\n",
    "    # We'll accumulate only from the true prompt onward\n",
    "    out = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    # --- incremental rollout ---\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.lm_head(x_t)  # (1,V)\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / float(temperature)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "\n",
    "        out = torch.cat([out, next_token], dim=1)\n",
    "        if out.size(1) > block_size:\n",
    "            out = out[:, -block_size:]  # keep UX identical to regular path\n",
    "\n",
    "        # advance one step\n",
    "        tok = next_token.squeeze(-1)     # (1,)\n",
    "        x_t = model.transformer.wte(tok) # (1,C)\n",
    "        for blk, st in zip(model.transformer.h, feat_states):\n",
    "            x_t = blk.step(x_t, st)\n",
    "\n",
    "    return decode_chars(out[0].tolist(), itos)\n",
    "    \n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "\n",
    "prompt = \"ROMEO: Juliet, do you love me?  JULIET:\"\n",
    "generated = decode_sequence_char_rapid(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=8024,\n",
    "    block_size=1024,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Juliet, do you love me?  JULIET:\n",
      "I mean, floweld grants himsides the\n",
      "mascleams are and\n",
      "way to fair praces, paring to bearst;\n",
      "And, till more nius frish nost friampleaus alm colamstrice,\n",
      "onotly shall I will my siry knot.,\n",
      "My look'd your is abora.\n",
      "\n",
      "MENENIUS:\n",
      "Cme, leaver-blord, burn'd afurth hard;\n",
      "But threaly\n",
      "And,\n",
      "Then wass thes doth a hand ther friard,\n",
      "And when have hath my dauiturn's yeal;\n",
      "Of iffer her be britter annaEdy you, yet for mone oble told.\n",
      "\n",
      "KING RICHARD II:\n",
      "I them to did, barm ack, when unmon'Hols?\n",
      "\n",
      "ad the put must not, lark! But is mone to le;\n",
      "ToLYCUS:\n",
      "Come to his is chars no not so the would\n",
      "For now oullow packouved my frit.\n",
      "Provost:\n",
      "For with repar?\n",
      "\n",
      "\n",
      "WARWICK:\n",
      "What hout so? Came or Freath,\n",
      "I never affore of lack than I mone warristion.\n",
      "\n",
      "JULIET:\n",
      "He go me?\n",
      "\n",
      "BRUTVIRA:\n",
      "O, the mushall strucetione to your poor him.\n",
      "\n",
      "KING EDWARD IV:\n",
      "What, when Alack thou mysel order.\n",
      "\n",
      "SICINIUS:\n",
      "And beveV:\n",
      "Thou beity the mand the for the did the prison.\n",
      "\n",
      "God flove thou be and the the oully to dis using'd\n",
      "en the supplaise, and it ong sis all get sight:\n",
      "O, my patee wall and like to both.\n",
      "\n",
      "WARWICk; foold servengequices, all&.\n",
      "\n",
      "ROMEO:\n",
      "Then his I will him one son of his a fears.\n",
      "\n",
      "COMINIUS:\n",
      "Thou  carram man\n",
      "My lare Mentable a--all bowour, great no it.\n",
      "\n",
      "JULIET:\n",
      "Why, we one inteo, thigh tribantal and in can do peace pit\n",
      "In coull, and your of Goduch you persul now the as hee then a wor\n",
      "the done itself them the gent that for my of sacre\n",
      "Make enUP tor of pon another in ain the my swife,\n",
      "That is the him! They, his will tentit theyse unpresess\n",
      "don obled not is thyself of he's thouse oun and erear,\n",
      "And, untin the for you kneed werearm,\n",
      "Sirs than that like fram tomen a fin thim:\n",
      "But make re leasby of it you host to the greatts good damels: but so.\n",
      "\n",
      "Servingman:\n",
      "And my tim the the is horse I dese insue to my forth,\n",
      "Swign of should to calous in the mare with peer be him ports;\n",
      "For no the sir, I us trave many too.\n",
      "\n",
      "PRINCEN:\n",
      "To, do per, thalk a cound own hesirand to keepry might.\n",
      "\n",
      "Friest what! I have that be sight.\n",
      "\n",
      "ROMEO:\n",
      "Such it thou word, I swas may you.\n",
      "\n",
      "GYENcZioEd:\n",
      "TheX soness! every petay or I do prepase,\n",
      "I give my laniongs. What I sourp ame do the gries;\n",
      "For enver-taid ther; and reare, some sig be not thene that weard her will be some wither stress, fail his fyranWs\n",
      "The din my in thee dids edids that oce\n",
      "To so re when o' looke.\n",
      "\n",
      "YORK:\n",
      "Would that we sweet, I well the is you some is the own the reignt.\n",
      "\n",
      "Les:, thou lord, and you'll the you thy of that an this anst henrice\n",
      "With that he day fill good mfortune\n",
      "you hope, let me it ther mights, says, and solly the drice;\n",
      "But to his of some o't, and the see but motly tears Rave,\n",
      "O, my hourt, and that tell and lattempt you hear upon of the the crip,\n",
      "The may bacuse appoixl: mothine,\n",
      "And CleausHSellous by good partaget$ shame their grgue\n",
      "To the badeful rokk the conful shame.\n",
      "\n",
      "may have me hathurse.\n",
      "\n",
      "CAMILLO:\n",
      "Yet look on mess. Edward me sor moved;\n",
      "And and by my tord, then is sold,\n",
      "Every death, dears a enter alling.\n",
      "\n",
      "KING EDWARD IV:\n",
      "But what hour for him good cruse with ow the come here the king\n",
      "Were thy remans at marror and fare me partly us mut power no die sace;\n",
      "I wJar hime To re hands to was it.\n",
      "\n",
      "Retraway that make too:\n",
      "Why, the rit of what the wish nother's a death hantst sull be hear gat wRaulth of mercsuared.\n",
      "\n",
      "ELKING EDWARD IV:\n",
      "In proppease:\n",
      "Saye to you sage to with hall me.\n",
      "\n",
      "HASTINGS:\n",
      "For a the fair mark'should flame,' the lay moved,\n",
      "I do tryEigh you bown the in them;\n",
      "Be dit his usech in our but fare the kins\n",
      "With that the thies doo.\n",
      "\n",
      "EXETER:\n",
      "I shoope the from the tress bied.\n",
      "\n",
      "CAMILLO:\n",
      "I worrow the spreser thee,\n",
      "PuchXould thLew you gaterfor los;\n",
      "For I decrond anot; if your ther it.\n",
      "Repre come her the till be would do;\n",
      "Ere more this scorderliciant of eye sounds\n",
      "Tity seand angry, fone lefore to beguin griet\n",
      "Prover-piect and friban untis burnight the the-dire?\n",
      "\n",
      "COMINIUS:\n",
      "I moan ame hurd: hart defea,\n",
      "Wheir light sonly fail of unpreisu.\n",
      "\n",
      "Second Servingman:\n",
      "And I will I warly tranuray sman!\n",
      "\n",
      "COMINIUS:\n",
      "Have, he man, then conted as a live my trant.\n",
      "\n",
      "CORIOLANUS:\n",
      "No, I will to thee the crown.\n",
      "\n",
      "WARWICK:\n",
      "My mother fit is not a wis, what I say ou.\n",
      "\n",
      "JULIET:\n",
      "I will not this a quater\n",
      "My world be a this husholient some like ithou.\n",
      "\n",
      "LADY CAPULET:\n",
      "You that itself of a way,\n",
      "And that hy good cous.\n",
      "Commowcians, fare ever.\n",
      "\n",
      "LEONTES:\n",
      "Misenager, to some wrong the gaid the thring deaths wain,\n",
      "When comestending his are ti? the die;\n",
      "What, should love, you are it with one of thy lactory me the dent presed newl, and make beath!\n",
      "A last upon it was it this virture--tat's the nother.\n",
      "\n",
      "CORIOLANUS:\n",
      "No, if Gauntle and ene of antiment this men's made ount:\n",
      "Why soul sleake, our no my how I stake thexperseruest mfortunce\n",
      "To hir.\n",
      "\n",
      "SICINIUS:\n",
      "O eed hid you a be to deed, for higolqueen: queen,\n",
      "With then I warring be burgar hem.\n",
      "\n",
      "LORD CAS:\n",
      "Now to the preach a pounte.\n",
      "\n",
      "LADY ANNE:\n",
      "you the ar namen well ongue can a could he like of justicI him confull is frimisely ey\n",
      "And threus haughter crangent forturs?\n",
      "\n",
      "KING EDWARD IV:\n",
      "And thou not he pleasure with then my bosthere,--\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Busher the dish perof them corly feed you known aster,\n",
      "Blackly him the well me good I alty time the sould thou hastiry\n",
      "A passPdows.---\n",
      "\n",
      "O'K:\n",
      "I was trace\n",
      "Pray:\n",
      "Denery that you their sup of the mild to my father.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "My graughting, cheat he milly have for mi-dured to low,\n",
      "The sback to all be day brother, King hee with be and this love\n",
      "Again the peoplicious ent be the his hall uste,\n",
      "And shaPressis lew with his plaish\n",
      "And the live chame impetQeEatens\n",
      "A prided their port the queen me to them an theyears,\n",
      "As away the give as merangther, away.\n",
      "\n",
      "Briety the prolute?\n",
      "\n",
      "GLOUCESTER:\n",
      "Baundear, come may upon my poor poor his cours distract.\n",
      "I re see be strove of so.\n",
      "\n",
      "COMINIUS:\n",
      "A gavee,\n",
      "Think I wou: inder I sir, such beginus, are to the bears the with hin have men time from justice,\n",
      "Alain tie time do.\n",
      "\n",
      "tENraction:\n",
      "Bear, no eit Rome in come ouve mone busid too be to now.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Upon love me her come; but as that ear no\n",
      "I gave poor the as mans and a shatter\n",
      "Beminy proudinely in this kinsman,\n",
      "Which per.\n",
      "Stall, and the gold me to young be which convy him;\n",
      "The loben aputy a noto a cat would unity,\n",
      "Unless by breare,\n",
      "And I fame, in atch, an thy peacentention\n",
      "Which hasters thee, ears I have forshipol\n",
      "Of brlish him speaks, but is age.\n",
      "I hope some of mi: grow.\n",
      "\n",
      "LADY CAPULET:\n",
      "Marcheop! he and 'tis show of the rather do will not sir.\n",
      "Peer istords, to might, Bole thou do will is faith,\n",
      "Se hath cold many sires to my hear?\n",
      "\n",
      "First.\n",
      "\n",
      "EDWARD:\n",
      "Good a plain, and my lis at to friend to fell hie comestion put of thou that d.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "My do be you that wouldies.--\n",
      "The day, 'tis pale to some to strup\n",
      "Ort his from crows,\n",
      "For Go be aulity live forbe, what evious arit;\n",
      "And unbide of you stme good\n",
      "Marcursed sagger than like us no you hose:\n",
      "I do some stain come it of four sor eaus\n",
      "That me all woman to the ragaps son.\n",
      "\n",
      "COMINIUS:\n",
      "A their you moon! fare father of upon the thesem.\n",
      "The herold ackle apper, of they sill\n",
      "I say, so, if I parrity\n",
      "Anterms, Is dear leight we of you,\n",
      "You instrous my prin I thene is thou tell beye untime a proople, sor poor his father incheard time a prepeaus.\n",
      "\n",
      "EXETER:\n",
      "Thirds ushart pring in this soundem.\n",
      "The canius new forment\n",
      "'Tas approfess the like own come latishly butterms of the that come thy nukesles hear my may barn durnly the druzph hed.\n",
      "\n",
      "Nurse unkner aw hall this fillof\n",
      "TTrust cothen ingly all go reaps live\n",
      "The was acrounart of the gunter'd what ong to make and on the stands; it in blea, your to we to the a pleast.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "What ame, if the the reye hath?a\n",
      "six many a me break to tXhantes;\n",
      "Whouldst hough self Tandurs in the discompers;\n",
      "You would uncE's thou bid upon to it dauth\n",
      "That I noble the such of she ad usic'd\n",
      "Make you.\n",
      "\n",
      "SLY:\n",
      "Come, liket un time?\n",
      "\n",
      "COMINIUS:\n",
      "No, body poor:\n",
      "The put it gentle server'd my counse untels, and's the peact ityrim, would to cer for bact?\n",
      "\n",
      "This now onthing these rew--purpisom the few'd.\n",
      "\n",
      "First Service, at dis my lord.\n",
      "\n",
      "GLOUCESTER:\n",
      "But yet of it no might she conve\n",
      "To me more awask, he downs lez the \n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode_sequence_char_rolling(\n",
    "    model, stoi, itos, prompt,\n",
    "    max_new_tokens=100,\n",
    "    block_size=1024,\n",
    "    temperature=1.0,\n",
    "    space_fallback=' ',\n",
    "    strict_window=False,          # if True, periodically re-prime caches on the last block\n",
    "    reprime_every=None            # if strict_window, how often to re-prime (int). Default: block_size\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolling-block generator that:\n",
    "      - keeps the ENTIRE generated text (no trimming of output),\n",
    "      - maintains a rolling block window internally,\n",
    "      - optionally re-primes feature caches on the last `block_size` tokens to strictly\n",
    "        mimic block-window semantics seen during training.\n",
    "\n",
    "    If strict_window=False (default): fastest path; caches stream forever.\n",
    "    If strict_window=True: we periodically reinitialize the per-layer states using the\n",
    "      most recent `block_size` tokens. This ensures exact 'sliding window' behavior.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    B = 1\n",
    "\n",
    "    # ---- encode prompt (fallback to space if empty) ----\n",
    "    space_id = stoi.get(space_fallback, 0)\n",
    "    prompt_ids = encode_chars(prompt, stoi)\n",
    "    if len(prompt_ids) == 0:\n",
    "        prompt_ids = [space_id]\n",
    "\n",
    "    # ---- left-pad ONCE to match your training forward's left-pad-to-block ----\n",
    "    pad_len = max(0, block_size - len(prompt_ids))\n",
    "    pad_ids = [space_id] * pad_len\n",
    "    priming_ids = pad_ids + prompt_ids  # padding only used for priming; not returned\n",
    "\n",
    "    # ---- per-block feature caches (one state per block) ----\n",
    "    feat_states = [\n",
    "        CausalPyramidState(\n",
    "            num_scales=model.config.n_scales,\n",
    "            C=model.config.n_embd,\n",
    "            device=device,\n",
    "            batch_size=B,\n",
    "            tau=1e-6\n",
    "        ) for _ in model.transformer.h\n",
    "    ]\n",
    "\n",
    "    # helper: (re-)prime caches with a sequence of token ids (left-pad to block if shorter)\n",
    "    def _reprime_with_ids(tok_ids):\n",
    "        # optionally left-pad the window up to block_size (only needed if strict semantics desired)\n",
    "        if len(tok_ids) < block_size:\n",
    "            tok_ids = [space_id] * (block_size - len(tok_ids)) + tok_ids\n",
    "        ids_t = torch.tensor([tok_ids], dtype=torch.long, device=device)  # (1, T)\n",
    "        x_last = None\n",
    "        # fresh states\n",
    "        new_states = [\n",
    "            CausalPyramidState(\n",
    "                num_scales=model.config.n_scales,\n",
    "                C=model.config.n_embd,\n",
    "                device=device,\n",
    "                batch_size=B,\n",
    "                tau=1e-6\n",
    "            ) for _ in model.transformer.h\n",
    "        ]\n",
    "        for t in range(ids_t.size(1)):\n",
    "            x_last = model.transformer.wte(ids_t[:, t])  # (1,C)\n",
    "            for blk, st in zip(model.transformer.h, new_states):\n",
    "                x_last = blk.step(x_last, st)\n",
    "        return new_states, x_last\n",
    "\n",
    "    # ---- initial priming with left-padded prompt ----\n",
    "    ids = torch.tensor([priming_ids], dtype=torch.long, device=device)\n",
    "    x_t = None\n",
    "    for t in range(ids.size(1)):\n",
    "        x_t = model.transformer.wte(ids[:, t])  # (1,C)\n",
    "        for blk, st in zip(model.transformer.h, feat_states):\n",
    "            x_t = blk.step(x_t, st)\n",
    "\n",
    "    # ---- FULL output accumulator (never trimmed) ----\n",
    "    out_full = list(prompt_ids)  # store ints\n",
    "\n",
    "    # ---- rolling window buffer of last block_size tokens (prompt + generated) ----\n",
    "    window = deque(prompt_ids, maxlen=block_size)\n",
    "\n",
    "    # strict-window settings\n",
    "    if reprime_every is None:\n",
    "        reprime_every = block_size\n",
    "    steps_since_reprime = 0\n",
    "\n",
    "    # ---- incremental rollout ----\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model.lm_head(x_t)  # (1,V)\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / float(temperature)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "        next_id = int(next_token.item())\n",
    "\n",
    "        # record full output\n",
    "        out_full.append(next_id)\n",
    "\n",
    "        # advance rolling window\n",
    "        window.append(next_id)\n",
    "\n",
    "        # step one token\n",
    "        x_t = model.transformer.wte(next_token.squeeze(-1))  # (1,C)\n",
    "        for blk, st in zip(model.transformer.h, feat_states):\n",
    "            x_t = blk.step(x_t, st)\n",
    "\n",
    "        # optionally re-prime to strict sliding-window semantics\n",
    "        if strict_window:\n",
    "            steps_since_reprime += 1\n",
    "            if steps_since_reprime >= reprime_every and len(window) == block_size:\n",
    "                feat_states, x_t = _reprime_with_ids(list(window))\n",
    "                steps_since_reprime = 0\n",
    "\n",
    "    # decode full continuation (prompt + all generated)\n",
    "    return decode_chars(out_full, itos)\n",
    "    \n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "\n",
    "prompt = \"ROMEO: Juliet, do you love me?  JULIET:\"\n",
    "generated = decode_sequence_char_rolling(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=8024,\n",
    "    block_size=1024,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "5dmTnuUwlmiC",
    "outputId": "54ec00a2-9d54-4db3-c174-0f23ddba4b6a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAGGCAYAAABmGOKbAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAO1lJREFUeJzt3QmYnePdP/B7IgtBErEkQiKoin1JXsHrT22JF62tltQSqlRLrdVSSxpLVdUupFpLq1TE1lqKiLWWIBRBUm1FSppEaASRhZz/9buvnrlmSyQxT+bMzOdzXY/JebbzPOfcZ8z33FtVqVQqJQAAAKDRtWn8UwIAAABB6AYAAICCCN0AAABQEKEbAAAACiJ0AwAAQEGEbgAAACiI0A0AAAAFEboBAACgIEI3AAAAFEToBoAKMXHixFRVVZVuvPHGWusfeOCBtPnmm6dll102b58xY0aTXSMAsHiEbgC+0NVXX53DXv/+/Zv6UipO796982sTS5s2bVKXLl3SJptsko4++ug0ZsyYL33+999/Px1wwAFpueWWS8OGDUs33XRTWn755dPPfvazdPfddy9WmI/ljjvuqLf9pz/9ad42ffr01Jy0hnL5ta99LW288cZNfRkAfAltv8zBALQON998cw6Xzz33XPr73/+evvKVrzT1JVWUqIU+5ZRT8r8/+uij9MYbb6SRI0emX//61+mkk05Kl1xyySKdZ6211kqffvppateuXfW6559/Pp/z3HPPTbvsskv1+gjd3/zmN9Pee++9WNd6zjnnpH333TeH1eZOuQSgOVDTDcBCvfXWW+npp5/OwXHVVVfNQWdpmz9/fpo9e3aqVGussUY65JBD8vK9730vXXHFFemf//xnDsSXXnppuuaaaxZ6/GeffZbmzp2bg3A0IV9mmWWqt02bNi3/jBr0xvhy4JVXXkl33XVXKtInn3ySmnO5jLIWZY7KfO8BmhuhG4CFijCz0korpT322CPXrNYMN/PmzUtdu3ZNRxxxRL3jZs6cmQPkD3/4w+p1c+bMSUOGDMk1kh06dEg9e/ZMP/rRj/L6miJ8Hnfccfm5Ntpoo7xv9GsOv/zlL9O2226bVl555dzkum/fvun222+v9/xRY3z88cenVVZZJa244orpG9/4Rnr33XfzuaM5dU2x/tvf/nbq1q1bfq54zuuvv/5LvW5xbdEUPF6f888/P5VKpVpNveM+LrvssrTuuuvm53z99dfr9emOpsWDBw/O//6f//mfvO3www/PPyPc/Pa3v61uNh7rv8hBBx2UvvrVr+ba7vL1LEzU1sfrG/cSr2N8qRCvVU3xvCussEL6xz/+kXbffff8Wh988MG13sc4z4YbbpjPs80226RXX301b//Vr36Vy0KUk7jXuP/GKJc1Rf/3aG0QNeLxOq+55prpsMMOq25K/9hjj+XrvPXWW9OZZ56Zv0Dp2LFjLr+L+hpMmTIlfwbi3PEcq6++etprr71q3c8LL7yQBg4cmM8R51p77bVzmWsM8UVKvA/rrLNOfi27d++ezx1dE8oeffTRfJ8NfeFyyy235G3PPPNM9brx48fn1zXKb5yzX79+6U9/+lOt46KcxnGPP/54+v73v59WW221/BqEaJ1x4oknVr/usW3XXXdNL774YqPcM0Bzonk5AAsVYSaaI7dv3z4NGjQo19pGk+cIgdEMep999kl33nlnDlCxT1n0N44wHUEvRM1hBN+//OUvub/zBhtskMNX1AT/7W9/q9c/+ZFHHkm33XZbDm0RVOKP93D55Zfn80Swi9rhCEv7779/uvfee3MAK4sQEscfeuihaeutt87BoOb2sqlTp+bt5YAYtaZ//vOf05FHHpmDVwSHJRVhNF6f6667LofqCPNlN9xwQ65RjdciQkmEm7q1q2eccUZaf/3107XXXpuDcgS1COnRzPw73/lO2mqrrfLxIdZ/kahBj2AZoTPCV7yvCxKBKoJkvM8XXHBBfp3itX/qqafSSy+9VKvmPWrqI1But912+cuECK1lTz75ZA5rxx57bH4c59pzzz3zly3RJzvC2n/+85/0i1/8IgfFeN+/bLks+/jjj9P/+3//Lzf3j3NvueWWOWzH9bzzzju5XJVF8/04V3xJFOU2/r2or8F+++2XXnvttfSDH/wgl9NonTBq1Kg0adKk6scDBgzIZeu0007Lx0Ugj89NY4jnipYVca0RuONaoszEz2effTaX7fhSI77kitctymTd1zLKT3whEuK4//3f/81fQMT1xhgC8VmKlhsxJkDd4+M9jHs7++yzq2u6jznmmPxlWHym4guX+AIgPvvxXsT7ANCqlABgAV544YWoDi2NGjUqP54/f35pzTXXLJ1wwgnV+zz44IN5n3vuuafWsbvvvntpnXXWqX580003ldq0aVN68skna+03fPjwfPxTTz1VvS4ex76vvfZavWuaNWtWrcdz584tbbzxxqWddtqpet3YsWPzOU488cRa+x5++OF5/ZAhQ6rXHXnkkaXVV1+9NH369Fr7HnTQQaXOnTvXe7661lprrdIee+yxwO2XXnppfs4//vGP+fFbb72VH3fq1Kk0bdq0WvuWt91www3V6+Lfse7555+vte/yyy9fGjx48EKvre55L7rootJnn31WWm+99UqbbbZZfj9DvB6x/b333qt+TVdbbbX8un766afV57n33nvzfmeffXb1uriGWHfaaafVe95Y36FDh/z8Zb/61a/y+u7du5dmzpxZvf7000/P62vu+2XKZYjrjP3uvPPOeuco3/ujjz6a94myWvO9XtTX4D//+U/1a7sgd911V4Pv4aLYYYcdShtttNFC92mojP7hD3/Iz/nEE0/Ueo3j/ZgxY0b1uiiDbdu2rfWZ2HnnnUubbLJJafbs2bVer2233TaXnbplc7vttsvlqqb47Bx77LGLfb8ALZHm5QAsUNSARZPrHXfcMT+OGrMDDzww1y5//vnned1OO+2UawxHjBhRfVzUXEbtW+xbFs10o3a7T58+ubaxvMTx5eavNe2www65hqyuaJpb83k+/PDDXJtZs9lquSl61MDVFDWRNUUujJq7r3/96/nfNa8ram7j3F+2OWzUdpeb29YUtaNRO7i0lWu7X3755QWOfh5NoaN2Nl6/aFpcFi0F4v2777776h0TfdkbsvPOO1e3Ugjlkcbj/qMpet31UWPbGOUyxHu72Wab1auZLR9TUzTjr1m2FvU1iGOiVjyaqUd5bEi5RjxaY0SXjMZW87qj9USU32i9EWqW32jhELX4NbtjxOc2WipEs/nwwQcf5NYGMWJ+lNny5yFqquMz8eabb9ZrXn/UUUfVGoegfM8xev/kyZMb/X4BmhuhG4AGRXiJEBPBJgatitGhY4lwFM1sR48enfdr27ZtDlB//OMfq/tmR7PZCBc1Q3f8sR7NViNo1lyij3HNAcPKoil1QyK4RKCIIBRNsuMc0bQ4AnLZ22+/nafvqnuOuqNbv/fee7nPbzTFrXtd5X7qda9rcUUT51AzYC7s/paGaJofr8WC+nbH6xeiaXtdETjL28uiDJT78tbVq1evWo87d+6cf0ZT54bWLyi4Lm65DNHPfFGn26r7fizqaxBdAy688MLcJSG+CNh+++1zU/no513zC6T4jAwdOjR/QRX9vaN7Qd2xDJZUBOUTTjghP38E8Ci/5fup+bmI646m8jX7v8e/4/NU/mzEaxll4qyzzqr3mYjxGBb1sxqvwbhx4/L7HN0gYhyFRflCBaAl0qcbgAZFbde///3vHHBiqSv+WI9+qiH6bUef7gge0e8z+n/GH/hRy1gW/ZVj/uoFTZ9VN4TVrL2r2T84+nNHsIn+wDFgVfQrjwATg0EtrnIf6qjlKw9YVtemm26avowIHg0F/obub2nXdke/9/iy5MuK4BlfcizouRZn/RcN8LY45XJxfJn3I/r9R2uJaDnw4IMP5sAafcDjWrfYYotcqx61y9G/+p577sn7RB/ziy++OK8rt4ZYUlErHSO5n3rqqXmE+jhflO3ddtut3jgBUdsdAT36tEfoj+e/6qqrqreX94++7VGz3ZBFKctxTdECJcYOeOihh9JFF12Uv5yIL+T+7//+70vdL0BzI3QD0KAILzHi8LBhw+ptiz+c44/p4cOH5z+4IwRHAI6mqjGYVoSNGASsphioKZo0R3PjJZ0jOpoLRw13hJYIemURuuvOdx3hIWpC11tvver1UYtXU9TeRQ101J7WnAO7sUQtd7xO8YVCNK1vTF92nu34ouG8887Lta/xRUbd1y9MmDChuvl/Wawrb6/0chllrvylx+Ja3Ncgnivmao8lWnVE+I1Q/fvf/756n6hRjiVGs48viaLFQXxxEIPiLaloGRC1+/E+xkBmZXENDYkvyE4++eT0hz/8oXpO+JotUmIE9BDrv+xnIn4nRPP8WKJ2PAZQi3sXuoHWRvNyAOqJP8YjwMQo0zFtUN0lRiSO/p7lKYSiljPWRy1eTJMVfURr/iFfrvmKvqC//vWvG3y+RZnfN2pHI2zW7Lcbo0DX7ZtcrqGL2vCarrzyynrni2a/EeYbCmfR/HxJxT3FyOnR9De+gPiyIbmuGFE6msZ/2druv/71r/WmgorpoSLYRnit2QQ6WjLE6NMNjQJfieUy3tv4oqehabK+qEZ9UV+DWbNm1ZtDPgJ4fJlTPi6Ccd3ni1AevmwT83KLgbrnj+noGhLN2yP0xpcB8QVG1IbXHMU97jlGOo+WK9GiYEk+E/H5rNmsvXzeHj16NFqTeoDmRE03APVEaInwUrcGtCxq66KWOP5oL4fr+BmhNvp9RjPyujW7EUCj2XlMJRSDpsWURPHHecwHHOuj9jqCzsJE0Inm6REUvvWtb+Xas6jxjOauMVdxWcyrHIErgkcMAFWeMiymJgs1A/DPf/7zfD3RJzgGhIrB2yIoxwBUDz/8cP73F4kvE8o1mlG7HdODxcBx0a83aj6/+93vpsYW9xjXF69HhJnoV1sejGxRRU1rTJUVwbumqOWMpsDRrz36I8eUXOXpsmJQtJj3ujmUy2huHc26Y0q5aM4dr1m8n3GeCNM1uz/UtaivQZSpaL0RXypF2Yn+7RHyY9/ydHkxn3p8ARQDukUgj3uIL586deqU5zb/IhF0o1VCXfGex3tY7kce4yjENF/RnDtaeSxINDGPLylCvP91xWcqWqzE5zg+E1H7HfcT83hHs/T4ImNh4v6ij388R7zG0dw9ympM6Ra1/wCtTlMPnw5A5fn6179eWnbZZUuffPLJAveJ6bfatWtXPdVWTCnUs2fPPIXQeeed1+AxMQ3ThRdemKdAiqmLVlpppVLfvn1LQ4cOLX344YfV+8U5FjTd0HXXXZenLYrj+/Tpk6ctKk95VVNce5yja9eupRVWWKG09957lyZMmJD3+/nPf15r36lTp+Z94/rjnmI6q5g26dprr/3C1yqmDItzxlJVVZWnAov7O+qoo0pjxoxZ6PRdC9q2KFOGjR8/vrT99tuXlltuubx9YdOHLew5y+evOWVY2YgRI0pbbLFFfq3jdTz44INL77zzTq194nlj+rKGNPQ+LuhaylN3jRw5slHL5fvvv1867rjjSmussUapffv2eWqxuOby9i963i96DeI8cY9RFuN1iKmy+vfvX7rtttuq93nxxRdLgwYNKvXq1SufJ6Yi23PPPfPUZ4syZVj5/am7RBkNcT377LNPqUuXLvn5999//9LkyZPrTY9XNmfOnPzZi31rTodW0z/+8Y/SYYcdlj8L8XrG6xfXfPvtt39h2Yzzn3rqqXlauhVXXDG/LvHvq6+++gvvF6Alqor/NHXwB4ClIWp0Y2CrqJWOGkJojaL7R7SOiMHfrrvuuqa+HIAWT59uAFqk6P9bVzQ3j/7n0RwXWqsYAyGarEczcwCKp083AC1S9HEdO3Zsns85+tnGAFixHH300fWmJ4PWYMyYMXnsg+jHHS0+oq86AMXTvByAFmnUqFF5GqUY1CwGN+vVq1cezC1GEo8QDq1NzMseXSti5PQbb7wxbbzxxk19SQCtgtANAAAABdGnGwAAAAoidAMAAEBBdGprBPPnz0+TJ09OK664YqqqqmrqywEAAKBg0VP7o48+ytMwxuwoCyJ0N4II3EbCBQAAaH3+9a9/pTXXXHOB24XuRhA13OUXu1OnTk19ORRk3rx56aGHHkoDBgxI7dq1a+rLoZVTHqkUyiKVQlmkkiiPrcPMmTNz5Ws5Dy6I0N0Iyk3KI3AL3S37l2fHjh3ze+yXJ01NeaRSKItUCmWRSqI8ti5VX9DF2EBqAAAAUBChGwAAAAoidAMAAEBBhG4AAAAoiNANAAAABRG6AQAAoCBCNwAAABRE6AYAAICCCN0AAABQEKEbAAAACiJ0AwAAQEGEbgAAACiI0A0AAAAFEboBAACgIEI3AAAAFEToBgAAgIII3QAAAFAQoRsAAAAKInQDAABAQYRuAAAAKIjQDQAAAAURugEAAKAgQjcAAAAUROgGAACAggjdAAAAUBChGwAAAAoidAMAAEBBhG4AAAAoiNANAAAABRG6AQAAoCBCNwAAABRE6AYAAICCCN0AAABQEKEbAAAACtIsQ/ewYcNS796907LLLpv69++fnnvuuYXuP3LkyNSnT5+8/yabbJLuv//+Be57zDHHpKqqqnTZZZcVcOUAAAC0Js0udI8YMSKdfPLJaciQIenFF19Mm222WRo4cGCaNm1ag/s//fTTadCgQenII49ML730Utp7773zMm7cuHr73nXXXenZZ59NPXr0WAp3AgAAQEvX7EL3JZdcko466qh0xBFHpA033DANHz48dezYMV1//fUN7n/55Zen3XbbLZ166qlpgw02SOeee27acsst01VXXVVrv3fffTf94Ac/SDfffHNq167dUrobAAAAWrJmFbrnzp2bxo4dm3bZZZfqdW3atMmPn3nmmQaPifU19w9RM15z//nz56dDDz00B/ONNtqowDsAAACgNWmbmpHp06enzz//PHXr1q3W+ng8fvz4Bo+ZMmVKg/vH+rILL7wwtW3bNh1//PGLdB1z5szJS9nMmTPzz3nz5uWFlqn83nqPqQTKI5VCWaRSKItUEuWxdZi3iO9vswrdRYia82iCHv3DYwC1RXHBBRekoUOH1lv/0EMP5abutGyjRo1q6kuAasojlUJZpFIoi1QS5bFlmzVrVssL3ausskpaZpll0tSpU2utj8fdu3dv8JhYv7D9n3zyyTwIW69evaq3R236KaeckkcwnzhxYr1znn766Xkwt5o13T179kwDBgxInTp1+tL3SeV+kxW/OHfddVf9/mlyyiOVQlmkUiiLVBLlsXWY+d8Wzy0qdLdv3z717ds3jR49Oo9AXu6PHY+PO+64Bo/ZZptt8vYTTzyxel18AGJ9iL7cDfX5jvUxWFtDOnTokJe64gPlQ9XyeZ+pJMojlUJZpFIoi1QS5bFlW9T3tlmF7hA1zIMHD079+vVLW221Va6N/uSTT6oD8mGHHZbWWGON3AQ8nHDCCWmHHXZIF198cdpjjz3Srbfeml544YV07bXX5u0rr7xyXuq+eFETvv766zfBHQIAANBSNLvQfeCBB6b33nsvnX322XkwtM033zw98MAD1YOlTZo0KY9oXrbtttumW265JZ155pnpJz/5SVpvvfXS3XffnTbeeOMmvAsAAABag2YXukM0JV9Qc/LHHnus3rr9998/L4uqoX7cAAAA0KLn6QYAAIDmROgGAACAggjdAAAAUBChGwAAAAoidAMAAEBBhG4AAAAoiNANAAAABRG6AQAAoCBCNwAAABRE6AYAAICCCN0AAABQEKEbAAAACiJ0AwAAQEGEbgAAACiI0A0AAAAFEboBAACgIEI3AAAAFEToBgAAgIII3QAAAFAQoRsAAAAKInQDAABAQYRuAAAAKIjQDQAAAAURugEAAKAgQjcAAAAUROgGAACAggjdAAAAUBChGwAAAAoidAMAAEBBhG4AAAAoiNANAAAABRG6AQAAoCBCNwAAABRE6AYAAICCCN0AAABQEKEbAAAACiJ0AwAAQEGEbgAAACiI0A0AAAAFEboBAACgIEI3AAAAFEToBgAAgIII3QAAAFAQoRsAAAAKInQDAABAQZpl6B42bFjq3bt3WnbZZVP//v3Tc889t9D9R44cmfr06ZP332STTdL9999fvW3evHnpxz/+cV6//PLLpx49eqTDDjssTZ48eSncCQAAAC1ZswvdI0aMSCeffHIaMmRIevHFF9Nmm22WBg4cmKZNm9bg/k8//XQaNGhQOvLII9NLL72U9t5777yMGzcub581a1Y+z1lnnZV/3nnnnWnChAnpG9/4xlK+MwAAAFqaZhe6L7nkknTUUUelI444Im244YZp+PDhqWPHjun6669vcP/LL7887bbbbunUU09NG2ywQTr33HPTlltuma666qq8vXPnzmnUqFHpgAMOSOuvv37aeuut87axY8emSZMmLeW7AwAAoCVpm5qRuXPn5jB8+umnV69r06ZN2mWXXdIzzzzT4DGxPmrGa4qa8bvvvnuBz/Phhx+mqqqq1KVLlwa3z5kzJy9lM2fOrG6qHgstU/m99R5TCZRHKoWySKVQFqkkymPrMG8R399mFbqnT5+ePv/889StW7da6+Px+PHjGzxmypQpDe4f6xsye/bs3Mc7mqR36tSpwX0uuOCCNHTo0HrrH3rooVzrTssWLSOgUiiPVAplkUqhLFJJlMeWLboqt7jQvTS+qYhm5qVSKV1zzTUL3C9q2mvWnkdNd8+ePdOAAQMWGNRpGeUjfnHuuuuuqV27dk19ObRyyiOVQlmkUiiLVBLlsXWY+d8Wzy0qdK+yyippmWWWSVOnTq21Ph537969wWNi/aLsXw7cb7/9dnrkkUcWGp47dOiQl7riA+VD1fJ5n6kkyiOVQlmkUiiLVBLlsWVb1Pe2WQ2k1r59+9S3b980evTo6nXz58/Pj7fZZpsGj4n1NfcP8a1Tzf3LgfvNN99MDz/8cFp55ZULvAsAAABai2ZV0x2iWffgwYNTv3790lZbbZUuu+yy9Mknn+TRzEPMsb3GGmvkftfhhBNOSDvssEO6+OKL0x577JFuvfXW9MILL6Rrr722OnB/85vfzNOF3XvvvbnPeLm/d9euXXPQBwAAgFYRug888MD03nvvpbPPPjuH48033zw98MAD1YOlxTRfMaJ52bbbbptuueWWdOaZZ6af/OQnab311ssjl2+88cZ5+7vvvpv+9Kc/5X/HuWp69NFH09e+9rWlen8AAAC0HM0udIfjjjsuLw157LHH6q3bf//989KQ3r1754HTAAAAoLE1qz7dAAAA0JwI3QAAAFAQoRsAAAAKInQDAABAQYRuAAAAKIjQDQAAAAURugEAAKAgQjcAAAAUROgGAACAggjdAAAAUBChGwAAAAoidAMAAEBB2i7JQaNHj87LtGnT0vz582ttu/766xvr2gAAAKBZW+zQPXTo0HTOOeekfv36pdVXXz1VVVUVc2UAAADQ2kL38OHD04033pgOPfTQYq4IAAAAWmuf7rlz56Ztt922mKsBAACA1hy6v/Od76RbbrmlmKsBAACA1ty8fPbs2enaa69NDz/8cNp0001Tu3btam2/5JJLGvP6AAAAoPWE7ldeeSVtvvnm+d/jxo2rtc2gagAAAPAlQvejjz66uIcAAABAq7TYfboBAACAgmq6wwsvvJBuu+22NGnSpDyaeU133nnnkpwSAAAAWpzFrum+9dZb85Rhb7zxRrrrrrvSvHnz0muvvZYeeeSR1Llz52KuEgAAAFpD6P7Zz36WLr300nTPPfek9u3bp8svvzyNHz8+HXDAAalXr17FXCUAAAC0htD9j3/8I+2xxx753xG6P/nkkzxq+UknnZSnEgMAAACWMHSvtNJK6aOPPsr/XmONNaqnDZsxY0aaNWvW4p4OAAAAWqzFHkht++23T6NGjUqbbLJJ2n///dMJJ5yQ+3PHup133rmYqwQAAIDWELqvuuqqNHv27PzvM844I7Vr1y49/fTTab/99ktnnnlmEdcIAAAArSN0d+3atfrfbdq0SaeddlpjXxMAAAC0zj7d5cHUolZ70KBBadq0aXndn//85zx1GAAAALCEofvxxx/P/bnHjBmT7rzzzvTxxx/n9S+//HIaMmTI4p4OAAAAWqzFDt3RnPy8887LA6fFlGFlO+20U3r22Wcb+/oAAACg9YTuV199Ne2zzz711q+22mpp+vTpjXVdAAAA0PpCd5cuXdK///3veutfeumlPG83AAAAsISh+6CDDko//vGP05QpU1JVVVWaP39+euqpp9IPf/jDdNhhhy3u6QAAAKDFWuzQ/bOf/Sz16dMn9ezZMw+ituGGG6btt98+bbvttubpBgAAgC8zT3cMnvbrX/86nXXWWWncuHE5eG+xxRZpvfXWW9xTAQAAQIu22KG7rFevXnkBAAAAGil0l0qldPvtt6dHH300TZs2Lffprinm7gYAAACWIHSfeOKJ6Ve/+lXacccdU7du3fJgagAAAEAjhO6bbrop12bvvvvui3soAAAAtCqLPXp5586d0zrrrFPM1QAAAEBrDt0//elP09ChQ9Onn35azBUBAABAa21efsABB6Q//OEPabXVVku9e/dO7dq1q7X9xRdfbMzrAwAAgNZT0z148OA0duzYdMghh6T99tsv7bXXXrWWpWHYsGE58C+77LKpf//+6bnnnlvo/iNHjkx9+vTJ+2+yySbp/vvvrzci+9lnn51WX331tNxyy6VddtklvfnmmwXfBQAAAC3dYtd033fffenBBx9M2223XWoKI0aMSCeffHIaPnx4DtyXXXZZGjhwYJowYUKufa/r6aefToMGDUoXXHBB2nPPPdMtt9yS9t5771wjv/HGG+d9fvGLX6Qrrrgi/fa3v01rr712Ouuss/I5X3/99RzUAQAAYKnUdPfs2TN16tQpNZVLLrkkHXXUUemII45IG264YQ7fHTt2TNdff32D+19++eVpt912S6eeemraYIMN0rnnnpu23HLLdNVVV1XXckdwP/PMM3NN/aabbpp+97vfpcmTJ6e77757Kd8dAAAArTp0X3zxxelHP/pRmjhxYlra5s6dm5u2R/PvsjZt2uTHzzzzTIPHxPqa+4eoxS7v/9Zbb6UpU6bU2idGaI9a9AWdEwAAAAppXh59uWfNmpXWXXfdXMNcdyC1Dz74IBVl+vTp6fPPP0/dunWrtT4ejx8/vsFjIlA3tH+sL28vr1vQPnXNmTMnL2UzZ87MP+fNm5cXWqbye+s9phIoj1QKZZFKoSxSSZTH1mHeIr6/ix26oyl2axf9w2PatLoeeuih/EUELduoUaOa+hKgmvJIpVAWqRTKIpVEeWzZojK6kNAdo5c3lVVWWSUts8wyaerUqbXWx+Pu3bs3eEysX9j+5Z+xLkYvr7nP5ptv3uA5Tz/99DyYW82a7ujrPmDAgCbt707x32TFL85dd921XgsPWNqURyqFskilUBapJMpj6zDzvy2eGz10N6X27dunvn37ptGjR+cRyMP8+fPz4+OOO67BY7bZZpu8/cQTT6xeFx+AWB9itPII3rFPOWTHizdmzJj0ve99r8FzdujQIS91xQfKh6rl8z5TSZRHKoWySKVQFqkkymPLtqjvbbMK3SFqmKO2vV+/fmmrrbbKzd0/+eSTPJp5OOyww9Iaa6yRm4CHE044Ie2www55ALg99tgj3XrrremFF15I1157bd5eVVWVA/l5552X1ltvveopw3r06FEd7AEAAGBJNLvQfeCBB6b33nsvnX322Xmgs6idfuCBB6oHQps0aVIe0bxs2223zXNzx5RgP/nJT3KwjqnAynN0hxiNPYL70UcfnWbMmJHnII9zmqMbAACAVhW6QzQlX1Bz8scee6zeuv333z8vCxK13eecc05eAAAAoMnm6QYAAAAasab7iSeeSEuid+/eqVevXkt0LAAAALSK0L2k04SddNJJ6fjjj1+iYwEAAKBVhO633nqr+CsBAACA1tinO0YDX2aZZRZ7MTAZAAAArVmhNd1dunRZouMAAACg1YTutdZaq/grAQAAgBbGlGEAAABQKaF76tSp6dBDD009evRIbdu2rdePGwAAAFiM5uU1HX744WnSpEnprLPOSquvvnqqqqpa3FMAAABAq7DYofsvf/lLevLJJ9Pmm29ezBUBAABAa21e3rNnz1QqlYq5GgAAAGjNofuyyy5Lp512Wpo4cWIxVwQAAACtqXn5SiutVKvv9ieffJLWXXfd1LFjx9SuXbta+37wwQeNf5UAAADQUkN31G4DAAAABYTuwYMHL9LJ1HIDAADAl+jT3ZCHHnooHXjggWnNNddsjNMBAABA6w7db7/9dhoyZEjq3bt3+vrXv54fz5kzp3GvDgAAAFpL6J47d2669dZb0y677JK+8pWvpIcffjj96Ec/Su+++276zW9+U9xVAgAAQEvt0x1+8IMfpFtuuSV169YtHXzwwenXv/51Wnvttau3T506tahrBAAAgJYduocNG5YOO+ywdOGFF+bgDQAAADRS8/Kbb745NyPv1atXGjhwYLrpppvSxx9/vKiHAwAAQKuzyKF70KBBadSoUWn8+PGpf//+6cwzz8w13gcddFC65557cn9vAAAA4EuMXh79uM8555w0ceLEdOedd6ZSqZT233//tN122y3uqQAAAKBFW+Ipw6qqqnIz8xEjRqTJkyen888/P2288caNe3UAAADQGkN3TV27dk0nnnhievnllxvjdAAAANB6Ri+fNGnSEp28S5cuqVOnTkt0LAAAALSK0N27d+8lan4+ZMiQdPbZZy/JdQEAAEDrCN3z588v/koAAACgNYbuGLE8aq4XV/TzPv7445fkugAAAKB1hO4bb7xxiU6+JM3SAQAAoFWF7h122KH4KwEAAIAWplGmDAMAAADqE7oBAACgIEI3AAAAFEToBgAAgIII3QAAAFAQoRsAAAAKInQDAABAQYRuAAAAKIjQDQAAAAURugEAAKAgQjcAAAAUROgGAACAgjSr0P3BBx+kgw8+OHXq1Cl16dIlHXnkkenjjz9e6DGzZ89Oxx57bFp55ZXTCiuskPbbb780derU6u0vv/xyGjRoUOrZs2dabrnl0gYbbJAuv/zypXA3AAAAtHTNKnRH4H7ttdfSqFGj0r333pueeOKJdPTRRy/0mJNOOindc889aeTIkenxxx9PkydPTvvuu2/19rFjx6bVVlst/f73v8/nPuOMM9Lpp5+errrqqqVwRwAAALRkbVMz8cYbb6QHHnggPf/886lfv3553ZVXXpl233339Mtf/jL16NGj3jEffvhhuu6669Itt9ySdtppp7zuhhtuyLXZzz77bNp6663Tt7/97VrHrLPOOumZZ55Jd955ZzruuOOW0t0BAADQEjWbmu4IwtGkvBy4wy677JLatGmTxowZ0+AxUYs9b968vF9Znz59Uq9evfL5FiTCeteuXRv5DgAAAGhtmk1N95QpU3Iz8Jratm2bw3FsW9Ax7du3z2G9pm7dui3wmKeffjqNGDEi3XfffQu8ljlz5uSlbObMmflnBPxYaJnK7633mEqgPFIplEUqhbJIJVEeW4d5i/j+NnnoPu2009KFF174hU3Ll4Zx48alvfbaKw0ZMiQNGDBggftdcMEFaejQofXWP/TQQ6ljx44FXyVNLcYUgEqhPFIplEUqhbJIJVEeW7ZZs2Y1j9B9yimnpMMPP3yh+0Q/6+7du6dp06bVWv/ZZ5/lEc1jW0Ni/dy5c9OMGTNq1XbH6OV1j3n99dfTzjvvnAdmO/PMMxd6PTHQ2sknn1yrpjtGP4+gHiOr03K/yYpfnLvuumtq165dU18OrZzySKVQFqkUyiKVRHlsHWb+t8VzxYfuVVddNS9fZJtttsnhOfpp9+3bN6975JFH0vz581P//v0bPCb2i0I+evToPFVYmDBhQpo0aVI+X1mMWh4DrQ0ePDidf/75X3gtHTp0yEtd8Vw+VC2f95lKojxSKZRFKoWySCVRHlu2RX1vm81AajHi+G677ZaOOuqo9Nxzz6Wnnnoqjy5+0EEHVY9c/u677+aB0mJ76Ny5c57LO2qlH3300RzYjzjiiBy4Y+TycpPyHXfcMddSx37R1zuW9957r0nvFwAAgOavyWu6F8fNN9+cg3Y0A49Ry6P2+oorrqjVjCNqsmu2rb/00kur943BzwYOHJiuvvrq6u233357DtgxT3csZWuttVaaOHHiUrw7AAAAWppmFbpjpPKYc3tBevfunUqlUq11yy67bBo2bFheGvLTn/40LwAAANDYmk3zcgAAAGhuhG4AAAAoiNANAAAABRG6AQAAoCBCNwAAABRE6AYAAICCCN0AAABQEKEbAAAACiJ0AwAAQEGEbgAAACiI0A0AAAAFEboBAACgIEI3AAAAFEToBgAAgIII3QAAAFAQoRsAAAAKInQDAABAQYRuAAAAKIjQDQAAAAURugEAAKAgQjcAAAAUROgGAACAggjdAAAAUBChGwAAAAoidAMAAEBBhG4AAAAoiNANAAAABRG6AQAAoCBCNwAAABRE6AYAAICCCN0AAABQEKEbAAAACiJ0AwAAQEGEbgAAACiI0A0AAAAFEboBAACgIEI3AAAAFEToBgAAgIII3QAAAFAQoRsAAAAKInQDAABAQYRuAAAAKIjQDQAAAAURugEAAKAgQjcAAAAUpFmF7g8++CAdfPDBqVOnTqlLly7pyCOPTB9//PFCj5k9e3Y69thj08orr5xWWGGFtN9++6WpU6c2uO/777+f1lxzzVRVVZVmzJhR0F0AAADQWjSr0B2B+7XXXkujRo1K9957b3riiSfS0UcfvdBjTjrppHTPPfekkSNHpscffzxNnjw57bvvvg3uGyF+0003LejqAQAAaG2aTeh+44030gMPPJB+85vfpP79+6ftttsuXXnllenWW2/NQbohH374YbruuuvSJZdcknbaaafUt2/fdMMNN6Snn346Pfvss7X2veaaa3Lt9g9/+MOldEcAAAC0dG1TM/HMM8/kJuX9+vWrXrfLLrukNm3apDFjxqR99tmn3jFjx45N8+bNy/uV9enTJ/Xq1Sufb+utt87rXn/99XTOOefk8/zzn//8wmuZM2dOXspmzpyZf8ZzxULLVH5vvcdUAuWRSqEsUimURSqJ8tg6zFvE97fZhO4pU6ak1VZbrda6tm3bpq5du+ZtCzqmffv2OazX1K1bt+pjIjwPGjQoXXTRRTmML0rovuCCC9LQoUPrrX/ooYdSx44dF/POaG6iewNUCuWRSqEsUimURSqJ8tiyzZo1q3mE7tNOOy1deOGFX9i0vCinn3562mCDDdIhhxyyWMecfPLJtWq6e/bsmQYMGJAHeaPlfpMVvzh33XXX1K5du6a+HFo55ZFKoSxSKZRFKony2DrM/G+L54oP3aeccko6/PDDF7rPOuusk7p3756mTZtWa/1nn32WRzSPbQ2J9XPnzs19tWvWdsfo5eVjHnnkkfTqq6+m22+/PT8ulUr55yqrrJLOOOOMBmu0O3TokJe64gPlQ9XyeZ+pJMojlUJZpFIoi1QS5bFlW9T3tslD96qrrpqXL7LNNtvk8Bz9tGNAtHJgnj9/fh5YrSGxX7wQo0ePzlOFhQkTJqRJkybl84U77rgjffrpp9XHPP/88+nb3/52evLJJ9O6667bSHcJAABAa9TkoXtRRRPw3XbbLR111FFp+PDhucnGcccdlw466KDUo0ePvM+7776bdt555/S73/0ubbXVVqlz5855GrBoCh59v6Pp9w9+8IMcuMuDqNUN1tOnT69+vrp9wQEAAKBFhu5w880356AdwTpGLY/a6yuuuKJ6ewTxqMmu2aH90ksvrd43Bk0bOHBguvrqq5voDgAAAGhNmlXojtrqW265ZYHbe/fuXd0nu2zZZZdNw4YNy8ui+NrXvlbvHAAAALAk2izRUQAAAMAXEroBAACgIEI3AAAAFEToBgAAgIII3QAAAFAQoRsAAAAKInQDAABAQYRuAAAAKIjQDQAAAAURugEAAKAgQjcAAAAUROgGAACAggjdAAAAUBChGwAAAAoidAMAAEBBhG4AAAAoiNANAAAABRG6AQAAoCBCNwAAABRE6AYAAICCCN0AAABQEKEbAAAACiJ0AwAAQEGEbgAAACiI0A0AAAAFEboBAACgIEI3AAAAFEToBgAAgIII3QAAAFAQoRsAAAAKInQDAABAQYRuAAAAKIjQDQAAAAURugEAAKAgbYs6cWtSKpXyz5kzZzb1pVCgefPmpVmzZuX3uV27dk19ObRyyiOVQlmkUiiLVBLlsXWY+d/8V86DCyJ0N4KPPvoo/+zZs2dTXwoAAABLOQ927tx5gdurSl8Uy/lC8+fPT5MnT04rrrhiqqqqaurLocBvsuKLlX/961+pU6dOTX05tHLKI5VCWaRSKItUEuWxdSiVSjlw9+jRI7Vps+Ce22q6G0G8wGuuuWZTXwZLSfzi9MuTSqE8UimURSqFskglUR5bvoXVcJcZSA0AAAAKInQDAABAQYRuWEQdOnRIQ4YMyT+hqSmPVAplkUqhLFJJlEdqMpAaAAAAFERNNwAAABRE6AYAAICCCN0AAABQEKEbavjggw/SwQcfnOdT7NKlSzryyCPTxx9/vNBjZs+enY499ti08sorpxVWWCHtt99+aerUqQ3u+/777+c53auqqtKMGTMKugtagiLK4ssvv5wGDRqUevbsmZZbbrm0wQYbpMsvv3wp3A3NzbBhw1Lv3r3Tsssum/r375+ee+65he4/cuTI1KdPn7z/Jptsku6///5a22P4mLPPPjutvvrqueztsssu6c033yz4LmgJGrMszps3L/34xz/O65dffvnUo0ePdNhhh6XJkycvhTuhuWvs34s1HXPMMflvw8suu6yAK6cSCN1QQ4Sc1157LY0aNSrde++96YknnkhHH330Qo856aST0j333JN/uT7++OP5f9777rtvg/tGcNp0000LunpakiLK4tixY9Nqq62Wfv/73+dzn3HGGen0009PV1111VK4I5qLESNGpJNPPjmPuvviiy+mzTbbLA0cODBNmzatwf2ffvrp/GVO/H576aWX0t57752XcePGVe/zi1/8Il1xxRVp+PDhacyYMTnwxDnjiyJYWmVx1qxZ+TxnnXVW/nnnnXemCRMmpG984xtL+c5obor4vVh21113pWeffTZ/CUQLFqOXA6XS66+/HiP5l55//vnqdX/+859LVVVVpXfffbfBY2bMmFFq165daeTIkdXr3njjjXyeZ555pta+V199dWmHHXYojR49Om//z3/+U+Dd0JwVXRZr+v73v1/acccdG/kOaM622mqr0rHHHlv9+PPPPy/16NGjdMEFFzS4/wEHHFDaY489aq3r379/6bvf/W7+9/z580vdu3cvXXTRRbXKa4cOHUp/+MMfCrsPmr/GLosNee655/LvybfffrsRr5yWpqiy+M4775TWWGON0rhx40prrbVW6dJLLy3oDmhqarrhv5555pncjLdfv37V66IJZJs2bXLNTEOi5jCaq8V+ZdGUqFevXvl8Za+//no655xz0u9+97t8PmiqsljXhx9+mLp27drId0BzNXfu3FyWapajKHfxeEHlKNbX3D9EDVB5/7feeitNmTKl1j6dO3fOzTMXVjZp3Yooiwv6HRjNeuN3LizNsjh//vx06KGHplNPPTVttNFGBd4BlcBf//Bf8UdhNL2tqW3btjmQxLYFHdO+fft6/7Pu1q1b9TFz5szJTYwuuuiiHICgqcpiQ83fosncFzVbp/WYPn16+vzzz3O5WdRyFOsXtn/55+KcE4ooi3VF94bo4x3/j47xM2BplsULL7ww/7/9+OOPL+jKqSRCNy3eaaedlr/FXtgyfvz4wp4/+szGgFWHHHJIYc9B89DUZbGm6Fe211575f5pAwYMWCrPCVApomXQAQcckAf5u+aaa5r6cmhlouY8BjK98cYb8//7afnaNvUFQNFOOeWUdPjhhy90n3XWWSd179693oAYn332WR5FOrY1JNZHs6MYibxmDWOMGF0+5pFHHkmvvvpquv322/Pj+B98WGWVVfJAVkOHDv3S90jz0NRlsWZ3h5133jnXcJ955plf6p5oWeL30jLLLFNvBoaGylFZrF/Y/uWfsS5GL6+5z+abb17AXdASFFEW6wbut99+O/8/Wi03S7ssPvnkk/n/8zVbQEZtevydECOYT5w4sZB7oemo6abFW3XVVXPf1oUt0Sx3m222yYElvn0si/8ZR5+b6HvYkL59+6Z27dql0aNHV6+LkVAnTZqUzxfuuOOOPFXTX//617z85je/qf6FG9M70Xo0dVkMMWr5jjvumAYPHpzOP//8gu+Y5ibKX5SlmuUoyl08rlmOaor1NfcPMep+ef+11147/6FZc5+ZM2fm8QkWdE4ooizWDNwxZd3DDz+cp1iEpV0Woy/3K6+8Uv23YSwxenn0737wwQcLviOaRFOP5AaVZLfddittscUWpTFjxpT+8pe/lNZbb73SoEGDao0yuf766+ftZcccc0ypV69epUceeaT0wgsvlLbZZpu8LMijjz5q9HKapCy++uqrpVVXXbV0yCGHlP79739XL9OmTVvq90fluvXWW/PI4jfeeGMeSf/oo48udenSpTRlypS8/dBDDy2ddtpp1fs/9dRTpbZt25Z++ctf5hHzhwwZkkfSj/JW9vOf/zyf449//GPplVdeKe21116ltddeu/Tpp582yT3SOsvi3LlzS9/4xjdKa665Zumvf/1rrd+Dc+bMabL7pHX+XqzL6OUtm9ANNbz//vs52KywwgqlTp06lY444ojSRx99VL39rbfeyoE5gnNZ/NEY0y6ttNJKpY4dO5b22Wef/D/wBRG6aaqyGP/Tj2PqLvE/eqjpyiuvzF/gtG/fPk+V8+yzz1Zvi6kPBw8eXGv/2267rfTVr34177/RRhuV7rvvvlrbY9qws846q9StW7f8h+vOO+9cmjBhwlK7H5qvxiyL5d+bDS01f5fC0vi9WJfQ3bJVxX+apo4dAAAAWjZ9ugEAAKAgQjcAAAAUROgGAACAggjdAAAAUBChGwAAAAoidAMAAEBBhG4AAAAoiNANAAAABRG6AQAAoCBCNwC0Uocffnjae++9m/oyAKBFE7oBgIowd+7cpr4EAGh0QjcAUM8ll1ySNtlkk7T88sunnj17pu9///vp448/zts++eST1KlTp3T77bfXOubuu+/O+3/00Uf58b/+9a90wAEHpC5duqSuXbumvfbaK02cOLFeTfv555+fevTokdZff/2lfJcAUDyhGwCop02bNumKK65Ir732Wvrtb3+bHnnkkfSjH/0ob4tgfdBBB6Ubbrih1jHx+Jvf/GZaccUV07x589LAgQPzv5988sn01FNPpRVWWCHttttutWq0R48enSZMmJBGjRqV7r333qV+nwBQtKpSqVQq/FkAgIoTNc0zZszINdRfJGq1jznmmDR9+vT8+Lnnnkvbbrttrs1effXV07Rp09Iaa6yRHn744bTDDjuk3//+9+m8885Lb7zxRqqqqsrHRNiOWu94vgEDBuTnf+CBB9KkSZNS+/btC79fAGgKaroBgHoiPO+88845SEdt9aGHHpref//9NGvWrLx9q622ShtttFGuBQ8Rstdaa620/fbb58cvv/xy+vvf/56PjRruWKKJ+ezZs9M//vGP6ueJJuwCNwAtmdANANQS/a733HPPtOmmm6Y77rgjjR07Ng0bNixvq9k0/Dvf+U668cYbq5uWH3HEEdW12tH/u2/fvumvf/1rreVvf/tb+ta3vlV9jmiqDgAtWdumvgAAoLJEyJ4/f366+OKLc9/ucNttt9Xb75BDDsn9vKPv9+uvv54GDx5cvW3LLbdMI0aMSKuttloedA0AWis13QDQin344Yf1aqNXWWWVPBDalVdemf75z3+mm266KQ0fPrzesSuttFLad99906mnnpr7aK+55prV2w4++OB8nhixPAZSe+utt9Jjjz2Wjj/++PTOO+8s5bsEgKYjdANAKxZBeIsttqi1RMiOKcMuvPDCtPHGG6ebb745XXDBBQ0ef+SRR+Ym59/+9rdrre/YsWN64oknUq9evXIw32CDDfK+0adbzTcArYnRywGAJRYB/aSTTkqTJ082IBoANECfbgBgscUo5v/+97/Tz3/+8/Td735X4AaABdC8HABYbL/4xS9Snz59Uvfu3dPpp5/e1JcDABVL83IAAAAoiJpuAAAAKIjQDQAAAAURugEAAKAgQjcAAAAUROgGAACAggjdAAAAUBChGwAAAAoidAMAAEBBhG4AAABIxfj/4DVU/vptdgoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian rank (final token, projected): 66\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import grad\n",
    "'''\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        embeddings = torch.cat([self.transformer.wte[i](idx)  for i in range(self.n_head)], dim=-1)\n",
    "        embeddings = embeddings + pos_emb\n",
    "        x = self.transformer.drop(embeddings)\n",
    "        x_orig = x.clone()\n",
    "        for stage in self.transformer.h:  # stages are ExplorerEngineerStage\n",
    "          x = stage(x, x_orig)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "'''\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_drift_trajectories(model, idx, pad_token_id=None):\n",
    "    \"\"\"\n",
    "    Drift vectors Œîh_l = h_{l+1} - h_l aligned to the shrinking time axis.\n",
    "    idx: [B, T_visible]\n",
    "    pad_token_id: pad token id to prepend once (?1 default=0 if None)\n",
    "    returns: list of arrays, one per layer-gap, each shaped [B]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = idx.device\n",
    "    B, T_vis = idx.size()\n",
    "\n",
    "    tok = idx\n",
    "\n",
    "    # token + pos emb\n",
    "    x = model.transformer.wte(tok)\n",
    "\n",
    "    layers = []\n",
    "    with torch.no_grad():\n",
    "        for block in model.transformer.h:\n",
    "            x = block(x)             # time shrinks by n_head each block (per your setup)\n",
    "            layers.append(x.clone()) # store aligned outputs\n",
    "\n",
    "    # compute drifts with alignment: previous tail matches current length\n",
    "    drifts = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        a = layers[i]\n",
    "        b = layers[i + 1]\n",
    "        a_aligned = a[:, -b.size(1):, :]\n",
    "        d = (b - a_aligned)\n",
    "        drifts.append(d.norm(dim=-1).mean(dim=-1).cpu().numpy())  # [B]\n",
    "    return drifts\n",
    "\n",
    "\n",
    "def compute_jacobian_rank(model, idx, projection_dim=324, pad_token_id=None):\n",
    "    \"\"\"\n",
    "    Rank of Jacobian d logits_proj(T_final) / d emb(T_input_last) with pad-once and shrink.\n",
    "    idx: [B, T_visible]\n",
    "    pad_token_id: pad token id to prepend once (?2 default=0 if None)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = idx.device\n",
    "    B, T_vis = idx.size()\n",
    "\n",
    "    tok = idx\n",
    "\n",
    "    # embeddings with grad\n",
    "    tok_emb = model.transformer.wte(tok)\n",
    "    emb = tok_emb\n",
    "    emb = emb.detach()                 # sever any stale history just in case\n",
    "    emb.requires_grad_(True)         # make x a grad leaf for œÑ at layer 0\n",
    "    def forward_emb(x):\n",
    "        for block in model.transformer.h:\n",
    "            x = block(x)             # time shrinks each block\n",
    "        logits = model.lm_head(x)    # [B, T_out, V]\n",
    "        return logits[:, -1, :projection_dim]  # final token after shrink, projected\n",
    "\n",
    "    output = forward_emb(emb)         # [B, P]\n",
    "    jac_rows = []\n",
    "    for i in range(output.shape[-1]):\n",
    "        go = torch.zeros_like(output)\n",
    "        go[:, i] = 1.0\n",
    "        gi = grad(output, emb, grad_outputs=go, retain_graph=True)[0]  # [B, T_all, C]\n",
    "        jac_rows.append(gi[:, -1, :].detach().cpu().numpy().squeeze()) # grad w.r.t. last input step\n",
    "    J = np.stack(jac_rows, axis=0)     # [P, C]\n",
    "    return int(np.linalg.matrix_rank(J))\n",
    "\n",
    "def plot_drift(drift_norms):\n",
    "    \"\"\"\n",
    "    Plot average drift norm per layer.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot([np.mean(d) for d in drift_norms], marker='o')\n",
    "    plt.title(\"Average Drift Norm Across Layers\")\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(\"‚ÄñŒîh‚Äñ mean\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Example usage\n",
    "input_ids = torch.randint(0, model.config.vocab_size, (1, model.config.block_size)).to(device)\n",
    "\n",
    "jac_rank = compute_jacobian_rank(model, input_ids)\n",
    "\n",
    "drift_norms = compute_drift_trajectories(model, input_ids)\n",
    "plot_drift(drift_norms)\n",
    "\n",
    "print(\"Jacobian rank (final token, projected):\", jac_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "JycrO2qclmiC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collect_drift_matrix(model, idx, pad_token_id=None):\n",
    "    \"\"\"\n",
    "    Build a drift matrix by stacking aligned per-layer deltas:\n",
    "      Œî_l = h_{l+1} - tail_match(h_l)\n",
    "    Tail-match aligns time since each block trims from the left.\n",
    "\n",
    "    idx: [B, T_visible]\n",
    "    pad_token_id: ?1 left-pad token id; default 0 if None\n",
    "    returns: torch.FloatTensor [N, D] on CPU where N = sum_l B * T_l\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = idx.device\n",
    "    B, T_vis = idx.size()\n",
    "\n",
    "    tok = idx\n",
    "\n",
    "    # 2) embed like forward() does\n",
    "    x = model.transformer.wte(tok)\n",
    "    x = x.detach()                 # sever any stale history just in case\n",
    "    x.requires_grad_(True)         # make x a grad leaf for œÑ at layer 0\n",
    "    # 3) collect per-block states (time shrinks each block)\n",
    "    states = []\n",
    "    with torch.no_grad():\n",
    "        for block in model.transformer.h:\n",
    "            x = block(x)     # length shrinks by your per-layer trim\n",
    "            states.append(x.clone())\n",
    "\n",
    "    # 4) aligned drifts: match tail of h_l to h_{l+1}\n",
    "    drift_chunks = []\n",
    "    for i in range(len(states) - 1):\n",
    "        a = states[i]       # [B, T_a, C]\n",
    "        b = states[i + 1]   # [B, T_b, C], T_b <= T_a\n",
    "        a_aligned = a[:, -b.size(1):, :]     # tail-match\n",
    "        d = (b - a_aligned).reshape(-1, b.size(-1))  # [B*T_b, C]\n",
    "        drift_chunks.append(d.cpu())\n",
    "\n",
    "    drift_matrix = torch.cat(drift_chunks, dim=0) if drift_chunks else torch.empty(0, x.size(-1))\n",
    "    return drift_matrix  # [N, D] on CPU\n",
    "\n",
    "\n",
    "def run_drift_pca(drift_matrix, k=40):\n",
    "    \"\"\"\n",
    "    PCA on drift samples. drift_matrix can be torch.Tensor [N, D] (CPU or CUDA) or np.ndarray.\n",
    "    \"\"\"\n",
    "    if isinstance(drift_matrix, torch.Tensor):\n",
    "        drift_np = drift_matrix.detach().cpu().numpy()\n",
    "    else:\n",
    "        drift_np = np.asarray(drift_matrix)\n",
    "    k = min(k, drift_np.shape[0], drift_np.shape[1])  # guard\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(drift_np)\n",
    "    explained = pca.explained_variance_ratio_\n",
    "    return explained, pca\n",
    "\n",
    "\n",
    "def plot_explained_variance(explained):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(np.cumsum(explained) * 100, marker='o')\n",
    "    plt.xlabel(\"Principal Component\")\n",
    "    plt.ylabel(\"Cumulative Variance Explained (%)\")\n",
    "    plt.title(\"Drift Trajectory PCA: Explained Variance\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "HjQQpXKBlmiC"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import seaborn as sns\n",
    "\n",
    "def get_projected_residuals(drift_matrix, pca):\n",
    "    \"\"\"\n",
    "    Project Œîh onto PCA space and get residuals.\n",
    "    \"\"\"\n",
    "    proj = pca.transform(drift_matrix.numpy())\n",
    "    recon = pca.inverse_transform(proj)\n",
    "    residuals = drift_matrix.numpy() - recon\n",
    "    return proj, residuals\n",
    "\n",
    "def fit_gmm(proj_data, k=4):\n",
    "    \"\"\"\n",
    "    Fit GMM to PCA-projected drift vectors to identify latent regimes.\n",
    "    \"\"\"\n",
    "    gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=0)\n",
    "    gmm.fit(proj_data)\n",
    "    labels = gmm.predict(proj_data)\n",
    "    return gmm, labels\n",
    "\n",
    "def plot_gmm_clusters(proj_data, labels):\n",
    "    \"\"\"\n",
    "    Plot GMM clustering over first 2 PCA components.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=proj_data[:, 0], y=proj_data[:, 1], hue=labels, palette=\"tab10\", s=10)\n",
    "    plt.title(\"Latent Regimes from Drift PCA (GMM Clusters)\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997
    },
    "id": "bl2x-8MulmiC",
    "outputId": "aeca5079-05c7-49fd-d1dc-251fe3ddd1b7"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 66)) while a minimum of 1 is required by PCA.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m drift_matrix = collect_drift_matrix(model, input_ids)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# PCA and variance\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m explained, pca = \u001b[43mrun_drift_pca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrift_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m plot_explained_variance(explained)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Residuals and GMM regime fit\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mrun_drift_pca\u001b[39m\u001b[34m(drift_matrix, k)\u001b[39m\n\u001b[32m     54\u001b[39m k = \u001b[38;5;28mmin\u001b[39m(k, drift_np.shape[\u001b[32m0\u001b[39m], drift_np.shape[\u001b[32m1\u001b[39m])  \u001b[38;5;66;03m# guard\u001b[39;00m\n\u001b[32m     55\u001b[39m pca = PCA(n_components=k)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrift_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m explained = pca.explained_variance_ratio_\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m explained, pca\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:440\u001b[39m, in \u001b[36mPCA.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    424\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[32m    425\u001b[39m \n\u001b[32m    426\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m \u001b[33;03m        Returns the instance itself.\u001b[39;00m\n\u001b[32m    439\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:503\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    494\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPCA with svd_solver=\u001b[39m\u001b[33m'\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not supported for Array API inputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m     )\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28mself\u001b[39m._fit_svd_solver = \u001b[38;5;28mself\u001b[39m.svd_solver\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/sklearn/utils/validation.py:1128\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1126\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1128\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1129\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1130\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1131\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1132\u001b[39m         )\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1135\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 66)) while a minimum of 1 is required by PCA."
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "input_ids = torch.randint(0, model.config.vocab_size, (1, model.config.block_size)).to(device)\n",
    "\n",
    "# Drift matrix\n",
    "drift_matrix = collect_drift_matrix(model, input_ids)\n",
    "\n",
    "# PCA and variance\n",
    "explained, pca = run_drift_pca(drift_matrix, k=40)\n",
    "plot_explained_variance(explained)\n",
    "\n",
    "# Residuals and GMM regime fit\n",
    "proj, residuals = get_projected_residuals(drift_matrix, pca)\n",
    "gmm, labels = fit_gmm(proj, k=4)\n",
    "plot_gmm_clusters(proj, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "m3fnblfTlmiD",
    "outputId": "708a5405-cfe2-4727-cf34-e512d0a25247"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch, pickle, matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# ---------- load vocab ----------\n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "\n",
    "# ---------- model bits ----------\n",
    "device = next(model.parameters()).device\n",
    "model.eval()                      # <‚Äî turn off dropout everywhere\n",
    "wte = model.transformer.wte\n",
    "wte.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_char_embedding(word):\n",
    "    ids = [stoi[c] for c in word if c in stoi]\n",
    "    if len(ids) < 2:              # ‚Üê need ‚â•2 chars for HailFire geometry\n",
    "        return None\n",
    "    with torch.no_grad():\n",
    "        t = torch.tensor(ids, device=device).unsqueeze(0)      # [1, T]\n",
    "        emb = model.transformer.wte(t).mean(dim=1).squeeze(0)                    # (d,)\n",
    "        return emb.cpu().numpy()\n",
    "\n",
    "# ---------- categories ----------\n",
    "categories = {\n",
    "    \"Verbs\":        [\"play\",\"run\",\"eat\",\"sleep\",\"jump\",\"talk\",\"walk\"],\n",
    "    \"Nouns\":        [\"dog\",\"tree\",\"car\",\"book\",\"child\",\"house\",\"apple\"],\n",
    "    \"FunctionWords\":[\"the\",\"and\",\"in\",\"on\",\"to\",\"of\",\"a   \"],\n",
    "    \"Punctuation\":  [\".    \",\",    \",\"!    \",\"?    \"],\n",
    "}\n",
    "\n",
    "# ---------- gather ----------\n",
    "vecs, labels, words = [], [], []\n",
    "for cat, wl in categories.items():\n",
    "    for w in wl:\n",
    "        v = get_char_embedding(w)\n",
    "        if v is not None:\n",
    "            vecs.append(v)\n",
    "            labels.append(cat)\n",
    "            words.append(w)\n",
    "\n",
    "vecs = np.stack(vecs)             # ‚Üê now guaranteed rectangular\n",
    "\n",
    "# ---------- PCA ----------\n",
    "proj = PCA(n_components=2).fit_transform(vecs)\n",
    "\n",
    "# ---------- plot ----------\n",
    "colors = dict(Verbs=\"blue\", Nouns=\"red\",\n",
    "              FunctionWords=\"green\", Punctuation=\"purple\")\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "for cat in categories:\n",
    "    idx = [i for i,l in enumerate(labels) if l==cat]\n",
    "    plt.scatter(proj[idx,0], proj[idx,1], label=cat, color=colors[cat])\n",
    "    for i in idx:\n",
    "        plt.text(proj[i,0], proj[i,1], words[i], fontsize=9)\n",
    "\n",
    "plt.title(\"Word-class clusters (char-averaged embeddings)\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "UzqWUbNRlmiD"
   },
   "outputs": [],
   "source": [
    "file_path = 'simple_model.pth'\n",
    "\n",
    "# 3. Save the model's state_dict\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
