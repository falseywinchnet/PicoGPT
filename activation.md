Beyond GeLU: Toward the Ideal Activation Function for Deep Networks
Introduction and Motivation
Activation functions (AFs) are the non-linear “gates” of neural networks, shaping how information flows and how gradients propagate[1]. From the early days of logistic sigmoid and Tanh to the dominant Rectified Linear Unit (ReLU) and its modern successors, each activation brought advantages but also notable limitations[2]. Sigmoid and Tanh functions, while biologically inspired, suffer from vanishing gradients and output saturation at extreme inputs[2], making deep networks hard to train. The simpler ReLU addressed those issues and became a state-of-the-art default[3] – used famously in AlexNet and many networks thereafter – due to its stability and sparse activations[4]. However, ReLU gates by sign, outputting zero for any negative input, which leads to the “dying ReLU” problem where neurons can irreversibly shut off (zero output and gradient)[2]. Variants like Leaky ReLU and PReLU introduced a small slope for negative inputs to mitigate this[5][6], but they still break the symmetric data distribution and can shift the output mean away from zero.
Over the past several years, researchers have proposed a myriad of new activation functions to overcome these pitfalls. Exponential Linear Units (ELUs) introduced a smooth negative saturation (approaching a value like -1) to use negative inputs and preserve gradient flow[5]. The Scaled ELU (SELU) went further by inducing “self-normalization” – automatically maintaining zero mean and unit variance in the network’s activations under certain conditions[7]. More recently, smooth activations like Swish and Gaussian Error Linear Unit (GELU) have become popular, especially in Transformer-based models (e.g. BERT, GPT) where ReLU was found suboptimal. Swish (also known as SiLU) was discovered via automated search and is defined as $f(x)=x\cdot \text{sigmoid}(βx)$, with $β$ often set to 1 (or learned)[8]. It can interpolate between linear and ReLU behavior by adjusting $β$, effectively tuning its non-linearity to the task[9]. GELU, introduced by Hendrycks and Gimpel, is $f(x)=x\,\Phi(x)$ (where $\Phi$ is the Gaussian CDF), and was shown to outperform ReLU and even ELU across vision, NLP, and speech tasks[10]. Unlike ReLU’s hard switch at $x=0$, GELU weights inputs by their value rather than by sign[10], yielding a smoother gating that improves training convergence. These innovations – along with others like Mish (defined as $x\tanh(\text{softplus}(x))$)[11] – have incrementally advanced the state of the art, addressing specific issues in prior functions. Mish, for instance, is a non-monotonic smooth activation that showed improved accuracy in certain vision tasks (e.g. boosting YOLOv4 detection AP by 2% over Leaky ReLU, and ResNet-50 ImageNet accuracy by ~1% over ReLU)[12], attributed to its smoothly non-zero gradient acting as a regularizer.
Yet, no existing activation function perfectly satisfies all desired properties for deep networks. Each trade-off that solved one problem introduced another. For example, Swish/GELU are smooth and empirically powerful, but they are non-monotonic (output can dip negative for a range of negative inputs) and not truly zero-centered outputs; ReLU is simple and Lipschitz 1 continuous, but not differentiable at 0 and can obliterate information for $x<0$. ELU/SELU enforce a mean shift toward 0, but saturate for large negative inputs (risking gradient zero in that regime) and require careful initialization or conditions to truly self-normalize. Mish is smooth and self-regularizing, but it too is non-monotonic and relatively untested in large language models. In short, each activation thus far optimizes for certain criteria at the expense of others.
The “Beyond GELU” Challenge: In this context, we propose to formally define what the ideal activation function would look like for today’s deep feed-forward networks (especially Transformers and large MLPMixer/FFN blocks that power modern LLMs and vision models). We outline below the key characteristics such an activation must satisfy. Our goal is to spur researchers (human and AI alike) to devise a new activation function (or family of functions) that combines the strengths of prior activations while eliminating their weaknesses. This is not just a theoretical exercise: a truly superior activation could improve training stability, speed, and performance of deep networks, and might even reduce the need for heavy normalization layers or adaptive tricks. In fact, recent work on Dynamic Tanh (DyT) suggests that a well-chosen activation can replace normalization in Transformers[13]. DyT is a simple element-wise $f(x)=\tanh(αx)$ (with learnable scale $α$) that emulates LayerNorm’s effects by scaling inputs and squashing extremes via the bounded tanh curve[13]. This success hints that with the right properties, an activation function itself can ensure stable signal propagation.
We invite the community – including automated ML researchers and emergent AI research agents (e.g. “Google Scientist”, GPT-Max, Grok Max, etc.) – to take up the Beyond GeLU Challenge: design and identify activation functions that meet all the criteria of the ideal gate outlined below. By rigorously evaluating candidates on a comprehensive benchmark (which we also propose), one can discover if a new activation can truly push beyond GeLU and Swish, ushering in the next leap in deep learning performance.
PLACEHOLDER – Illustration of various activation function curves (ReLU, Tanh, Swish, GELU, etc.) and their characteristics. Each function has different smoothness, symmetry, and behavior in positive/negative regimes. 
Figure 1: Representative curves of common activation functions, highlighting their differences. For example, ReLU is linear for $x>0$ and zero for $x<0$, causing a sharp “kink” at 0. Tanh is smooth and bounded between -1 and 1, but saturates (flat slope) in the extremes. Swish and GELU are smooth and unbounded, approaching linear behavior for large $x$, yet introducing a slight non-monotonic bump for negative inputs (dotted circle). No single existing curve checks all the boxes of the ideal behavior (see criteria below). (Placeholder).
Criteria for the Ideal Activation Function
What properties would the “perfect” activation function have? We enumerate the key criteria below, based on both empirical insights and theoretical considerations for stable deep network training. The ideal gate should satisfy all of these as much as possible:
Tail Competence: Faithful handling of infrequent or extreme inputs. The activation must preserve discriminatory power and gradient flow even for rare or out-of-distribution inputs in the tails. In classification terms, this means even rarely-activated neurons or those corresponding to rarely-seen features/classes should still get meaningful gradient updates and not saturate to a constant. Unlike functions that saturate (e.g. sigmoid/tanh at extreme values) or zero-out large negative inputs (ReLU), the ideal activation continues to respond in low-density regions of the input space. This ensures that “unusual” patterns in data can still be learned, rather than being ignored by a dormant neuron. Tail competence implies no complete flattening of the activation function at extreme values – the function’s slope might diminish gracefully, but not vanish entirely. It also means the activation’s output range or growth should be sufficient to represent outlier values without distortion.
Controlled Silence: Ability to yield zero output when needed, without permanent dead zones. Sparsity is often desirable for efficiency and regularization – e.g. having some neurons “silent” (output exactly 0) for certain inputs can act as an intrinsic dropout and encourage feature economy. The ideal activation should allow intentional silence (for example, output 0 for an appropriate range of inputs or gating signal) to facilitate conditional computation or sparse representations. However, it must not inadvertently kill neurons for all future inputs (the dead neuron problem). In other words, if a neuron’s inputs shift distribution, it should be able to recover from the silent state. ReLU fails here: once a neuron’s weights bias it to produce negative inputs consistently, it outputs 0 constantly and its gradient is zero, effectively dead. Leaky ReLU addressed the irreversibility by keeping a small negative slope, though at the cost of never truly silencing. The ideal function might achieve a middle ground – e.g. it could output exactly zero for a range of inputs (for true silence), but its gradient in that range need not be exactly zero or can re-activate once inputs move out of that range. Controlled silence implies no accidental deadlocks: neurons should not get stuck off forever due to unfortunate initialization or transient conditions. Mechanisms like a small gradient leak in the “off” state or a smooth approach to zero can ensure recoverability.
Gradient Stability: No exploding or vanishing gradients across the activation’s domain. A deep network’s trainability hinges on gradients propagating through many activation layers without blowing up or decaying to nothing. The activation’s derivative should be well-behaved for all input magnitudes – ideally bounded and neither too small nor too large. Functions like ReLU have a gradient of 0 or 1 – stable in magnitude, but the 0 on one side causes gradient absence for negative inputs. Sigmoid and tanh have gradients that drop to near 0 at the tails (vanishing gradient), making deep stacks hard to train[2]. On the flip side, an activation with a very large slope in some region would amplify gradients (and input changes) exponentially, causing training instabilities. The ideal activation would maintain a moderate, nearly constant slope (close to 1.0) over the bulk of its input range, ensuring gradient magnitudes stay in a similar range from layer to layer. This also relates to the function’s Lipschitz continuity: we desire a Lipschitz constant around 1. In fact, a near-identity slope through most of the range helps signal propagation (no shrinking or magnifying), while still providing non-linearity where needed. In practical terms, this means the derivative $f'(x)$ should never be zero (except possibly at extreme infinities) and never explode beyond a certain factor. A Lipschitz constant capped around ~1.1 or so (i.e. the function never locally magnifies changes by more than ~10%) would be ideal. Such gradient stability would avoid both the vanishing gradient of saturating functions and any risk of gradient explosion.
Manifold Integrity: Preservation of the input space’s topology and useful structure. This criterion is more abstract but important: the activation function should not arbitrarily distort or collapse the geometry of data. For example, if the input features lie on some manifold or have a certain dimensional structure, the activation should not reduce the effective dimensionality or “fold” the space in a harmful way. A dramatic example of violating this is a hard threshold (like a step function) which can send many distinct inputs to the exact same output value – collapsing dimensions and losing information. ReLU, for instance, collapses all negative inputs to 0, which is a form of degeneracy – an entire half-line of inputs maps to a single point in output. This can harm the expressiveness of the network and the gradients (many inputs become indistinguishable after activation). The ideal activation would be injective (one-to-one) or at least monotonic, so that it does not identify distinct inputs as the same output. In other words, it should be invertible (at least in a broad sense over its range), or as close to invertible as possible. Monotonic functions preserve ordering of inputs, which helps maintain the structure of the data manifold through the layers. Additionally, preserving dimensionality means the activation should not create flat regions that effectively reduce variance along some direction (except in the controlled manner noted above). Manifold integrity also implies that local distances are reasonably preserved (related to having derivative ~1): points that are close in input should not suddenly become far in output (no tearing of the manifold), and vice versa. Maintaining these properties helps the network’s deeper layers continue to discriminate inputs without needing excessive extra capacity to undo activation distortions.
Mean and Variance Neutrality: No bias shift or uncontrolled variance change. Ideally, an activation function should output signals that have roughly zero mean (assuming the inputs were zero-mean) and a comparable variance to the input’s. This way, each layer does not dramatically alter the distribution of activations, which can ease optimization (the principle behind self-normalizing networks). ReLU, for example, outputs only non-negative values, so even if inputs are mean 0, outputs have positive mean – this biases subsequent layers and often necessitates batch or layer normalization to correct[14]. ELU and SELU were specifically designed to counteract this: ELU’s negative saturation at -1 pulls the mean toward 0, and SELU scales outputs so that mean zero and unit variance are roughly maintained through layers[7]. The ideal activation would inherently keep the output distribution well-centered and not too far in scale from the input. This could be achieved by symmetric shape (odd function or nearly odd around the origin) and by not having unbounded outputs that inflate variance. Note that being strictly zero-mean for arbitrary input distributions may be impossible, but the activation should not introduce severe skew. For example, a good property is: if $x$ is symmetric around 0, then $f(x)$ should also be symmetric (or at least $E[f(x)] = 0$). Mean neutrality helps avoid the compounding bias that can occur in deep stacks[1], and variance neutrality (or a controlled variance scaling) helps prevent signals from exploding or vanishing as they pass through many layers. If an activation tends to amplify variance, the network might need stronger normalization or smaller initial weights; if it shrinks variance, deeper layers could become ineffectual. Thus, variance stability is desired – e.g. output variance maybe equal to input variance, or at least bounded within a factor ~1 (not growing with depth).
Continuity and Smoothness: High-order differentiability for optimization. A kink or discontinuity in the activation function can hinder gradient-based optimization, as it introduces sudden changes in the loss surface. While ReLU’s non-differentiability at 0 hasn’t prevented its success, smoother alternatives often train better once other conditions are comparable[15]. The ideal activation should be continuous and differentiable everywhere, including at the origin or any “turning” points. Moreover, having continuous second (or even third) derivatives could be beneficial – this means no abrupt changes in the slope either. Smooth activations lead to smoother loss landscapes, which can make gradient descent more stable. Functions like Swish, GELU, Mish, and Softplus are all smooth (infinitely differentiable in fact), whereas ReLU has a first-derivative jump (Heaviside step at 0) and even leaky/parametric ReLUs have a discontinuous second derivative at 0 (slope suddenly changes). Curvature continuity might help optimizers like second-order methods or adaptive gradient methods to perform more reliably, and can reduce “gradient noise”. In essence, the ideal function should introduce nonlinearity without introducing non-smooth points. This criterion also connects to avoiding high Lipschitz constants – a very steep section (even if continuous) is like a large second derivative, which can destabilize training. Thus, a gentle nonlinearity that bends the output without any hard corners is preferred.
Robustness to Scale Extremes: Graceful behavior for very large or very small inputs. In practical networks, especially without normalization, neuron pre-activations can sometimes grow very large in magnitude (due to weight initialization variance, or during training spikes). An activation function should handle these extremes without numerical issues or completely saturating. For extremely large positive inputs, an ideal activation might grow roughly linearly (preventing output from blowing up faster than input growth, which could otherwise cause overflow). For extremely large negative inputs, it might approach an asymptote (to avoid a huge negative output which could destabilize following layers) but never exactly flatlines. Functions like tanh and sigmoid cope with $x\to\pm\infty$ by asymptoting, but they flatline (zero gradient) which is problematic. ReLU for $x\to -\infty$ simply outputs 0 (flatline) and for $x\to +\infty$ outputs $\to +\infty$ (which could overflow if not controlled). A compromise might be an activation that for large $|x|$ still increases (or decreases) but at a very reduced slope – enough to keep gradients alive. Additionally, numerical robustness matters: the function should not produce NaN or Inf for any finite input (so it’s good to avoid operations that might overflow in float arithmetic). The ideal activation should thus be designed with consideration for finite precision: e.g., using functions that don’t exceed float32’s range for typical input ranges or providing safe outputs for out-of-range inputs. In summary, handle both tiny and huge inputs gracefully – no abrupt underflow to zero for moderately small values, and no overflow or zero-gradient for moderately large values.
Approximately Identity Initialization (Lipschitz ~1): When a network is first initialized (random weights), it’s often beneficial if each layer initially behaves not too far from an identity mapping. This preserves signal variance and helps deep networks start in a trainable regime. The activation function plays a big role: if its gain is too high or too low, signals can explode or vanish as they pass through many layers. The ideal activation would have a global slope around 1 – in effect, if you feed a small input, the output is of comparable scale. In more technical terms, we want the function’s Lipschitz constant (the maximum slope magnitude) to be close to 1, and certainly not far above 1. This ensures it doesn’t amplify signals (>1 slope could amplify certain modes) nor strongly dampen them (<1 slope everywhere would contract signals exponentially over layers). Some researchers explicitly design activations to be 1-Lipschitz for theoretical robustness and stability reasons (e.g. for Wasserstein GANs or spectral norm constraints). Here, our focus is on the benefit to optimization: a Lipschitz ~1 activation means each layer’s nonlinearity neither explosively grows activations nor squashes them. For instance, ReLU has a slope of 1 in the positive region (Lipschitz 1), which is good, but 0 in negative (which violates other criteria). Swish/GELU have an average slope ~0.5 around 0 but approach slope 1 for large positive inputs; their maximum slope actually slightly exceeds 1 (approximately 1.1 for Swish[16]), which is still within a reasonable range. We would like an activation that essentially behaves like $f(x)\approx x$ for small $x$ (identity near origin), providing a residual-like behavior, while still introducing nonlinearity for larger magnitudes. This way, at initialization, the network is almost linear (easy to propagate signals), and nonlinearity “kicks in” as needed during training or for larger signals. Maintaining this pseudo-linearity (slope ~1) across the range also ties back to preserving manifold structure and gradient flow as noted above.
In summary, the ideal activation function would combine the best aspects of many existing functions: the zero-centered, variance-preserving nature of SELU; the smoothness of GELU/Swish; the unbounded but gentle growth of ReLU in the positive direction; the capacity for sparsity of ReLU (ability to output exact zero); the monotonic invertibility of an odd function like tanh (to avoid collapsing distinct inputs); and a near-identity behavior that makes it play well with deep compositions. We acknowledge that some of these requirements conflict – for example, having true invertibility/monotonicity means the function can’t output a constant region (which would violate controlled silence). Thus, a perfect compromise might not exist in a single simple formula. However, this challenge is about pushing the frontier: perhaps a novel construction (or even an adaptive one) can achieve an excellent balance across all these criteria, significantly better than any single existing activation can.
Limitations of Current Activation Functions
To better appreciate the gap between current activations and the ideal, we briefly survey how popular functions fare on the above criteria, and what trade-offs they make. From early sigmoidal activations to modern non-monotonic smooth functions, each will be assessed against our wish list:
Sigmoid and Tanh – Smooth but Saturating
The classic logistic sigmoid ($1/(1+e^{-x})$) and hyperbolic tangent ($\tanh x$) were among the first non-linearities in neural nets. They are smooth, differentiable (indeed infinitely differentiable) and bounded. Tanh is zero-centered (range $[-1,1]$) which is nicer for mean neutrality than sigmoid (range $[0,1]$ with positive output). However, both suffer severely from vanishing gradients in their saturation regions[2]. For large $|x|$, $\sigma(x)$ approaches 0 or 1 with near-zero derivative, and $\tanh(x)$ approaches -1 or 1 with derivative $\to 0$. This violates tail competence (extreme inputs produce nearly the same output, losing distinction) and gradient stability (gradients disappear for those inputs). In practice, deep networks with sigmoids/tanh can get stuck because early layers saturate and gradients can’t propagate backward to update weights. They also are not Lipschitz ~1 globally: sigmoid’s maximum slope is 0.25, tanh’s is 1 (at 0) but it decreases to 0 beyond. So they tend to contract signals (a sequence of sigmoids will shrink input variance exponentially). On the positive side, they are monotonic (preserving input order) and invertible (one-to-one mapping from $\mathbb{R}$ to $(0,1)$ or $(-1,1)$), which means they maintain manifold integrity except for the collapsing of infinities to the asymptotes. They also produce bounded outputs, which can prevent numeric blow-up. But the saturation drawback made them largely unsuitable for very deep networks[2]. Notably, networks using these often needed careful initialization or layer-wise training in the 1990s. Thus, while smooth and theoretically nice, sigmoid and tanh fail several ideal criteria: no tail competence (they flatline on tails), poor gradient stability (vanishing), not mean-neutral in sigmoid’s case (outputs always positive), and not identity-like (they distort magnitudes, especially sigmoid scaling everything into (0,1)). They inspired many later tweaks – e.g. scaled sigmoid or penalized tanh variants that try to alleviate gradient issues by scaling or adding linear components[14], but the fundamental saturation issue remains.
ReLU – Sparse and Simple, but Harshly Nonlinear
The Rectified Linear Unit $\text{ReLU}(x)=\max(0,x)$ became ubiquitous after it was shown to enable training much deeper networks than sigmoid/tanh in the ImageNet-era vision models[3]. ReLU’s appeal is in its simplicity and effectiveness: for $x>0$, it’s identity ($f(x)=x$ with derivative 1), and for $x<0$, it outputs 0 (acting like an off switch, derivative 0). This contributed to faster convergence and helped avoid some problems of sigmoids[4]. Evaluating ReLU on our criteria: it scores well on gradient stability in the positive region (slope = 1, no vanishing or exploding), and it’s Lipschitz 1 everywhere (piecewise linear with slopes 0 or 1). It also offers true silence for negative inputs – a form of built-in sparsity which can improve efficiency and mimic biological neurons’ inactivity. However, ReLU fails on a few key ideal traits: it is not smooth (there’s a non-differentiability “kink” at 0) and more importantly it annihilates all negative inputs, violating manifold integrity and tail competence for negative values. Distinct negative inputs are all mapped to the same output (0), a major information collapse. The “dying ReLU” problem is a direct outcome – if a neuron’s weighted sum stays negative for all training samples, it outputs 0 constantly and its gradient is zero, so it never updates[2]. This irreversible off state (once weights push it negative, it can stay dead) means ReLU lacks controlled silence – it’s silence without a recovery mechanism. ReLU also shifts mean: even if input is zero-mean, after ReLU the output is non-negative (mean > 0 unless input was always 0-centered with half mass at exactly 0). This can accumulate bias unless mitigated by normalization. ReLU is monotonic increasing for $x>0and constant for $x<0, so it is piecewise monotonic but not globally (there’s a flat region). Its output is unbounded on the high end, which is good for not saturating positive tails (high tail competence in the positive direction), but it completely fails tail competence on negative side (all tail values produce identical output 0). In summary, ReLU introduced the crucial property of linear identity mapping for positive inputs (keeping gradients alive through many layers for half the distribution), which was a game-changer. But it at the same time sacrificed negative inputs entirely. This trade-off is one reason networks with ReLU often rely on techniques like leaky initialization or will have many neurons (hoping some will receive mostly positive inputs so as not to die). ReLU’s shortcomings spurred many follow-ups:
Leaky ReLU (LReLU): $f(x) = x$ if $x>0$, and $f(x)=αx$ if $x<0$ with a small $α>0$ (e.g. 0.01). Introduced to fix dying ReLUs[5], leaky ReLU ensures a small gradient exists even for negative inputs. This improves recoverability – a “dead” neuron still has a chance because its negative inputs yield a small negative output and gradient. LReLU preserves monotonicity (still one linear piece per region) and is Lipschitz max(1,α) (e.g. 1 if α<1). It, however, loses the true silence (outputs never exactly 0 for $x<0$, just a small nonzero value). It also still has a discontinuous derivative at 0 (slope jumps from α to 1). If $α$ is fixed and small, one issue is it’s not adaptive to the data – maybe 0.01 is too low or too high for a given layer. Also, the output is now negatively unbounded (though scaled by α), so negative extreme inputs produce large magnitude negatives at output (but again scaled down, so maybe not too problematic). Importantly, the mean shift is less than ReLU because negative side isn’t entirely zero; one can even set $α$ such that the output mean is zero for a symmetric input distribution (if α is the ratio of positive area to negative area, theoretically).
Parametric ReLU (PReLU)[17]: This extended Leaky ReLU by making the negative slope $α$ a trainable parameter (per layer or per neuron). PReLU allows the network to learn how much negative information to allow. In practice, PReLU can improve performance on some tasks (e.g. Kaiming He et al. reported gains in image classification by using PReLUs in deep nets). However, learnable $α$ introduces risk of the parameter going too low or high; it could potentially turn the function into a purely linear or purely one-sided if it saturates at extreme values. There’s also a small risk of overfitting noted with too many extra parameters[18], especially if each neuron has its own slope. PReLU meets controlled silence better (it can choose to be nearly linear or nearly ReLU as needed), but if $α$ becomes zero, it’s back to ReLU’s issues; if $α$ becomes 1, it’s just linear (no non-linearity). It still isn’t smooth at 0 and still not bounded on either side.
Other ReLU variants: There are many, aiming to tackle specific issues. For example, RReLU adds randomness to the negative slope during training (to regularize, then fixes an average at test)[19]. CReLU concatenates a ReLU of the input and ReLU of the negative of input (essentially providing both half-wave rectified outputs)[20] – this doubles features and preserves some symmetry, at the cost of more neurons. BReLU caps the positive output by an upper bound, making it bounded on both ends (to avoid blow-up)[21]. None of these fundamentally alter the piecewise linear nature and the discontinuity at 0; they are mostly workarounds for dying neurons or unbounded growth. S-shaped ReLU (SReLU) is a notable variant that introduces two “knees”: it’s a piecewise linear function with three segments (negative, middle, positive) and learnable slopes and transition points[22]. SReLU basically learns an arbitrary S-shaped curve for each neuron. This can approximate many shapes (including something akin to tanh or a smoothed ReLU if it wanted). It addresses monotonicity (it can be monotonic if slopes are set so) and can avoid a sharp corner by effectively having a small linear segment in the middle. However, it increases parameter count (four extra parameters per neuron) and risk of overfitting or complexity. It was shown to increase non-linearity capacity and sometimes performance[22], but didn’t catch on widely, likely because of the training complexity and perhaps marginal gains in general settings. Still, SReLU embodies the idea of trainable activation shape, which is one path toward the ideal: let the network learn the best shape per neuron. Similar approaches include APL (Adaptive Piecewise Linear) which sums multiple hinge functions[23], and other splines or kernels for activation. These can in theory approximate the ideal function shape if given enough flexibility, but they are prone to the network not utilizing them fully or even collapsing them to trivial shapes without careful regularization.
In summary, ReLU and its linear variants score high on simplicity, computational efficiency, and positive-side performance, but fail on smoothness, negative-side gradient flow, and mean neutrality. They inspired fixes for dying neurons (leaky slopes) and even trainable shapes, but those fixes introduce their own compromises (no true zeros, extra parameters, etc.). The ideal activation would need to retain ReLU’s core strength (efficient gradient propagation in the unsaturated regime and sparsity) while eliminating the dead zone and rough edges.
ELU, SELU and Self-Normalizing Units – Smoother Exponential Twists
Exponential Linear Unit (ELU) was proposed to combine the advantages of ReLU (identity for positive inputs, unbounded on the upside) with a saturating negative side that brings outputs towards a small negative value rather than 0[5]. ELU is defined as: $$ \text{ELU}(x) = \begin{cases} x & x \ge 0,\ α(e^x - 1) & x < 0, \end{cases} $$ typically with $α=1$. For $x<0$, ELU produces a smooth curve that asymptotically approaches $-α$ as $x\to -\infty$ (for $α=1$, it saturates at -1). At $x=0$, ELU is continuous (both branches give 0) and its first derivative is also continuous (left derivative $α e^0 = α$ equals right derivative 1 if $α=1$). This smoothness is a big improvement over ReLU’s kink – ELU is differentiable everywhere (though second derivative is discontinuous at 0 since the curvature changes). How does ELU fare on the ideals?
Gradient flow: For negative inputs, ELU’s gradient is $α e^x$, which for large negative $x$ goes to 0, meaning it does saturate like tanh on the far left. This means extremely negative inputs will have vanishing gradients (so not perfect tail competence or gradient stability). However, compared to ReLU which had zero gradient for all negative inputs, ELU at least gives a gradient for moderate negatives (e.g. at $x=-1$, gradient ≈0.37 if $α=1$). So it ameliorates the dying neuron issue: a neuron would have to receive very large negative sums to become completely stuck; for mild negatives it still learns. Also, by capping the output at -1, it prevents the output from going to -∞, which helps keep the variance bounded and shifts the mean upward.
Mean/variance neutrality: ELU outputs lie in $[-α,\infty)$, so with $α=1$ that’s $[-1,\infty)$. This negative saturation to -1 helps push the average output closer to 0 (if inputs are roughly symmetric, positives average out with some -1 outputs on negatives). It was observed that ELU can converge faster and sometimes achieve better accuracy than ReLU because of this self-adjusted mean[24][25]. It doesn’t strictly maintain unit variance though; one may still need to combine it with proper initialization.
Controlled silence: ELU doesn’t output exact zero for $x<0$ (unless $x=-\infty$ which is unreachable), so neurons can always recover from negative inputs (no permanent death). It doesn’t provide true silence except at that asymptote, so in practice neurons won’t be exactly off for any finite input – they’ll output somewhere between -1 and 0 for negative domain. This sacrifices sparsity: ELU networks tend to be “dense” (all neurons fire to some degree) which might be a downside for memory or some regularization aspects.
Smoothness: ELU is continuous and differentiable (once differentiable; second derivative has a jump at 0 because left side second derivative at 0 is $α e^0 = α$ and right side second derivative is 0). So it’s much smoother than ReLU, though not as smooth as an infinitely differentiable function like Swish.
Monotonicity and invertibility: ELU is monotonic increasing for all $x$ (the exponential part is monotonic increasing in $x$ for negative $x$). So it preserves input order strictly. It is not one-to-one onto $\mathbb{R}$ because its range is limited to $[-1,\infty)$ for $α=1$; two different inputs won’t map to the same output except the limit case of $-\infty$ mapping to -1. In practice it’s effectively one-to-one for finite inputs. This is good for manifold integrity (no collapse except compressing all $x < -3$ or so to outputs near -1, but strictly speaking different $x$ give different outputs until the limit).
Lipschitz & gradient: The slope on positive side is 1; on negative side the maximum slope is at $x=0^-$ which is $α$ (which equals 1 if $α=1$). For $x$ very negative, slope → 0. So the global Lipschitz constant of ELU is max(1, α); with α=1 it is 1 Lipschitz everywhere (the steepest it gets is slope=1 at 0+ or 0-). That’s excellent for stability – it doesn’t amplify signals by more than 1. However, the minimum slope is 0 as $x\to -\infty$, so it does locally contract there.
Overall, ELU made a solid step toward the ideal on several fronts: monotonic, nearly Lipschitz-1, more symmetric output distribution (bounded negative output), and partially addresses negative gradient flow. It falls short in that it still saturates for very negative inputs and doesn’t provide any true zero outputs (no sparsity).
Scaled ELU (SELU) took ELU further by finding specific scale factors to make the network automatically normalize. SELU is defined as $f(x) = λ \cdot \text{ELU}(x)$ for $x<0$ (with a specific $α$ in ELU), and $λ x$ for $x\ge0$[26]. The constants $α \approx 1.673$ and $λ \approx 1.050$ were chosen such that if inputs to a layer have mean 0 and variance 1, the outputs will also have mean 0 and variance 1 (in the limit as layers go to infinity, under some i.i.d. assumptions)[7]. This self-normalizing property was a remarkable attempt to eliminate the need for BatchNorm/LayerNorm: networks with SELU (along with a specialized “AlphaDropout” to keep the noise consistent) could maintain healthy activations without explicit normalization. SELU meets many ideal criteria well: it’s monotonic, smooth (like ELU, one differentiability at least), zero-mean output by design[7], and maintains variance. It still saturates to $-λ$ on the left, though, so extremely negative inputs are heavily squashed. In practice, SELU networks must be structured in a specific way (e.g. no BatchNorm, use AlphaDropout, specific initialization) to fully reap the self-normalization. SELU didn’t become as widespread as hoped, partly because its benefits show when a network is very deep fully-connected and properly initialized – conditions that BatchNorm or LayerNorm also handle, and those normalization techniques are more general and easier to apply. Nonetheless, SELU demonstrated that careful design can achieve mean/variance neutrality internally, a key part of our ideal. It highlights that an ideal activation might inherently stabilize distributions, simplifying training.
In terms of weaknesses relative to the ideal: ELU/SELU lack true sparsity (no exact zeros except asymptotically) and can still saturate (hence not perfect tail competence). They also introduce a fixed level of nonlinearity – not adaptive beyond scaling. Some later research extended ELU with trainable parameters (e.g. Parametric ELU (PELU) where $α$ itself is learned[7], or CELU which makes it continuously differentiable at 0 by adjusting the formulation). These offer marginal gains. The fundamental trade-off remains: by solving ReLU’s negative-side issues via an exponential, they inevitably reintroduce some vanishing gradient issues akin to sigmoid/tanh on that side.
Swish and GELU – Smooth, Non-Monotonic Blends
Swish (also known as SiLU in some frameworks) emerged around 2017-2018[8] as a result of an exhaustive search over possible activation forms. The form $f(x) = x \cdot \sigma(βx)$ (with $\sigma$ the logistic function) was found to consistently perform well in deep networks[8]. If we set $β=1$, $f(x) = x\,\sigma(x) = \frac{x}{1+e^{-x}}$. This function is smooth (C∞) and interestingly non-monotonic: for negative $x$, as $x$ becomes more negative, $\sigma(x)$ →0 but $x$ is negative, so $f(x)$ approaches 0 from below. There is a small window where as $x$ increases from $-\infty$ to 0, $f(x)$ first becomes more negative (reaching a minimum) and then increases to 0 at $x=0$. In other words, Swish has a slight “dip” into negative output for moderately negative inputs, before rising to 0. After 0, it increases and eventually behaves like linear $y\approx x$ for large positive inputs (since $\sigma(x)\to1$). This non-monotonic behavior was a departure from previous activations, and it appears to act like a self-gating mechanism: small negative inputs yield slightly negative outputs, which might help the network by not completely discarding negative signals (as ReLU would), yet still suppressing them compared to positive signals. Empirically, Swish was shown to match or exceed ReLU/ELU in a variety of tasks[27]. For instance, in a Transformer tested on machine translation, Swish achieved performance comparable to the best known activations[28].
Swish in terms of our criteria: - Smoothness: Excellent – infinitely differentiable, no kinks. - Gradient stability: The derivative is $f'(x) = \sigma(βx) + βx \sigma'(βx)$. For β=1, this simplifies to $\sigma(x) + x\sigma(x)(1-\sigma(x))$. The derivative ranges between 0 (as $x\to -\infty$) and 1 (as $x\to +\infty$) and actually reaches a maximum a bit above 1 for some intermediate x (roughly $x\approx 2$ yields $f'(x)\approx1.1$). So Swish is almost Lipschitz 1, just slightly above[16]. It doesn’t have flat zero-derivative regions except at the limit $x\to -\infty$. Thus, no dead zones in finite range – even at $x=-5$, $f'(x)$ is small (~0.007) but not exactly zero, meaning a neuron could recover if its input distribution shifts upward a bit. This is a big win for gradient flow: it’s much harder for a Swish neuron to truly die; it can always creep back given enough training iterations or weight updates. - Controlled silence: Swish does not produce exact zeros except at $x=0$ (which yields 0 output). Negative inputs produce negative outputs that get very close to 0 for large negative values, but never identically 0 for any finite input. This means it doesn’t inherently sparsify the activations – neurons are almost always “on”, albeit perhaps with a very small output for strongly negative inputs. In scenarios where sparsity is desirable (e.g. certain attention or routing mechanisms), Swish doesn’t provide it; one might then explicitly threshold small values to 0 if needed. However, the flipside is it avoids dead neurons. So Swish opts to sacrifice true silence for continuous responsiveness. - Mean neutrality: Swish is not symmetric, and it will produce a positive-skewed output. For example, input $x$ symmetric around 0 will yield outputs that are mostly positive (since negative $x$ yield outputs near 0, whereas positive $x$ yield up to themselves). This means the mean output is >0. So like ReLU, Swish tends to shift mean upwards (though likely less drastically than ReLU, since moderate negatives yield moderate negatives, not exactly zero). If used without normalization, this could accumulate bias. Usually, in deep nets, Swish is paired with BatchNorm or used in architectures where this shift is managed. It does not inherently normalize mean/var. - Monotonicity: Not monotonic due to the aforementioned bump in negative region. Is that bad? Not necessarily – the non-monotonicity can be seen as a feature: it gives the neuron a regime (slightly negative inputs) where an increase in input decreases the output (until the minimum point is reached). This can act like a form of self-regularization or widening of the repertoire of transformations. Some argue non-monotonic activations allow each neuron to encode a bit more complex behavior (like firing strongly for both very negative and very positive inputs, but suppressing around 0, theoretically useful for some pattern detection). However, non-monotonicity does slightly compromise manifold preservation: it can fold the input-output mapping (two different input values can yield the same output, specifically there’s one output value in the negative range that comes from two different input points due to the dip). In practice the dip is mild, and Swish is almost monotonic for $x> -1$. - Tail handling: For extremely large inputs, Swish behaves linearly (no saturation on the right tail, which is good for allowing growth, albeit one must rely on network weights or normalization to prevent uncontrolled escalation). On the left tail, as $x\to -\infty$, $f(x) \to 0^-$, effectively saturating at 0 from below. So it does saturate negatively similar to ReLU (though approaching 0 from below rather than staying at -1 as ELU does). So negative tail competence is partial – distinctions between extremely large negative inputs are lost (all map to ~0 output). But in practice, if your neuron is seeing hugely negative values, something else is probably off (weights too large, no normalization). The important part is that moderate negative inputs are distinguishable. - Computational complexity: Swish is slightly costlier than ReLU (due to computing a sigmoid). But it’s trivial on modern hardware with fused ops, and certainly worth it for many models given the gains observed[27].
GELU (Gaussian Error Linear Unit) gained fame through its use in BERT (2018) and later Transformers. It was actually introduced earlier by Hendrycks and Gimpel (2016)[10]. GELU’s formula $x \Phi(x)$ (where $\Phi$ is standard normal CDF) is numerically very close to Swish with β≈1.1 in shape. It can be seen as $x \cdot P(Z \le x)$ for $Z\sim N(0,1)$ – i.e. $x$ scaled by the probability that a standard Gaussian is less than $x$. Intuitively, very negative $x$ have tiny probability (so $x \Phi(x) \approx 0$), very positive $x$ have probability ~1 (so $x * 1 = x$), and around $x=0$ it’s $0 * 0.5 = 0$. It transitions smoothly between these regimes. Like Swish, GELU is smooth and non-monotonic. In fact, it also has a small negative dip – for $x<0$ it yields slightly negative outputs (e.g. GELU(-1) ≈ -0.16, GELU(-2) ≈ -0.045, and GELU(-3) ≈ -0.0039; note how it rises back toward 0 for more negative inputs). This means GELU preserves a bit more negative information than ReLU (which would’ve given 0 for any $x<0$). Hendrycks and Gimpel reported that across a variety of tasks (vision, text, speech), replacing ReLU/ELU with GELU gave small but consistent performance improvements[10]. The likely reasons: improved gradient flow due to smoothness, and a form of input-dependent gating that’s neither as harsh as ReLU nor as symmetric as tanh.
On our criteria, GELU is almost interchangeable with Swish in assessment: - It’s smooth and differentiable everywhere. - It has no abrupt saturation except at $-\infty$. The derivative of GELU is similarly bounded between 0 and a bit above 1. So gradient stability is good; there’s no region of complete dead grad except far left tail. - It’s non-monotonic (same caveat as Swish). - It’s not zero-mean output for symmetric input (will skew positive outputs). - It doesn’t produce exact zeros except at $x=0$. So no inherent sparsity. - It preserves order almost everywhere except that small region of negativity (so manifold integrity is mostly fine). - Lipschitz constant ~1 (it doesn’t amplify beyond maybe 1.1 factor at peak). - It handles large input like: large positive -> output large (no cap), large negative -> output ~0 (caps at 0 from negative side). - It weights inputs by their probability of being positive, effectively. So one can interpret it as “if $x$ is likely to be positive, keep it; if likely to be negative, dampen it”. This is a softer decision than ReLU’s hard threshold at 0, arguably a more refined way to gate signals.
Because of their strong empirical performance, Swish and GELU are often considered near the top of the activation function hierarchy today. They address many earlier problems: no dead neurons, smoother loss landscape, good gradient propagation. But as highlighted, they don’t solve everything. Notably, they do not inherently normalize outputs (so unlike SELU, they don’t keep mean/var in check). In very deep networks, that’s why we still see LayerNorm applied in Transformers using GELU – the activation alone doesn’t prevent the gradual drift in distributions. They also do not provide sparsity, which might be a missed regularization opportunity (some research suggests inducing sparsity in intermediate activations can improve interpretability or reduce overfitting). And their non-monotonic nature, while usually harmless, means they aren’t information-lossless (some folding of inputs happens).
Mish is another in this category, proposed in 2019 as a self-regularized non-monotonic function[11]. Its formula $x \tanh(\text{softplus}(x))$ essentially combines elements of Swish and tanh. Softplus $(\log(1+e^x))$ is a smooth version of ReLU (approximate to $\max(0,x)$), and applying tanh to it yields a bounded output between -1 and 1, then multiplying by $x$ re-introduces unbounded growth for large positive $x$. The result is a smooth, non-monotonic curve quite similar to Swish/GELU in shape. Mish tends to have a slightly larger negative dip and a gentler asymptote. It was reported to slightly outperform Swish in certain computer vision benchmarks[12], possibly due to even smoother second-order behavior (Mish’s first derivative is continuous and maybe has smaller variation). However, the differences are subtle and in NLP or transformer models, Mish hasn’t been as widely adopted as GELU. In criteria terms, Mish ticks the same boxes: smooth, no dead gradient, non-monotonic, not mean-neutral, not sparse. One interesting claim in the Mish paper was that Mish’s first derivative being non-monotonic could act as a regularizer and aid optimization[29], hinting that subtle properties of the derivative (like how it behaves at different orders) might influence training dynamics beyond our listed criteria. This suggests our list could even be extended (e.g. consider higher-order smoothness or how the derivative saturates).
In summary, Swish/GELU/Mish represent the current pinnacle of activation design in practice – they are close to our ideal in many ways: they keep gradients alive, are smooth, and they integrate beneficial behaviors (gating like ReLU but softly). The main gaps remaining are in distribution control (mean/var) and true sparsity or invertibility. They let a trickle of gradient through everywhere (preventing death, but also meaning no neuron fully turns off). They don’t inherently keep output zero-centered. They are also fixed functions – they don’t adapt shape based on data (aside from Swish’s optional β parameter, which is often just set to 1 or learned as a single scalar per layer). Perhaps an ideal activation might be a family that adapts to the model’s needs.
Adaptive and Trainable Activations – Learning the Shape
Another approach towards the “ideal” is to not commit to a specific functional form at all, but instead learn the activation function during training. We already saw simple instances: PReLU learns a negative slope, SReLU learns a piecewise linear curve per neuron. There are more general techniques as well: - APL (Adaptive Piecewise Linear): sums of hinge functions that can approximate any piecewise linear shape[23]. - Polynomial/Rational activations: e.g. learn a polynomial function coefficients or even a rational function (ratio of polynomials) as the activation. These can be very expressive; rational functions can approximate smooth activations arbitrarily well and were shown to sometimes outperform fixed functions by fitting the data’s required nonlinearity better. The downside is they can be prone to instability (a learned polynomial of high degree might blow up outside the training range or oscillate). - Spline-based activations: use B-splines or Hermite splines with control points that are learned, ensuring a smooth curve that the network can tweak. - Implicit/Neural activations: here a small neural network (like a tiny MLP) is embedded as the activation function, taking the scalar pre-activation and outputting a scalar activation. This essentially gives the full power of a neural approximator to shape the curve. However, it’s expensive and could collapse to trivial solutions if not carefully regularized.
The motivation for all these is that maybe the ideal activation is too complex to guess upfront, so let the model figure it out. Notably, a 2021 survey[30] pointed out that while modern fixed activations (ELU, Swish, GELU, Mish, etc.) improved upon ReLU, they “still are unable to precisely approximate dependencies characterized by rapid changes”[30]. In other words, if the true function relating a neuron’s input to the desired output is highly non-linear or piecewise, a fixed shape might not capture it well – especially if that shape has only one or two parameters of flexibility. Trainable activations increase the network’s capacity to learn complex mappings.
However, there are trade-offs: - Additional parameters and risk of overfitting (the network could waste capacity shaping activation instead of learning useful features). PReLU’s authors noted potential overfitting[18], and one can imagine more complex parameterizations exacerbate this. - Harder optimization: if the activation shape is also being learned, the loss surface includes these parameters, which could lead to instabilities (activation parameters might oscillate or settle into something suboptimal without careful scheduling). - Many trainable schemes still need constraints to ensure reasonable behavior (e.g. ensuring Lipschitz bounds or monotonicity if desired – otherwise a learned activation might grow uncontrolled or become oddly shaped that hurts generalization).
So far, no trainable activation method has clearly outperformed the well-chosen fixed functions across the board in major applications like language modeling. They often show promise on smaller benchmarks or specific datasets, but practitioners tend to stick with GELU/Swish, possibly due to their robustness and predictability. Nonetheless, the idea of adaptivity is appealing for achieving our ideal: it might be that different layers or even different neurons in a network benefit from different activation shapes. For instance, neurons in early layers might want nearly linear behavior (to preserve raw info), whereas mid-layer neurons might want a saturating behavior to make decisions, and later ones something else. An “ideal” activation might therefore be a flexible one that can morph into ReLU-like or tanh-like or something in between depending on context. Swish partly does this with β: a learned β per layer can make it closer to linear (β→0 gives $f(x)\approx0.5x$, essentially linear scaling) or closer to a hard ReLU (β large makes $\sigma(βx)$ step-like)[8]. In fact, researchers extended Swish to E-Swish (simply a scaled version $β x\sigma(x)$) to control amplitude[31], or learnable sigmoid-shaped functions that generalize the logistic curve[32]. There’s also PAU (Polynomial Adaptive Unit) and similar recent proposals (some mentioned in the survey[33] as “most recent functions”) that indeed give more degrees of freedom.
It is quite plausible that the ultimate solution will involve some trainability – a fixed function might not tick every box perfectly for all layers of all networks. However, adding too much flexibility can break other criteria (like Lipschitz bounds or smoothness) unless constrained. A promising compromise is to design a parametric family of activations with a few parameters that control key properties (e.g. a parameter for negative slope, one for saturation level, one for smoothness) and allow the network to tune those. That way, we keep the search space limited to “safe” shapes.
For example, one could imagine an activation defined as: $$f(x) = a \cdot x \cdot \tanh(bx) + c \cdot x,$$ just as a hypothetical family where $a,b,c$ are parameters. With certain choices it could replicate ReLU ($c=1, a=0$ gives $f(x)=x$ for positives, and 0 for negatives if we treat $\tanh(bx)$ saturation), or tanh-like ($c=0, a=1$ yields $x\tanh(bx)$ which is like Mish without the softplus smoothing), or something new when $a,c$ combine. This is speculative, but it illustrates that by blending linear and saturating components one can cover a spectrum of behaviors. The ideal activation function might not be a single equation, but rather a well-structured activation module.
Finally, a mention on normalized or attention-inspired activations: Dynamic Tanh (DyT) that we discussed[13] is one example where the activation is purposed to handle distribution rather than just non-linearity. It effectively says: use $\tanh$ to keep outputs bounded (preventing runaway variance) and learn a scale $\alpha$ to adjust how steep it is (matching the data’s scale). By doing so, it eliminated the need for LayerNorm in Transformers, achieving comparable performance[34][13]. This is a strong hint that the activation function and normalization can be deeply linked. In some sense, LayerNorm itself applies a linear scaling and shift to each layer’s output to impose mean 0 and variance 1. An ideal activation could incorporate that scaling internally. Imagine an activation that not only does $f(x)$ elementwise, but also ensures collectively the outputs have certain statistics. That’s beyond traditional elementwise definition, but one could conceive a statistical activation that adjusts based on the input distribution (perhaps through a small attention mechanism). For now, DyT achieved it in a simple, elegant way: a single scalar $\alpha$ per layer to set the “gain” of tanh. This suggests that combining criteria of distribution preservation (mean/var neutrality) with the pointwise nonlinearity is a fruitful direction.
In conclusion, while numerous activation functions exist and each addresses some subset of our desired criteria, none fully meets the mark. Table 1 below qualitatively summarizes a few discussed functions against the criteria (✓ = satisfies, ~ = partially, ✕ = does not satisfy):

Table 1: Qualitative comparison of activation functions on ideal criteria. (✓ = mostly satisfies, ~ = partially, ✕ = does not satisfy)
Note: The ideal activation would be ✓ in all columns. As seen, each existing function has at least a couple of ✕ or ~. For instance, Swish/GELU check many boxes (smooth, no dead grad, etc.) but miss on output neutrality and true sparsity/monotonicity. SELU achieves distribution neutrality but at cost of saturating negatives and no sparsity. ReLU has sparsity and simplicity but fails on smoothness and killing info. This landscape underscores the open opportunity to craft a new activation that minimizes these compromises.
Toward “Beyond GeLU”: A Research Challenge and Benchmark
Having defined what we desire and surveyed where current solutions fall short, we now frame the Beyond GeLU Challenge. The objective: discover or invent an activation function (or family) that excels across all the aforementioned criteria in practical training scenarios. This challenge is aimed not just at human researchers but also automated machine-learning systems that can perform neural architecture search or symbolic regression to find novel activation forms.
Key components of the challenge include:
1. Benchmarking Suite for Activation Functions
A crucial step is establishing a standardized benchmark to evaluate activation functions fairly and comprehensively. Historically, new activation functions are often tested on a limited set of tasks (whatever the proposers were focused on) and compared to a few baselines, sometimes yielding results biased to that context. We propose a diverse and transparent benchmark that measures performance on multiple fronts corresponding to our criteria:
Synthetic Gradient Propagation Test: Construct a deep chain of layers (without any normalization) with a given activation function and random weights. Input a small signal and measure how the magnitude of activations and gradients change as they propagate through, both forward and backward. This directly assesses gradient stability and Lipschitz behavior. Ideally, the norm of outputs after N layers should neither explode nor vanish as N grows (a sign of near-neutral impact on variance). This test can reveal if an activation causes exponential growth or decay of signals.
Rare Feature Learning Task: Design a toy dataset where one input feature or one class of samples is extremely rare (tail distribution), but important. For example, a classification task with 100 classes where class 100 has only 1% of the training data. Evaluate networks with different activations on how well they learn the rare class. This probes tail competence – can the activation’s neurons that fire only rarely still get trained enough to recognize that class? If an activation saturates or deadlocks in those regions, the model will likely have poor recall for the rare class. Metrics: accuracy on rare class, or loss decrease on rare vs frequent classes.
Sparsity and Dead Neuron Check: Train a network (say, on a vision or text task) and monitor the percentage of neurons that are effectively inactive (output nearly zero) for most inputs. Also track how many neurons never recovered once they became inactive (if applicable). This evaluates controlled silence. For an ideal activation, we might see some neurons output zero for some inputs (if it offers sparsity), but none should be permanently stuck inactive across all inputs unless it’s by design. We can also measure the average activation distribution: how many outputs are exactly zero vs. small non-zero vs. large – to gauge if the activation encourages useful sparsity or not.
Manifold Preservation Test: Take a set of input points with known distances or structure (e.g., points on a known manifold like a 2D spiral embedded in a higher dimension). Pass them through the activation function and measure how the distances or local neighborhood relationships change. For example, measure the correlation between pairwise distances pre- and post-activation, or use an embedding metric (like trustworthiness in dimensionality reduction) to see if the function is mostly an isometry. This assesses manifold integrity. Monotonic functions will score near-perfect on this (just a monotonic distortion), whereas functions with folds or plateaus will show lost neighbors or collapsed distances.
Output Distribution Tracking: Feed random data (zero-mean unit-variance) into a single-layer network with the activation and observe the output mean and variance. Do this for multiple layers in sequence. This examines mean/variance neutrality. We expect an ideal activation to output roughly zero mean and not deviate variance far from 1 (perhaps oscillating within a small range if slightly contracting or expanding). Plotting mean/var vs. depth for different activations would be illuminating: e.g., ReLU would show mean drifting up ~0.5 sigma per layer and variance potentially changing, whereas SELU would stay around 0 mean, 1 var by design, etc.[7]. The closer to flat lines (0 mean, stable var) over many layers, the better.
Smoothness and Optimization Landscape: This one is harder to quantify directly. One approach: measure how often the gradient of the activation changes abruptly in training. For instance, count occurrences of gradient sign flips or large jumps corresponding to points crossing a non-differentiable kink. ReLU would have lots of those events whenever an input crosses 0. A smooth activation would have none. Alternatively, run a simple optimization (like finding minimum of a 1-layer network’s loss) under different activations and see how smooth/fast the convergence is. This proxies the effect of activation smoothness on optimizer behavior.
Task Performance and Speed: Finally, include a set of real tasks (image classification, language modeling, etc.) to ensure that any candidate activation not only satisfies theoretical metrics but also delivers accuracy and training speed comparable or better than baseline (GeLU, ReLU, etc.). Measure final accuracy, convergence speed (epochs to reach X accuracy), and perhaps robustness (performance under distribution shift or adversarial noise, since some activations might confer robustness if they clamp extremes).
Crucially, this benchmark should be open-source and easy to run for any new activation proposal. Researchers should be encouraged to test their function on this suite and report the results. In this way, the community can accumulate a “scorecard” for activations on the various criteria, rather than single-mindedly focusing on accuracy boosts. Sometimes an activation might give similar accuracy but vastly improve gradient flow or allow deeper networks without norm issues – the benchmark would highlight those advantages.
2. Criteria Trade-off Visualization
We advocate visualizing how different activation functions trade off the criteria – for example, using a radar chart or spider chart with axes like Sparsity, Smoothness, Gradient Flow, Distribution Preservation, Lipschitz, Monotonicity, etc. and plotting different functions. This can be qualitative (based on our analyses) or quantitative (based on the above benchmark metrics). Such a visualization helps quickly convey which functions are closest to the ideal “full coverage” and where gaps remain.
PLACEHOLDER – Radar chart comparing several activation functions (ReLU, ELU, GELU, etc.) across key criteria: each axis represents one criterion (e.g., Smoothness, Tail Competence, etc.), and the ideal activation would cover the outer edge on all axes. Current functions cover some areas well and others poorly, illustrating the need for a new activation that balances all criteria. 
Figure 2: Conceptual radar chart of activation function properties. For instance, ReLU excels in simplicity/efficiency and gradient magnitude (Lipschitz ~1), but scores low in smoothness and manifold preservation (due to its discontinuity and collapsing of negatives). Swish/GELU score high in smoothness and gradient flow, moderate in manifold preservation (minor non-monotonic fold), but low in sparsity (no true zeros) and mean neutrality. An ideal activation (dashed line) would extend to the maximum on all axes – a balance not yet achieved by any single function. (Placeholder).
This kind of chart (Figure 2) would guide where to focus innovation: e.g., the chart might show that sparsity (silence) and mean neutrality are underrepresented in current top activations, suggesting new functions could target those without sacrificing the already-good traits like smoothness and stability.
3. Encouraging Automated Discovery
Given the difficulty of manually designing a function that meets all criteria, it’s fruitful to encourage automated methods. Neural architecture search techniques or even brute-force symbolic regression could be used to explore novel activation formulas. The search space is huge (all possible nonlinear functions), but one can constrain it to reasonable forms: for example, compositions of basic functions (exp, tanh, identity, max, etc.), or piecewise definitions with a small number of parameters. The challenge can include a machine-discovered track: let algorithms search for an activation that maximizes a composite score (weighted sum of benchmark metrics). Prior work like Ramachandran et al.’s search that found Swish[8] could be extended with more objectives. Instead of just maximizing accuracy, a multi-objective search could seek high accuracy and high score on our gradient propagation test, etc.
Involving “emergent AI researchers” (as the user mentioned, systems like GPT-based researchers or Google’s AutoML) is forward-looking. As these systems become more capable, they might generate candidate functions or even infer from theoretical grounds what shape would be ideal. Part of our challenge declaration could be a call-to-action for such systems: “Given this well-specified goal and evaluation, can an AI agent conceive an activation function beyond human intuition that ticks all the boxes?” This might sound speculative, but it aligns with the trend of using AI to optimize AI.
4. Practical Considerations and Constraints
When formulating new activations, we should not ignore pragmatic factors: - Computational cost: The function will be applied millions or billions of times in large networks, so it should be vectorizable and efficient. Avoiding very expensive operations (like integrals or iterative algorithms per activation) is important. Swish/GELU introduced negligible overhead (a sigmoid or an erf call), which was acceptable. Something significantly costlier might not justify its benefits unless those are huge. - Compatibility: The function should ideally be implementable on current hardware (GPUs/TPUs) easily. Functions that use weird conditional logic or non-differentiable components might not parallelize well. The ideal activation might be an analytic function or a simple combination of common ops. - Backwards compatibility: If the function is too exotic, existing initialization schemes or theoretical analyses might not apply. It helps if it can be integrated without needing to reinvent the whole training procedure. For example, if an activation always produces very high variance outputs, one would have to adjust initial weights lower – which is fine if known, but it should not be completely unpredictable. - Range and saturation: One might question if the ideal activation needs to be bounded or unbounded. Bounded (like tanh) ensures no explosion in values but can cause gradient issues; unbounded (like ReLU positive side) ensures capacity to grow but can cause numeric issues if unchecked. A compromise might be a function unbounded on one side but effectively bounded on the other (like ELU/Swish style), or bounded by design but with an adaptive scaling per layer (like DyT’s $\alpha$ taking care of needed range). - Stochastic or dynamic behavior: Some recent approaches make activation stochastic (like RReLU injecting randomness) or stateful (like an activation that oscillates or changes phase – the oscillatory activation paper suggests neurons that can produce periodic outputs). These are intriguing but probably beyond the scope of a general solution because they complicate analysis and reproducibility. Our focus is on deterministic, static activations.
5. Evaluation of Success
What would it mean to have succeeded in going “beyond GELU”? A newly proposed activation (call it “X”) would: - Match or exceed GELU/Swish in model performance on a broad set of tasks (if it’s clearly worse anywhere, it might not justify adoption). - Demonstrate clearly improved metrics in our benchmark, e.g., fewer dead neurons than ReLU, more stable depth scaling than GELU, more zero-centered outputs than Swish, etc. - Be simple enough to gain acceptance (ideally a formula or method that others can easily implement, not an obscure black-box). - Possibly enable dropping other crutches – for example, if X is truly variance-preserving and smooth, maybe networks using X don’t need BatchNorm or LayerNorm as much, simplifying architectures. Or if X yields inherent sparsity, maybe it improves interpretability or efficiency.
We would document these in a report or paper titled “Beyond GELU: Towards the Ideal Activation Function” (the very title of this document). The paper would present the criteria (as we have), show where popular functions stand, introduce the new candidate(s) that were found, and report their properties and results. Even if a single “silver bullet” function is elusive, this effort will clarify the landscape and perhaps yield a toolkit of functions suited for different situations.
In essence, Beyond GELU is both a call for optimization and a thought experiment: by imagining an activation function that hypothetically satisfies all our objectives, we set a target for what future activations (or combinations of them) should strive for. It’s possible the solution isn’t a single function but a combination (e.g., using multiple activation paths and combining them – somewhat like mixing a linear and non-linear pathway). If so, that still counts as an innovation in activation design.
Conclusion and Outlook
The evolution of activation functions has been a story of addressing one problem after another – from the saturation of sigmoids to the rigidity of ReLU to the normalization needs of today’s ultra-deep networks. “Beyond GELU” encapsulates the next step in this story: consolidating the lessons from all prior activations to define a more perfect non-linearity for neural networks. By laying out objective criteria, we aim to shift the search from ad-hoc trial-and-error to a principled exploration.
The ideal activation function we envision would handle rare events, remain stable and smooth, preserve data structure, keep outputs well-behaved, and allow the network to learn efficiently without extraneous fixes. Achieving all of this in one design is undoubtedly challenging – but even approaching it can bring tangible benefits. For example, an activation that is almost identity but prunes extreme outliers could let networks train deeper without normalization. Or one that is almost monotonic but has a tiny oscillation might act as built-in regularization. There is rich space to explore.
We encourage researchers to use the proposed benchmark to evaluate new ideas thoroughly. By reporting not just “my activation got 0.5% better accuracy on CIFAR” but also its impact on gradient flow, neuron utilization, etc., we can accumulate knowledge on what traits actually matter for generalization and training stability. Perhaps certain criteria are less crucial than we think, or maybe one of them (like smoothness or zero-mean output) is a game-changer that was undervalued. The systematic evaluation will reveal this.
It’s also worth noting that different architectures might benefit from different activations. The ideal for an RNN may differ from that for a feed-forward Transformer. Our discussion focused on Transformer-like FFN layers (where GELU is common), but one could extend the analysis to recurrent activations or convolutional nets. The challenge remains similar: find a function that aids learning without downsides. If multiple “ideal” activations emerge specialized for different domains, that’s fine – each can be a target for improvement in its area.
In closing, we reiterate the invitation to both human experts and AI-driven systems to take up the Beyond GeLU challenge. With clear goals and a suite of tests, this is an opportunity to let creativity and algorithms run wild in proposing solutions – be it a clever new formula, a trainable scheme, or even resurrecting an old idea with a modern twist. The next breakthrough in activation functions could unlock faster training, higher accuracy, and more robust models, especially as we push into regimes of extremely deep or autonomous neural network design. GeLU and Swish were not the end of the line – they were stepping stones. We stand at a point where we can articulate what the next stone (or leap) should look like, and we have the tools to find it.
The ideal activation may not exist in a closed-form analytic sense, but through guided search and experimentation, we can approximate it ever more closely. Just as ReLU was unimaginable in the era of sigmoid (who would have thought a half-linear function would work so well?), perhaps the next great activation is currently unimaginable to us – until someone tries it. We hope this document serves as both a map and a catalyst for that discovery.
The challenge is set – now, beyond GeLU, lies the future of AI’s most fundamental non-linearity. Let’s go build it.

[1] [3] [4] [5] [6] [7] [8] [9] [14] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [31] [32] [33] Activation Functions in Deep Learning: A Comprehensive Survey and Benchmark
https://arxiv.org/pdf/2109.14545
[2] [15] [30] A survey on modern trainable activation functions | Request PDF
https://www.researchgate.net/publication/349175040_A_survey_on_modern_trainable_activation_functions
[10] [1606.08415] Gaussian Error Linear Units (GELUs)
https://arxiv.org/abs/1606.08415
[11] [12] [29] [1908.08681] Mish: A Self Regularized Non-Monotonic Activation Function
https://arxiv.org/abs/1908.08681
[13] [34] Transformers without Normalization
https://arxiv.org/html/2503.10622v1
