{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7741bdf-98ab-4400-a0d1-4b831244ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "# --- GPT with auxiliary reverse-embedding loss from zb ---\n",
    "import math, inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# assumes Block and LayerNorm are defined elsewhere (as in your current setup)\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig,\n",
    "                 aux_scale: float = 1.0,\n",
    "                 noise_constituent: float = 1e-4,\n",
    "                 noise_final: float = 1e-4):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None and config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        # per-block orthonormal maps for blocks 0..3\n",
    "        need_blocks = 4\n",
    "        if config.n_layer < need_blocks:\n",
    "            raise ValueError(f\"need at least {need_blocks} transformer blocks for aux; got {config.n_layer}\")\n",
    "        self.aux_blocks = list(range(need_blocks))  # [0,1,2,3] fixed\n",
    "        self.aux_maps = nn.ModuleList()\n",
    "        for _ in self.aux_blocks:\n",
    "            lin = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            nn.init.orthogonal_(lin.weight)  # square => orthonormal rows & columns\n",
    "            self.aux_maps.append(lin)\n",
    "\n",
    "        # noise/scales\n",
    "        self.aux_scale_default = float(aux_scale)\n",
    "        self.noise_constituent = float(noise_constituent)\n",
    "        self.noise_final = float(noise_final)\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # reverse-embedding for one lane (list length T of [idxs, probs])\n",
    "    def _rev_embed_lane(self, lane_seq, device):\n",
    "        T = len(lane_seq)\n",
    "        if T == 0:\n",
    "            return torch.empty(0, self.config.n_embd, device=device)\n",
    "        idxs = torch.tensor([pair[0] for pair in lane_seq], device=device, dtype=torch.long)      # (T, K)\n",
    "        probs = torch.tensor([pair[1] for pair in lane_seq], device=device, dtype=torch.float32)  # (T, K)\n",
    "        probs = probs / (probs.sum(dim=-1, keepdim=True) + 1e-12)\n",
    "\n",
    "        E = self.transformer.wte.weight  # (V, D)\n",
    "        emb = E.index_select(0, idxs.reshape(-1)).reshape(*idxs.shape, E.size(1))  # (T, K, D)\n",
    "        if self.noise_constituent > 0:\n",
    "            emb = emb + torch.randn_like(emb) * self.noise_constituent\n",
    "        rev = torch.einsum('tkd,tk->td', emb, probs)  # (T, D)\n",
    "        if self.noise_final > 0:\n",
    "            rev = rev + torch.randn_like(rev) * self.noise_final\n",
    "        return rev\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 66\n",
    "    n_layer: int = 4          # explicitly 4 per your instruction\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "\n",
    "    # -------- reverse-embedding helpers (batchified, same dtype/device as x) --------\n",
    "def soft_ce(logits: torch.Tensor, target_probs: torch.Tensor, ignore_mask =None):\n",
    "        \"\"\"\n",
    "        logits: (B, T, V)\n",
    "        target_probs: (B, T, V) row-normalized\n",
    "        ignore_mask: (B, T) bool, True where we KEEP, False to ignore\n",
    "        \"\"\"\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        loss = -(target_probs * logp).sum(dim=-1)  # (B, T)\n",
    "        if ignore_mask is not None:\n",
    "            loss = loss * ignore_mask\n",
    "            denom = torch.clamp(ignore_mask.sum(), min=1)\n",
    "            return loss.sum() / denom\n",
    "        else:\n",
    "            return loss.mean()\n",
    "\n",
    "def sharpen_distribution(idx: torch.Tensor, p: torch.Tensor, V: int, alpha: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: (B, T, K) long\n",
    "        p:   (B, T, K) float\n",
    "        return dense (B, T, V) probs with sharpening exponent alpha (alpha>1 => more peaked)\n",
    "        \"\"\"\n",
    "        B, T, K = idx.shape\n",
    "        out = torch.full((B, T, V), 0.0, dtype=p.dtype, device=p.device)\n",
    "        # apply exponent (temperature-like). alpha==1 means unchanged\n",
    "        q = torch.clamp(p, min=1e-12) ** alpha\n",
    "        q = q / q.sum(dim=-1, keepdim=True)\n",
    "        out.scatter_add_(dim=-1, index=idx, src=q)\n",
    "        return out\n",
    "        \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig,\n",
    "                 aux_scale: float = 1.0,           # (12) fixed\n",
    "                 noise_constituent: float = 1e-6,    # (7) fixed\n",
    "                 noise_final: float = 1e-4):         # (7) fixed\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None and config.block_size is not None\n",
    "\n",
    "        self.config = config\n",
    "        self.aux_scale = float(aux_scale)\n",
    "        self.noise_constituent = float(noise_constituent)\n",
    "        self.noise_final = float(noise_final)\n",
    "\n",
    "        # core transformer\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        #self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        # per-block orthonormal linears (square DÃ—D, columns orthonormal)\n",
    "        self.aux_maps = nn.ModuleList()\n",
    "        for _ in range(4):\n",
    "            lin = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            nn.init.orthogonal_(lin.weight)\n",
    "            self.aux_maps.append(lin)\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # --- Patch your GPT class: replace forward with the following ---\n",
    "    def forward(self, idx, targets=None, zb=None,\n",
    "                               aux_scale: float = 1.0,\n",
    "                               depth_alphas  = None,\n",
    "                               warmup_ignores  = None):\n",
    "        \"\"\"\n",
    "        idx: (B, T) Long\n",
    "        targets: (B, T) Long or None\n",
    "        zb: tuple (Z_idx, Z_p) each (B, T, K); single Z shared across blocks\n",
    "        depth_alphas: per-block sharpening exponents (len == n_layer). e.g. [0.8, 1.0, 1.5, 2.0, ...]\n",
    "                      alpha<1 widens early, >1 sharpens later â€” approximates your 'gaussianâ†’peaked student-t'\n",
    "        warmup_ignores: per-block number of initial positions to ignore for aux\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size\n",
    "    \n",
    "        # embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "    \n",
    "        # defaults\n",
    "        L = len(self.transformer.h)\n",
    "        if depth_alphas is None:\n",
    "            # gentle â†’ sharp\n",
    "            depth_alphas = [0.8 + 1.2 * (i/(L-1)) for i in range(L)] if L > 1 else [1.0]\n",
    "        if warmup_ignores is None:\n",
    "            # ignore more in shallow blocks\n",
    "            warmup_ignores = [min(2**i - 1, T-1) for i in range(L)]  # 0,1,3,7,... capped\n",
    "    \n",
    "        # unpack Z\n",
    "        Z_idx, Z_p = zb if zb is not None else (None, None)\n",
    "    \n",
    "        aux_loss = None\n",
    "        for bidx, block in enumerate(self.transformer.h):\n",
    "            x = block(x)  # (B, T, D)\n",
    "            if Z_idx is not None and Z_p is not None:\n",
    "                V = self.lm_head.out_features\n",
    "                logits_b = self.lm_head(self.transformer.ln_f(x))  # (B, T, V)\n",
    "    \n",
    "                # per-depth skew of the SAME base Z\n",
    "                Z_dense = sharpen_distribution(Z_idx, Z_p, V, alpha=float(depth_alphas[bidx]))\n",
    "    \n",
    "                # warmup ignore mask (keep positions >= ignore)\n",
    "                keep = torch.arange(T, device=device).expand(B, T) >= int(warmup_ignores[bidx])\n",
    "                keep = keep.to(logits_b.dtype)\n",
    "                aux_b = soft_ce(logits_b, Z_dense, ignore_mask=(keep>0))\n",
    "                aux_loss = aux_b if aux_loss is None else aux_loss + aux_b\n",
    "    \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "    \n",
    "        ce_loss = None\n",
    "        if targets is not None:\n",
    "            ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "                                      targets.view(-1), ignore_index=-100)\n",
    "    \n",
    "        total = None\n",
    "        if ce_loss is None and aux_loss is None:\n",
    "            total = None\n",
    "        elif aux_loss is None:\n",
    "            total = ce_loss\n",
    "        elif ce_loss is None:\n",
    "            total = aux_scale * aux_loss\n",
    "        else:\n",
    "            total = ce_loss + aux_scale * aux_loss\n",
    "    \n",
    "        if targets is None:\n",
    "            logits = logits[:, [-1], :]\n",
    "    \n",
    "        return logits, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e1f1c-1f54-43b7-975c-4229c4ae1866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d76ace-c3c1-4a68-adce-851c842dbbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Downloading aochildes.txt...\n",
      "ðŸ“¥ Downloading cbt.txt...\n",
      "ðŸ“¥ Downloading children_stories.txt...\n",
      "ðŸ“¥ Downloading gutenberg.txt...\n",
      "ðŸ“¥ Downloading qed.txt...\n",
      "ðŸ“¥ Downloading simple_wikipedia.txt...\n",
      "ðŸ“¥ Downloading switchboard.txt...\n",
      "ðŸ“¥ Downloading wikipedia.txt...\n",
      "ðŸ“¥ Downloading shakespeare.txt...\n",
      "âœ… Done. Files saved to ./babylm_10m_cleaned\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"ðŸ“¥ Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"âŒ Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"âœ… Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95a695ab-753c-4b35-8834-a1d4f59859bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Char tokenizer finalized.\n",
      "ðŸ§¾ Train tokens: 1016242 | Val tokens: 99152\n",
      "ðŸ”¤ Vocab size: 66\n",
      "Loaded 1016242 train tokens and 99152 val tokens | vocab=66\n",
      "Building order-2 Markov...\n",
      "Building order-4 Markov...\n",
      "Building order-8 Markov...\n",
      "Building order-16 Markov...\n",
      "Building order-32 Markov...\n",
      "Building order-64 Markov...\n",
      "Building bigram db...\n",
      "âœ… Markov and Bigram models saved.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'windows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 162\u001b[39m\n\u001b[32m    159\u001b[39m     pickle.dump(bigram_db, f)\n\u001b[32m    161\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Markov and Bigram models saved.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChains: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33morder=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mw\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[43mwindows\u001b[49m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'windows' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"âœ… Char tokenizer finalized.\")\n",
    "print(f\"ðŸ§¾ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"ðŸ”¤ Vocab size: {len(stoi)}\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "data_dir = \"./babylm_char_tokenized\"\n",
    "train_path = os.path.join(data_dir, \"train.bin\")\n",
    "val_path   = os.path.join(data_dir, \"val.bin\")\n",
    "meta_path  = os.path.join(data_dir, \"meta.pkl\")\n",
    "train_ids = np.fromfile(train_path, dtype=np.uint16)\n",
    "val_ids   = np.fromfile(val_path,   dtype=np.uint16)\n",
    "\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "vocab_size = meta[\"vocab_size\"]\n",
    "\n",
    "def global_freqs(ids: np.ndarray, V: int) -> np.ndarray:\n",
    "    cnt = np.bincount(ids.astype(np.int64), minlength=V)\n",
    "    # normalize to probability (avoid zero)\n",
    "    p = cnt.astype(np.float64)\n",
    "    p = p / max(1.0, p.sum())\n",
    "    return p\n",
    "print(f\"Loaded {len(train_ids)} train tokens and {len(val_ids)} val tokens | vocab={vocab_size}\")\n",
    "p_global = global_freqs(train_ids, vocab_size)  # used for disciplined fill only\n",
    "\n",
    "def build_markov_chain(data: np.ndarray, window: int) -> Dict[Tuple[int, ...], Counter]:\n",
    "    chain = defaultdict(Counter)\n",
    "    for i in range(len(data) - window):\n",
    "        ctx = tuple(map(int, data[i:i+window]))\n",
    "        nxt = int(data[i+window])\n",
    "        chain[ctx][nxt] += 1\n",
    "    return chain\n",
    "\n",
    "ngram_orders = [2,4,8,16,32,64]\n",
    "markov_models: Dict[int, Dict[Tuple[int,...], Counter]] = {}\n",
    "for w in ngram_orders:\n",
    "    print(f\"Building order-{w} Markov...\")\n",
    "    markov_models[w] = build_markov_chain(train_ids, w)\n",
    "\n",
    "def build_bigram_db(data: np.ndarray, V: int, top_k=16, epsilon=1e-6, seed=1337):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    counts = np.zeros((V, V), dtype=np.int64)\n",
    "    a = data[:-1].astype(np.int64)\n",
    "    b = data[1:].astype(np.int64)\n",
    "    np.add.at(counts, (a, b), 1)\n",
    "    out = {}\n",
    "    all_ids = np.arange(V, dtype=np.int64)\n",
    "    for t in range(V):\n",
    "        row = counts[t]\n",
    "        tot = row.sum()\n",
    "        if tot == 0:\n",
    "            idx = rng.choice(V, size=top_k, replace=False)\n",
    "            p = np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "        else:\n",
    "            pr = row.astype(np.float64) / float(tot)\n",
    "            obs = np.flatnonzero(row)\n",
    "            if len(obs) >= top_k:\n",
    "                sel = np.argpartition(pr[obs], -top_k)[-top_k:]\n",
    "                idx = obs[sel]\n",
    "                p = pr[idx].astype(np.float32)\n",
    "                s = p.sum()\n",
    "                p = p/s if s > 0 else np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "            else:\n",
    "                need = top_k - len(obs)\n",
    "                mask = np.ones(V, dtype=bool); mask[obs] = False\n",
    "                extra = np.random.default_rng(seed+t).choice(np.nonzero(mask)[0], size=need, replace=False)\n",
    "                idx = np.concatenate([obs, extra])\n",
    "                p   = pr[idx].astype(np.float32)\n",
    "                # give epsilon to never-seen extras\n",
    "                unseen = (row[idx] == 0)\n",
    "                if unseen.any():\n",
    "                    p = p + unseen.astype(np.float32) * epsilon\n",
    "                p = p / p.sum()\n",
    "        order = np.argsort(-p)\n",
    "        out[t] = (idx[order].astype(np.int64), p[order])\n",
    "    return out\n",
    "\n",
    "print(\"Building bigram db...\")\n",
    "bigram_db = build_bigram_db(train_ids, vocab_size, top_k=64)  # collect a bit wider; we'll cap later\n",
    "\n",
    "# === Save ===\n",
    "model_dir = \"./markov_bigram_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(model_dir, \"markov_models.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(markov_models, f)\n",
    "\n",
    "with open(os.path.join(model_dir, \"bigram_db.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(bigram_db, f)\n",
    "\n",
    "print(\"âœ… Markov and Bigram models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcae90-1cdc-4ef5-8907-75961e9a5ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57eff45-c482-4e59-ace0-5c35e65c879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd4dd64a-8f8e-471e-97f2-7ffd1df4c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "# === Replacement dataloader that uses SAVED bigram + markov models and yields (X, Y, Z) ===\n",
    "import os, pickle, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# expects `vocab_size` and `device` already defined in the outer scope\n",
    "# expects saved models at ./markov_bigram_models/{bigram_db.pkl, markov_models.pkl}\n",
    "\n",
    "class DisciplinedZ:\n",
    "    def __init__(self, markov_models: Dict[int, Dict[Tuple[int,...], Counter]],\n",
    "                 bigram_db: Dict[int, Tuple[np.ndarray, np.ndarray]],\n",
    "                 p_global: np.ndarray,\n",
    "                 vocab_size: int,\n",
    "                 top_k: int = 32,\n",
    "                 epsilon: float = 1e-6):\n",
    "        self.models = markov_models\n",
    "        self.bigram_db = bigram_db\n",
    "        self.p_global = p_global.astype(np.float64)\n",
    "        self.V = vocab_size\n",
    "        self.K = top_k\n",
    "        self.eps = float(epsilon)\n",
    "        # global sort for fill\n",
    "        self.global_order = np.argsort(-self.p_global)\n",
    "\n",
    "    def _cands_from_counter(self, ctr: Optional[Counter]) -> Optional[np.ndarray]:\n",
    "        if not ctr:\n",
    "            return None\n",
    "        return np.fromiter((int(t) for t,_ in ctr.items()), dtype=np.int64)\n",
    "\n",
    "    def _probs_from_counter(self, ctr: Optional[Counter]) -> Optional[Dict[int, float]]:\n",
    "        if not ctr:\n",
    "            return None\n",
    "        tot = sum(ctr.values())\n",
    "        if tot == 0:\n",
    "            return None\n",
    "        return {int(t): c/tot for t, c in ctr.items()}\n",
    "\n",
    "    def _bigram_top(self, tok: int, limit: int) -> np.ndarray:\n",
    "        idx, prob = self.bigram_db.get(int(tok), (None, None))\n",
    "        if idx is None:\n",
    "            return np.array([], dtype=np.int64)\n",
    "        return idx[:limit]\n",
    "\n",
    "    def _btree_candidates(self, contexts: Dict[int, Tuple[Tuple[int,...], Optional[Counter]]], backoff_tok: int) -> np.ndarray:\n",
    "        # collect candidate sets from each available context\n",
    "        sets = []\n",
    "        for n, (_, ctr) in contexts.items():\n",
    "            c = self._cands_from_counter(ctr)\n",
    "            if c is not None and c.size > 0:\n",
    "                sets.append(set(c.tolist()))\n",
    "        if len(sets) == 0:\n",
    "            # no ctx â†’ use bigram set as starting point\n",
    "            return self._bigram_top(backoff_tok, self.K)\n",
    "\n",
    "        # try full intersection; if empty, progressively intersect strongest contexts first\n",
    "        inter = set.intersection(*sets) if len(sets) > 1 else sets[0]\n",
    "        if len(inter) == 0:\n",
    "            # heuristic: sort by context order (longest first), intersect greedily\n",
    "            sets_sorted = sorted(sets, key=lambda s: -len(s))\n",
    "            inter = sets_sorted[0].copy()\n",
    "            for s in sets_sorted[1:]:\n",
    "                new_inter = inter.intersection(s)\n",
    "                if len(new_inter) > 0:\n",
    "                    inter = new_inter\n",
    "        if len(inter) == 0:\n",
    "            # last resort: union (still disciplined; no random injection)\n",
    "            union = set()\n",
    "            for s in sets:\n",
    "                union |= s\n",
    "            inter = union\n",
    "\n",
    "        arr = np.fromiter(inter, dtype=np.int64)\n",
    "        if arr.size == 0:\n",
    "            return self._bigram_top(backoff_tok, self.K)\n",
    "        return arr\n",
    "\n",
    "    def _score_candidates(self, cands: np.ndarray, contexts: Dict[int, Tuple[Tuple[int,...], Optional[Counter]]]) -> np.ndarray:\n",
    "        # score = sum over contexts of presence * local prob\n",
    "        # local prob from per-context normalized counts\n",
    "        scores = np.zeros(cands.size, dtype=np.float64)\n",
    "        idxmap = {int(t): i for i, t in enumerate(cands)}\n",
    "        for n, (_, ctr) in contexts.items():\n",
    "            probs = self._probs_from_counter(ctr)\n",
    "            if probs is None:\n",
    "                continue\n",
    "            for t, p in probs.items():\n",
    "                if t in idxmap:\n",
    "                    scores[idxmap[t]] += float(p)\n",
    "        # tiny floor to avoid zeros\n",
    "        scores = scores + (scores == 0) * self.eps\n",
    "        return scores\n",
    "\n",
    "    def build_Z_for_sequence(self, seq: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        seq: array of length L = block_size (+optional pad)\n",
    "        returns:\n",
    "          topk_idx: (L, K) int64\n",
    "          topk_p:   (L, K) float32  (row-normalized)\n",
    "        \"\"\"\n",
    "        L = len(seq)\n",
    "        topk_idx = np.zeros((L, self.K), dtype=np.int64)\n",
    "        topk_p   = np.zeros((L, self.K), dtype=np.float32)\n",
    "        for j in range(L):\n",
    "            back_tok = int(seq[j])\n",
    "            # collect contexts\n",
    "            contexts = {}\n",
    "            for n in ngram_orders:\n",
    "                if j - (n-1) < 0:\n",
    "                    continue\n",
    "                ctx = tuple(int(x) for x in seq[j-(n-1):j+1])\n",
    "                ctr = self.models[n].get(ctx, None)\n",
    "                contexts[n] = (ctx, ctr)\n",
    "\n",
    "            # disciplined candidate set\n",
    "            cands = self._btree_candidates(contexts, back_tok)\n",
    "\n",
    "            # cap K by candidate count\n",
    "            if cands.size >= self.K:\n",
    "                # score & take best K\n",
    "                scores = self._score_candidates(cands, contexts)\n",
    "                order = np.argsort(-scores)[:self.K]\n",
    "                idx = cands[order]\n",
    "                sc  = scores[order]\n",
    "            else:\n",
    "                # we must fill with globally-most-common tokens (no randoms), excluding existing\n",
    "                scores = self._score_candidates(cands, contexts) if cands.size > 0 else np.array([], dtype=np.float64)\n",
    "                missing = self.K - cands.size\n",
    "                mask = np.ones(vocab_size, dtype=bool)\n",
    "                mask[cands] = False\n",
    "                fill = []\n",
    "                for t in self.global_order:\n",
    "                    if mask[t]:\n",
    "                        fill.append(int(t))\n",
    "                        if len(fill) == missing:\n",
    "                            break\n",
    "                if cands.size == 0:\n",
    "                    idx = np.array(fill, dtype=np.int64)\n",
    "                    sc  = np.full(len(fill), self.eps, dtype=np.float64)\n",
    "                else:\n",
    "                    idx = np.concatenate([cands, np.array(fill, dtype=np.int64)])\n",
    "                    sc  = np.concatenate([scores, np.full(missing, self.eps, dtype=np.float64)])\n",
    "\n",
    "            # normalize to prob\n",
    "            p = sc.astype(np.float64)\n",
    "            p = p / p.sum() if p.sum() > 0 else np.full_like(p, 1.0/len(p))\n",
    "            topk_idx[j, :] = idx\n",
    "            topk_p[j, :]   = p.astype(np.float32)\n",
    "        return topk_idx, topk_p\n",
    "\n",
    "discZ = DisciplinedZ(markov_models, bigram_db, p_global, vocab_size, top_k=32, epsilon=1e-6)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class GPUDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size: int, batch_size: int, builder: DisciplinedZ, pad_len:int=0, jitter:int=63, p_aligned:float=0.5, seed:int=1337):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = int(block_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.pad_len    = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = max(1, self.total // self.sample_len)\n",
    "        self.jitter = int(jitter)\n",
    "        self.p_aligned = float(p_aligned)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.builder = builder\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def _sample_block(self):\n",
    "        base_block = self.rng.integers(0, self.n_blocks)\n",
    "        start = base_block * self.sample_len\n",
    "        if self.rng.random() > self.p_aligned:\n",
    "            j = self.rng.integers(0, self.jitter + 1)\n",
    "            start = min(start + j, self.total)\n",
    "        return start\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        B, T = self.batch_size, self.block_size\n",
    "        X = np.empty((B, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((B, T), dtype=np.int64)\n",
    "        Z_idx = np.empty((B, T, self.builder.K), dtype=np.int64)\n",
    "        Z_p   = np.empty((B, T, self.builder.K), dtype=np.float32)\n",
    "        for i in range(B):\n",
    "            start = self._sample_block()\n",
    "            xseq = self.data[start : start + self.sample_len].astype(np.int64)\n",
    "            yseq = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + T].astype(np.int64)\n",
    "            X[i] = xseq\n",
    "            Y[i] = yseq\n",
    "            idxs, probs = self.builder.build_Z_for_sequence(xseq[:T])\n",
    "            Z_idx[i] = idxs\n",
    "            Z_p[i]   = probs\n",
    "        # torch tensors\n",
    "        X = torch.from_numpy(X[:, :T]).to(device)\n",
    "        Y = torch.from_numpy(Y).to(device)\n",
    "        Z_idx = torch.from_numpy(Z_idx).to(device)\n",
    "        Z_p   = torch.from_numpy(Z_p).to(device)\n",
    "        return X, Y, (Z_idx, Z_p)\n",
    "\n",
    "def collate_identity(batch):\n",
    "    Xs, Ys, Zs = zip(*batch)\n",
    "    X = torch.cat(Xs, dim=0)\n",
    "    Y = torch.cat(Ys, dim=0)\n",
    "    Zi = torch.cat([z[0] for z in Zs], dim=0)\n",
    "    Zp = torch.cat([z[1] for z in Zs], dim=0)\n",
    "    return X, Y, (Zi, Zp)\n",
    "\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "GPU_DATASET = GPUDataset(\n",
    "    np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r'),\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    builder=discZ,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    GPU_DATASET,\n",
    "    batch_size=1,            # keep outer loader at 1; inner dataset batches on GPU\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_identity\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efacb560-8e61-4e4e-9399-3c1611b476ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1.67M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "config =  GPTConfig(\n",
    "    block_size,\n",
    "    vocab_size,\n",
    "    n_layer=8,      \n",
    "    n_head = 8,\n",
    "    n_embd =128)\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, yb, zb in train_loader:\n",
    "        # xb: (B, T), yb: (B, T), zb: (Z_idx, Z_p) with shapes (B,T,K)\n",
    "        logits, loss = model(xb, None, zb, aux_scale=1.0)  # tweak aux_scale as desired\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total += loss.item()\n",
    "        print(loss.item())\n",
    "        losses.append(loss.item())\n",
    "    return total / len(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c43d3e5c-ba7e-4a1e-9b59-1b593316e2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1799936\n"
     ]
    }
   ],
   "source": [
    "print(sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ae6451f-1367-40a2-9fb9-85a5085ef5bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.929786682128906\n",
      "12.048897743225098\n",
      "11.785774230957031\n",
      "11.841695785522461\n",
      "11.919280052185059\n",
      "11.875351905822754\n",
      "11.689604759216309\n",
      "11.982873916625977\n",
      "11.467243194580078\n",
      "11.582125663757324\n",
      "11.963581085205078\n",
      "11.704437255859375\n",
      "11.951882362365723\n",
      "12.026013374328613\n",
      "11.945052146911621\n",
      "11.536442756652832\n",
      "11.19283676147461\n",
      "12.095396041870117\n",
      "12.714115142822266\n",
      "12.26547908782959\n",
      "11.745171546936035\n",
      "11.949601173400879\n",
      "12.613593101501465\n",
      "12.11513900756836\n",
      "12.396453857421875\n",
      "11.445077896118164\n",
      "12.177733421325684\n",
      "11.483196258544922\n",
      "12.004140853881836\n",
      "12.02764892578125\n",
      "11.926264762878418\n",
      "11.871381759643555\n",
      "11.81935977935791\n",
      "11.432693481445312\n",
      "12.154752731323242\n",
      "11.85028076171875\n",
      "12.163487434387207\n",
      "11.950431823730469\n",
      "11.26310920715332\n",
      "12.32371997833252\n",
      "11.771759033203125\n",
      "11.639663696289062\n",
      "11.906404495239258\n",
      "11.721084594726562\n",
      "11.883601188659668\n",
      "11.712666511535645\n",
      "11.616727828979492\n",
      "11.495132446289062\n",
      "11.902482986450195\n",
      "12.336654663085938\n",
      "11.551952362060547\n",
      "11.744660377502441\n",
      "11.683990478515625\n",
      "12.093704223632812\n",
      "11.752134323120117\n",
      "12.302739143371582\n",
      "12.39759635925293\n",
      "11.880578994750977\n",
      "11.289250373840332\n",
      "12.269998550415039\n",
      "11.832391738891602\n",
      "12.461377143859863\n",
      "11.214354515075684\n",
      "12.138986587524414\n",
      "12.070146560668945\n",
      "12.002963066101074\n",
      "11.964776039123535\n",
      "11.682333946228027\n",
      "11.765829086303711\n",
      "12.21760082244873\n",
      "11.427871704101562\n",
      "11.84443473815918\n",
      "11.863313674926758\n",
      "11.799389839172363\n",
      "11.830816268920898\n",
      "11.677043914794922\n",
      "12.025540351867676\n",
      "11.751630783081055\n",
      "11.872346878051758\n",
      "11.350543022155762\n",
      "11.791729927062988\n",
      "11.869531631469727\n",
      "11.098200798034668\n",
      "11.490666389465332\n",
      "11.95727252960205\n",
      "11.758871078491211\n",
      "11.802102088928223\n",
      "12.019086837768555\n",
      "11.700051307678223\n",
      "11.949933052062988\n",
      "11.842056274414062\n",
      "12.364141464233398\n",
      "11.722882270812988\n",
      "11.870498657226562\n",
      "12.046364784240723\n",
      "12.136824607849121\n",
      "12.521852493286133\n",
      "12.31488037109375\n",
      "11.806188583374023\n",
      "12.373294830322266\n",
      "11.977198600769043\n",
      "11.761500358581543\n",
      "12.57496452331543\n",
      "11.965680122375488\n",
      "12.004197120666504\n",
      "12.086263656616211\n",
      "12.097843170166016\n",
      "11.570021629333496\n",
      "11.62783145904541\n",
      "11.839900016784668\n",
      "11.989775657653809\n",
      "11.519018173217773\n",
      "12.133147239685059\n",
      "11.75217056274414\n",
      "11.910514831542969\n",
      "11.759148597717285\n",
      "11.827771186828613\n",
      "11.770524024963379\n",
      "11.769347190856934\n",
      "11.603950500488281\n",
      "11.831731796264648\n",
      "12.125350952148438\n",
      "11.420463562011719\n",
      "11.65890884399414\n",
      "12.269566535949707\n",
      "11.869226455688477\n",
      "12.23421573638916\n",
      "11.82606029510498\n",
      "12.144590377807617\n",
      "11.589570045471191\n",
      "11.873997688293457\n",
      "11.747505187988281\n",
      "11.743023872375488\n",
      "11.694211959838867\n",
      "12.353818893432617\n",
      "11.959251403808594\n",
      "11.589147567749023\n",
      "11.686736106872559\n",
      "11.86099624633789\n",
      "11.962981224060059\n",
      "11.997310638427734\n",
      "12.046658515930176\n",
      "11.78744888305664\n",
      "12.349874496459961\n",
      "11.839978218078613\n",
      "12.625273704528809\n",
      "11.852429389953613\n",
      "11.984417915344238\n",
      "11.890634536743164\n",
      "11.75797176361084\n",
      "12.031525611877441\n",
      "11.148765563964844\n",
      "12.192363739013672\n",
      "11.652054786682129\n",
      "12.297471046447754\n",
      "11.678083419799805\n",
      "11.58907699584961\n",
      "11.756779670715332\n",
      "11.698384284973145\n",
      "11.96729564666748\n",
      "11.85165786743164\n",
      "11.693535804748535\n",
      "12.165138244628906\n",
      "11.940154075622559\n",
      "11.662482261657715\n",
      "11.596630096435547\n",
      "11.911977767944336\n",
      "11.649382591247559\n",
      "11.843741416931152\n",
      "11.744074821472168\n",
      "11.576499938964844\n",
      "12.468201637268066\n",
      "12.114114761352539\n",
      "11.67358112335205\n",
      "11.516464233398438\n",
      "11.966439247131348\n",
      "12.002570152282715\n",
      "11.106801986694336\n",
      "11.362089157104492\n",
      "12.057165145874023\n",
      "11.662801742553711\n",
      "11.738980293273926\n",
      "11.861128807067871\n",
      "11.881282806396484\n",
      "11.206114768981934\n",
      "11.957038879394531\n",
      "11.86518669128418\n",
      "11.396819114685059\n",
      "11.80626392364502\n",
      "11.318734169006348\n",
      "12.045021057128906\n",
      "11.444849014282227\n",
      "11.820453643798828\n",
      "11.458087921142578\n",
      "11.645329475402832\n",
      "11.298506736755371\n",
      "11.517420768737793\n",
      "12.210071563720703\n",
      "11.841869354248047\n",
      "11.928001403808594\n",
      "12.018162727355957\n",
      "12.091010093688965\n",
      "11.738405227661133\n",
      "11.652356147766113\n",
      "12.18881607055664\n",
      "11.9777250289917\n",
      "11.366718292236328\n",
      "11.867729187011719\n",
      "12.072867393493652\n",
      "11.291199684143066\n",
      "11.462027549743652\n",
      "11.781013488769531\n",
      "11.569592475891113\n",
      "12.0650634765625\n",
      "11.171489715576172\n",
      "11.69491195678711\n",
      "10.923276901245117\n",
      "11.69756031036377\n",
      "11.44465160369873\n",
      "11.47091293334961\n",
      "11.449593544006348\n",
      "12.19373893737793\n",
      "11.864435195922852\n",
      "11.733453750610352\n",
      "11.80815315246582\n",
      "11.468337059020996\n",
      "12.169970512390137\n",
      "11.75546932220459\n",
      "12.43235969543457\n",
      "12.371177673339844\n",
      "11.674863815307617\n",
      "11.761425018310547\n",
      "11.914094924926758\n",
      "11.427449226379395\n",
      "11.962850570678711\n",
      "11.634968757629395\n",
      "12.499287605285645\n",
      "11.765764236450195\n",
      "11.291749954223633\n",
      "11.758687973022461\n",
      "11.862491607666016\n",
      "12.262325286865234\n",
      "11.459182739257812\n",
      "11.327518463134766\n",
      "11.330057144165039\n",
      "11.832971572875977\n",
      "11.703460693359375\n",
      "11.377236366271973\n",
      "12.234262466430664\n",
      "11.507584571838379\n",
      "11.419553756713867\n",
      "11.689594268798828\n",
      "11.72480583190918\n",
      "11.465500831604004\n",
      "11.956277847290039\n",
      "11.940180778503418\n",
      "12.108536720275879\n",
      "11.696746826171875\n",
      "11.507418632507324\n",
      "11.276169776916504\n",
      "11.554322242736816\n",
      "12.167074203491211\n",
      "11.399630546569824\n",
      "11.68493366241455\n",
      "12.139698028564453\n",
      "11.49955940246582\n",
      "11.697094917297363\n",
      "12.073217391967773\n",
      "11.364919662475586\n",
      "11.893035888671875\n",
      "11.375121116638184\n",
      "12.134513854980469\n",
      "11.25898265838623\n",
      "11.368606567382812\n",
      "11.23099136352539\n",
      "11.685185432434082\n",
      "11.273292541503906\n",
      "11.532743453979492\n",
      "11.912571907043457\n",
      "11.587567329406738\n",
      "11.5372896194458\n",
      "11.605584144592285\n",
      "11.945636749267578\n",
      "11.463875770568848\n",
      "11.41722297668457\n",
      "12.118375778198242\n",
      "11.160148620605469\n",
      "11.646537780761719\n",
      "11.88460922241211\n",
      "11.543929100036621\n",
      "11.281229972839355\n",
      "12.195947647094727\n",
      "11.696478843688965\n",
      "11.367925643920898\n",
      "11.5223388671875\n",
      "11.714653015136719\n",
      "11.249701499938965\n",
      "11.651500701904297\n",
      "11.78634262084961\n",
      "11.739980697631836\n",
      "11.685653686523438\n",
      "11.093992233276367\n",
      "11.61460018157959\n",
      "11.545475959777832\n",
      "11.35818099975586\n",
      "11.70267105102539\n",
      "11.529376029968262\n",
      "11.493669509887695\n",
      "10.99879264831543\n",
      "11.90052604675293\n",
      "11.538610458374023\n",
      "11.62903881072998\n",
      "11.635939598083496\n",
      "11.777963638305664\n",
      "11.279855728149414\n",
      "11.50312614440918\n",
      "11.801980972290039\n",
      "11.422685623168945\n",
      "12.291966438293457\n",
      "11.824104309082031\n",
      "11.431758880615234\n",
      "11.73240852355957\n",
      "11.792023658752441\n",
      "11.68057632446289\n",
      "11.397955894470215\n",
      "11.685537338256836\n",
      "11.35814094543457\n",
      "12.063030242919922\n",
      "11.164793968200684\n",
      "11.35236930847168\n",
      "11.712553024291992\n",
      "11.358190536499023\n",
      "11.336589813232422\n",
      "12.35698413848877\n",
      "11.68846607208252\n",
      "11.456857681274414\n",
      "11.336817741394043\n",
      "11.985308647155762\n",
      "11.86799144744873\n",
      "11.16383171081543\n",
      "11.319305419921875\n",
      "11.589729309082031\n",
      "11.619510650634766\n",
      "11.670949935913086\n",
      "11.552846908569336\n",
      "11.774173736572266\n",
      "12.106461524963379\n",
      "11.962179183959961\n",
      "11.707499504089355\n",
      "11.57558822631836\n",
      "11.696131706237793\n",
      "11.54520034790039\n",
      "11.871137619018555\n",
      "11.544798851013184\n",
      "11.726462364196777\n",
      "11.491144180297852\n",
      "11.397128105163574\n",
      "11.77982234954834\n",
      "11.774090766906738\n",
      "11.51798152923584\n",
      "11.884208679199219\n",
      "11.695670127868652\n",
      "11.997712135314941\n",
      "11.699272155761719\n",
      "12.107124328613281\n",
      "12.006818771362305\n",
      "11.75267219543457\n",
      "11.974018096923828\n",
      "11.71908187866211\n",
      "11.620347023010254\n",
      "11.572122573852539\n",
      "11.222297668457031\n",
      "11.079333305358887\n",
      "11.556800842285156\n",
      "11.46112060546875\n",
      "11.192570686340332\n",
      "11.42082691192627\n",
      "11.895757675170898\n",
      "11.436630249023438\n",
      "11.847821235656738\n",
      "11.604545593261719\n",
      "11.825736045837402\n",
      "11.380546569824219\n",
      "11.725566864013672\n",
      "11.628849029541016\n",
      "11.424654006958008\n",
      "11.934420585632324\n",
      "12.180922508239746\n",
      "11.704777717590332\n",
      "11.744122505187988\n",
      "11.609092712402344\n",
      "11.49712085723877\n",
      "11.628662109375\n",
      "11.830270767211914\n",
      "11.784730911254883\n",
      "11.40394115447998\n",
      "11.305103302001953\n",
      "12.017091751098633\n",
      "11.59272289276123\n",
      "11.267365455627441\n",
      "11.629433631896973\n",
      "11.83240032196045\n",
      "11.079329490661621\n",
      "11.432764053344727\n",
      "11.652200698852539\n",
      "11.489935874938965\n",
      "11.39823055267334\n",
      "11.521790504455566\n",
      "11.709814071655273\n",
      "10.779508590698242\n",
      "11.186083793640137\n",
      "11.91473388671875\n",
      "11.81532096862793\n",
      "11.235002517700195\n",
      "11.289342880249023\n",
      "11.565667152404785\n",
      "11.375579833984375\n",
      "11.4375581741333\n",
      "11.546161651611328\n",
      "11.714973449707031\n",
      "11.54856014251709\n",
      "11.708625793457031\n",
      "11.38009262084961\n",
      "11.579391479492188\n",
      "11.913132667541504\n",
      "12.007669448852539\n",
      "11.29033374786377\n",
      "11.601658821105957\n",
      "12.05899429321289\n",
      "11.675028800964355\n",
      "12.127265930175781\n",
      "11.783926963806152\n",
      "11.04344367980957\n",
      "11.451837539672852\n",
      "11.276229858398438\n",
      "11.872213363647461\n",
      "10.98794174194336\n",
      "11.658035278320312\n",
      "11.426318168640137\n",
      "11.842585563659668\n",
      "11.057168960571289\n",
      "11.83215045928955\n",
      "11.665681838989258\n",
      "11.84364128112793\n",
      "11.731471061706543\n",
      "12.31106948852539\n",
      "11.87077522277832\n",
      "11.744597434997559\n",
      "11.89904499053955\n",
      "11.680152893066406\n",
      "12.007177352905273\n",
      "11.613828659057617\n",
      "11.504913330078125\n",
      "11.444437980651855\n",
      "11.693706512451172\n",
      "11.505533218383789\n",
      "12.122846603393555\n",
      "11.428186416625977\n",
      "11.474650382995605\n",
      "11.148482322692871\n",
      "11.073029518127441\n",
      "11.39990234375\n",
      "11.48483657836914\n",
      "11.605774879455566\n",
      "11.625377655029297\n",
      "11.41326904296875\n",
      "11.623868942260742\n",
      "11.423551559448242\n",
      "11.459213256835938\n",
      "11.850479125976562\n",
      "11.123530387878418\n",
      "11.805015563964844\n",
      "11.519013404846191\n",
      "11.118917465209961\n",
      "11.33364200592041\n",
      "11.336786270141602\n",
      "11.565025329589844\n",
      "12.084773063659668\n",
      "11.649956703186035\n",
      "11.00583267211914\n",
      "11.516390800476074\n",
      "11.137840270996094\n",
      "11.737853050231934\n",
      "11.750140190124512\n",
      "11.495752334594727\n",
      "11.265543937683105\n",
      "11.7925386428833\n",
      "11.342782974243164\n",
      "11.728099822998047\n",
      "11.03408145904541\n",
      "11.4051513671875\n",
      "11.55965805053711\n",
      "11.426450729370117\n",
      "11.392313957214355\n",
      "11.796303749084473\n",
      "11.526752471923828\n",
      "11.528608322143555\n",
      "11.10547161102295\n",
      "11.250709533691406\n",
      "12.034882545471191\n",
      "11.603544235229492\n",
      "11.433464050292969\n",
      "10.849319458007812\n",
      "11.489404678344727\n",
      "11.586772918701172\n",
      "11.640589714050293\n",
      "11.59777545928955\n",
      "10.751996040344238\n",
      "11.47519302368164\n",
      "11.634634971618652\n",
      "11.291009902954102\n",
      "11.615974426269531\n",
      "11.439323425292969\n",
      "11.37362289428711\n",
      "11.706716537475586\n",
      "11.22508430480957\n",
      "11.897324562072754\n",
      "11.942496299743652\n",
      "11.428872108459473\n",
      "11.28701400756836\n",
      "11.534567832946777\n",
      "11.463740348815918\n",
      "11.21277904510498\n",
      "11.229015350341797\n",
      "10.959951400756836\n",
      "11.938202857971191\n",
      "11.772990226745605\n",
      "11.501056671142578\n",
      "11.533012390136719\n",
      "11.529661178588867\n",
      "10.775308609008789\n",
      "11.284385681152344\n",
      "11.344291687011719\n",
      "10.913053512573242\n",
      "11.406673431396484\n",
      "11.226472854614258\n",
      "11.550840377807617\n",
      "11.053330421447754\n",
      "11.758875846862793\n",
      "11.576966285705566\n",
      "10.925431251525879\n",
      "11.498942375183105\n",
      "11.674653053283691\n",
      "11.194042205810547\n",
      "11.957523345947266\n",
      "11.324748039245605\n",
      "11.633035659790039\n",
      "11.13886547088623\n",
      "11.304032325744629\n",
      "10.999507904052734\n",
      "11.696223258972168\n",
      "11.415008544921875\n",
      "11.430438995361328\n",
      "12.287409782409668\n",
      "11.192953109741211\n",
      "11.510348320007324\n",
      "11.65648365020752\n",
      "11.763508796691895\n",
      "11.385865211486816\n",
      "10.963033676147461\n",
      "11.645039558410645\n",
      "11.062873840332031\n",
      "11.065972328186035\n",
      "11.485809326171875\n",
      "11.785000801086426\n",
      "11.474437713623047\n",
      "10.976773262023926\n",
      "11.284686088562012\n",
      "11.46615982055664\n",
      "11.198549270629883\n",
      "10.905675888061523\n",
      "11.243512153625488\n",
      "10.972996711730957\n",
      "11.24863338470459\n",
      "11.947792053222656\n",
      "11.90562915802002\n",
      "11.787156105041504\n",
      "12.19824504852295\n",
      "11.647866249084473\n",
      "10.79923152923584\n",
      "11.596548080444336\n",
      "11.813273429870605\n",
      "11.894621849060059\n",
      "11.37080192565918\n",
      "12.375272750854492\n",
      "11.646994590759277\n",
      "11.612020492553711\n",
      "11.497871398925781\n",
      "10.587485313415527\n",
      "11.408246994018555\n",
      "11.345107078552246\n",
      "11.647505760192871\n",
      "11.282463073730469\n",
      "10.633622169494629\n",
      "11.522172927856445\n",
      "11.232388496398926\n",
      "11.777213096618652\n",
      "11.371389389038086\n",
      "11.517127990722656\n",
      "11.463830947875977\n",
      "11.112298011779785\n",
      "11.417017936706543\n",
      "10.670040130615234\n",
      "11.388236045837402\n",
      "10.887371063232422\n",
      "11.965312957763672\n",
      "11.222844123840332\n",
      "11.12121295928955\n",
      "11.144694328308105\n",
      "11.010796546936035\n",
      "11.258905410766602\n",
      "11.233281135559082\n",
      "10.959603309631348\n",
      "11.095213890075684\n",
      "10.83968734741211\n",
      "11.70340633392334\n",
      "11.483579635620117\n",
      "11.45966911315918\n",
      "11.178322792053223\n",
      "11.565195083618164\n",
      "11.475860595703125\n",
      "11.575461387634277\n",
      "11.201839447021484\n",
      "11.035207748413086\n",
      "11.778822898864746\n",
      "11.77930736541748\n",
      "11.773494720458984\n",
      "11.127110481262207\n",
      "11.399662971496582\n",
      "11.3286714553833\n",
      "11.500598907470703\n",
      "11.294393539428711\n",
      "11.059407234191895\n",
      "11.329730987548828\n",
      "11.396286010742188\n",
      "11.48365592956543\n",
      "11.86900520324707\n",
      "10.74084186553955\n",
      "11.258077621459961\n",
      "10.860832214355469\n",
      "11.317699432373047\n",
      "11.766338348388672\n",
      "11.34469985961914\n",
      "11.492774963378906\n",
      "11.378103256225586\n",
      "11.093411445617676\n",
      "11.348358154296875\n",
      "11.631155967712402\n",
      "10.744114875793457\n",
      "11.545047760009766\n",
      "11.604902267456055\n",
      "11.6895112991333\n",
      "11.856401443481445\n",
      "11.344305992126465\n",
      "11.632857322692871\n",
      "11.067930221557617\n",
      "11.734371185302734\n",
      "11.413640975952148\n",
      "11.293557167053223\n",
      "11.401566505432129\n",
      "11.07015609741211\n",
      "11.399999618530273\n",
      "11.644381523132324\n",
      "11.180286407470703\n",
      "11.56062126159668\n",
      "11.96292781829834\n",
      "10.90296745300293\n",
      "11.573554039001465\n",
      "12.109332084655762\n",
      "11.748988151550293\n",
      "11.497297286987305\n",
      "11.237088203430176\n",
      "11.051264762878418\n",
      "11.289692878723145\n",
      "11.527901649475098\n",
      "10.944610595703125\n",
      "11.846879005432129\n",
      "10.975852966308594\n",
      "11.22134780883789\n",
      "11.57543659210205\n",
      "11.806794166564941\n",
      "11.333230972290039\n",
      "11.039397239685059\n",
      "11.267387390136719\n",
      "11.295945167541504\n",
      "11.109013557434082\n",
      "11.254630088806152\n",
      "11.973076820373535\n",
      "11.46845531463623\n",
      "11.991386413574219\n",
      "11.064608573913574\n",
      "11.691393852233887\n",
      "11.513376235961914\n",
      "11.604352951049805\n",
      "11.78005599975586\n",
      "11.015027046203613\n",
      "11.53372573852539\n",
      "11.566768646240234\n",
      "11.739089965820312\n",
      "11.531378746032715\n",
      "11.379512786865234\n",
      "11.196762084960938\n",
      "11.437607765197754\n",
      "11.116392135620117\n",
      "11.63302993774414\n",
      "10.714886665344238\n",
      "11.343565940856934\n",
      "11.032449722290039\n",
      "11.569189071655273\n",
      "11.535589218139648\n",
      "10.842369079589844\n",
      "11.835367202758789\n",
      "11.634268760681152\n",
      "11.242663383483887\n",
      "11.268167495727539\n",
      "11.496527671813965\n",
      "11.129326820373535\n",
      "11.118029594421387\n",
      "11.874883651733398\n",
      "11.405474662780762\n",
      "11.36015510559082\n",
      "11.318753242492676\n",
      "11.622502326965332\n",
      "11.598894119262695\n",
      "11.008538246154785\n",
      "11.023908615112305\n",
      "11.56154727935791\n",
      "11.597100257873535\n",
      "11.124787330627441\n",
      "11.6512451171875\n",
      "11.374185562133789\n",
      "12.023082733154297\n",
      "11.457651138305664\n",
      "11.415047645568848\n",
      "11.663515090942383\n",
      "10.560769081115723\n",
      "11.214889526367188\n",
      "11.275482177734375\n",
      "11.58939266204834\n",
      "12.011476516723633\n",
      "11.476645469665527\n",
      "11.741805076599121\n",
      "11.19870376586914\n",
      "11.135941505432129\n",
      "11.568868637084961\n",
      "11.29455280303955\n",
      "11.53863525390625\n",
      "11.490652084350586\n",
      "11.324196815490723\n",
      "10.833115577697754\n",
      "11.277074813842773\n",
      "11.290082931518555\n",
      "11.144131660461426\n",
      "11.648674011230469\n",
      "11.24288558959961\n",
      "11.450764656066895\n",
      "11.556806564331055\n",
      "11.662862777709961\n",
      "11.4762601852417\n",
      "11.119905471801758\n",
      "11.540367126464844\n",
      "11.641465187072754\n",
      "11.062114715576172\n",
      "11.266311645507812\n",
      "11.243914604187012\n",
      "11.102998733520508\n",
      "11.563950538635254\n",
      "11.70608901977539\n",
      "10.715365409851074\n",
      "11.03784465789795\n",
      "11.167479515075684\n",
      "11.135551452636719\n",
      "11.226435661315918\n",
      "11.452610969543457\n",
      "11.590036392211914\n",
      "11.451383590698242\n",
      "11.099559783935547\n",
      "11.29893684387207\n",
      "11.169425964355469\n",
      "11.441742897033691\n",
      "10.611441612243652\n",
      "11.214877128601074\n",
      "10.991631507873535\n",
      "11.52247428894043\n",
      "11.098855018615723\n",
      "11.332406997680664\n",
      "10.567564010620117\n",
      "11.385899543762207\n",
      "11.450372695922852\n",
      "11.330878257751465\n",
      "11.517436981201172\n",
      "11.29733943939209\n",
      "11.173073768615723\n",
      "10.808942794799805\n",
      "11.322396278381348\n",
      "11.114358901977539\n",
      "11.3277006149292\n",
      "11.673517227172852\n",
      "11.434412956237793\n",
      "11.069570541381836\n",
      "11.667435646057129\n",
      "11.449402809143066\n",
      "10.999463081359863\n",
      "10.748034477233887\n",
      "10.98351001739502\n",
      "11.077418327331543\n",
      "11.484582901000977\n",
      "11.437287330627441\n",
      "10.74166202545166\n",
      "11.459259986877441\n",
      "11.554939270019531\n",
      "11.400786399841309\n",
      "11.209856986999512\n",
      "11.064558029174805\n",
      "11.290900230407715\n",
      "11.529841423034668\n",
      "10.925166130065918\n",
      "11.135418891906738\n",
      "11.019771575927734\n",
      "11.044018745422363\n",
      "11.391959190368652\n",
      "11.155905723571777\n",
      "11.32514762878418\n",
      "11.352155685424805\n",
      "12.255091667175293\n",
      "11.109894752502441\n",
      "11.070781707763672\n",
      "11.00463581085205\n",
      "10.76679801940918\n",
      "11.150049209594727\n",
      "11.212830543518066\n",
      "11.480827331542969\n",
      "11.636571884155273\n",
      "11.85631275177002\n",
      "10.982526779174805\n",
      "11.06175422668457\n",
      "11.090259552001953\n",
      "11.18091869354248\n",
      "10.952134132385254\n",
      "11.595438957214355\n",
      "10.960199356079102\n",
      "11.462103843688965\n",
      "10.726630210876465\n",
      "11.692492485046387\n",
      "11.192017555236816\n",
      "10.79992961883545\n",
      "11.049240112304688\n",
      "11.33120059967041\n",
      "11.382078170776367\n",
      "11.178245544433594\n",
      "11.306522369384766\n",
      "11.474848747253418\n",
      "11.805071830749512\n",
      "11.452411651611328\n",
      "11.489158630371094\n",
      "11.616349220275879\n",
      "11.154806137084961\n",
      "11.273005485534668\n",
      "11.070292472839355\n",
      "11.249752044677734\n",
      "10.980785369873047\n",
      "11.302029609680176\n",
      "10.991643905639648\n",
      "11.11336612701416\n",
      "11.58266830444336\n",
      "11.555734634399414\n",
      "11.741256713867188\n",
      "11.316995620727539\n",
      "11.397628784179688\n",
      "10.995451927185059\n",
      "11.251618385314941\n",
      "11.099310874938965\n",
      "11.52779483795166\n",
      "11.08036994934082\n",
      "10.56461238861084\n",
      "10.992883682250977\n",
      "11.00277042388916\n",
      "11.434148788452148\n",
      "11.572659492492676\n",
      "11.488605499267578\n",
      "11.211383819580078\n",
      "11.522075653076172\n",
      "11.333540916442871\n",
      "11.427878379821777\n",
      "11.28863525390625\n",
      "11.166182518005371\n",
      "11.317496299743652\n",
      "11.045675277709961\n",
      "11.233272552490234\n",
      "11.292335510253906\n",
      "11.11531925201416\n",
      "11.458186149597168\n",
      "11.226585388183594\n",
      "11.505901336669922\n",
      "11.087616920471191\n",
      "11.379593849182129\n",
      "11.401098251342773\n",
      "11.98144817352295\n",
      "11.420598030090332\n",
      "11.25727367401123\n",
      "11.487125396728516\n",
      "11.258467674255371\n",
      "11.0345458984375\n",
      "10.792498588562012\n",
      "11.205175399780273\n",
      "11.28106689453125\n",
      "11.530069351196289\n",
      "10.992897033691406\n",
      "10.96617603302002\n",
      "11.469038009643555\n",
      "11.172472953796387\n",
      "11.071080207824707\n",
      "11.981435775756836\n",
      "11.1854248046875\n",
      "11.259471893310547\n",
      "10.757613182067871\n",
      "11.263580322265625\n",
      "11.200126647949219\n",
      "10.355939865112305\n",
      "11.499175071716309\n",
      "10.941146850585938\n",
      "11.399255752563477\n",
      "11.036507606506348\n",
      "10.762483596801758\n",
      "11.602667808532715\n",
      "11.442493438720703\n",
      "11.32918930053711\n",
      "11.623163223266602\n",
      "11.017791748046875\n",
      "11.740439414978027\n",
      "11.217880249023438\n",
      "12.164445877075195\n",
      "11.076416969299316\n",
      "10.735526084899902\n",
      "11.029706954956055\n",
      "11.235724449157715\n",
      "11.46511173248291\n",
      "11.364032745361328\n",
      "11.0784330368042\n",
      "11.626872062683105\n",
      "11.343710899353027\n",
      "11.307299613952637\n",
      "11.110565185546875\n",
      "11.34696102142334\n",
      "11.03559398651123\n",
      "11.256828308105469\n",
      "10.800376892089844\n",
      "11.931649208068848\n",
      "10.947013854980469\n",
      "11.217117309570312\n",
      "11.433996200561523\n",
      "11.25910472869873\n",
      "11.24093246459961\n",
      "11.55062484741211\n",
      "11.044402122497559\n",
      "11.383607864379883\n",
      "11.533075332641602\n",
      "10.923011779785156\n",
      "11.397679328918457\n",
      "11.06875228881836\n",
      "11.055718421936035\n",
      "10.81989574432373\n",
      "11.644753456115723\n",
      "11.284981727600098\n",
      "11.15005111694336\n",
      "11.540074348449707\n",
      "11.046109199523926\n",
      "11.240453720092773\n",
      "11.405555725097656\n",
      "11.167939186096191\n",
      "10.738966941833496\n",
      "11.24337387084961\n",
      "11.069554328918457\n",
      "11.52324390411377\n",
      "11.553759574890137\n",
      "11.543388366699219\n",
      "10.824607849121094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     30\u001b[39m model.train()\n\u001b[32m     31\u001b[39m total = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# xb: (B, T), yb: (B, T), zb: (Z_idx, Z_p) with shapes (B,T,K)\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# tweak aux_scale as desired\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 208\u001b[39m, in \u001b[36mGPUDataset.__getitem__\u001b[39m\u001b[34m(self, _)\u001b[39m\n\u001b[32m    206\u001b[39m X[i] = xseq\n\u001b[32m    207\u001b[39m Y[i] = yseq\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m idxs, probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_Z_for_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxseq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m Z_idx[i] = idxs\n\u001b[32m    210\u001b[39m Z_p[i]   = probs\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mDisciplinedZ.build_Z_for_sequence\u001b[39m\u001b[34m(self, seq)\u001b[39m\n\u001b[32m    137\u001b[39m     sc  = scores[order]\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# we must fill with globally-most-common tokens (no randoms), excluding existing\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_score_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cands.size > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np.array([], dtype=np.float64)\n\u001b[32m    141\u001b[39m     missing = \u001b[38;5;28mself\u001b[39m.K - cands.size\n\u001b[32m    142\u001b[39m     mask = np.ones(vocab_size, dtype=\u001b[38;5;28mbool\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 102\u001b[39m, in \u001b[36mDisciplinedZ._score_candidates\u001b[39m\u001b[34m(self, cands, contexts)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t, p \u001b[38;5;129;01min\u001b[39;00m probs.items():\n\u001b[32m    101\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m idxmap:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m             scores[idxmap[t]] += \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# tiny floor to avoid zeros\u001b[39;00m\n\u001b[32m    104\u001b[39m scores = scores + (scores == \u001b[32m0\u001b[39m) * \u001b[38;5;28mself\u001b[39m.eps\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36c6dd9b-dafc-4c7e-94e9-fde8dd10f186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3bb2a0530>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQC5JREFUeJzt3Qd4VFX6x/E3hYQESCAEEkoCgSCdSO8oEmkugmJ3V2Cxgyvi34IKdkFdG7uI7qrYYdWlrCgg0kGKlFClhyKdQBISSJ//c04yw0x6cGbuTO738zzjtJu5JxnD/HLKe3wsFotFAAAA3MTXXScCAABQCB8AAMCtCB8AAMCtCB8AAMCtCB8AAMCtCB8AAMCtCB8AAMCtCB8AAMCt/MXD5OXlyfHjx6VGjRri4+NjdHMAAEA5qJqlFy5ckPr164uvr693hQ8VPKKiooxuBgAAuAJHjx6Vhg0belf4UD0e1saHhIQY3RwAAFAOqampuvPA+jnuVeHDOtSiggfhAwAA71KeKRNMOAUAAG5F+AAAAG5F+AAAAG5F+AAAAG5F+AAAAG5F+AAAAG5F+AAAAG5F+AAAAG5F+AAAAG5F+AAAAG5F+AAAAG5F+AAAAG7lcRvLucrZtEz559L9EhTgJ08NbGF0cwAAMC3T9HykXMqWT385JF+tO2x0UwAAMDXThA/rBr8Wg9sBAIDZmSd8+BTED9IHAACGMk/4KLgmewAAYCzzhA9rx4eF+AEAgJHMEz4K+j6IHgAAGMuEPR9GtwQAAHMzTfiwstD3AQCAoUwTPuj5AADAM5gofDDnAwAAT2Ce8GG9QfoAAMBQ5gkfthpjpA8AAIxkvqW2ZA8AAAxlnvBBdXUAADyCecJHwTUVTgEAMJZpwsflGacAAMBI5gkfBej3AADAWKYJH0w4BQDAM5gnfDDsAgCARzBP+LC7zaRTAACMY57wYdf1QfYAAMA45gkfdrfJHgAAGMc84cMufTDsAgCAccwTPuz6PogeAAAYxzThw37chY4PAACMY85hF/o+AAAwjHnCh91tej4AADCOecIHVcYAAPAI5gkfdrfp+QAAwDjmCR/M+QAAwCOYc6kt2QMAAMOYtOcDAAAYxTThwx4VTgEAMI5pwgc9HwAAeAZTzvlIvZRtaFsAADAzU/Z8vLloj5FNAQDA1MwTPuxuHzt/ycCWAABgbuYJH3ZdH8z5AADAOOYJH3a3We0CAIBxzBM+WO0CAIBHMFH4oMIpAACewDThAwAAeAZThg86PgAAMI4pwwcAADAO4QMAALiVOcMHM04BADCMKcMH0QMAAOOYM3yQPgAAMIw5wwd9HwAAGMaU4QMAAHhJ+Jg+fbq0a9dOQkJC9KV79+6yYMEC2/MZGRkyZswYqV27tlSvXl2GDx8up06dEk/DsAsAAF4SPho2bChTpkyRTZs2ycaNG+W6666ToUOHys6dO/Xzjz32mHz//ffy7bffyooVK+T48eNy8803i6chfAAAYBz/ihw8ZMgQh/uvvvqq7g1Zt26dDiYff/yxfP311zqUKDNmzJCWLVvq57t16+bclgMAAHPN+cjNzZVZs2ZJenq6Hn5RvSHZ2dkSHx9vO6ZFixYSHR0ta9euFU9CxwcAAF7S86Fs375dhw01v0PN65gzZ460atVKEhISJCAgQGrWrOlwfEREhJw8ebLE18vMzNQXq9TUVHE1C+MuAAB4T89H8+bNddBYv369PPTQQzJixAjZtWvXFTdg8uTJEhoaartERUVd8WsBAIBKGD5U70ZsbKx07NhRB4e4uDh57733JDIyUrKysiQ5OdnheLXaRT1XkgkTJkhKSortcvToUXE1Oj4AAPDiOh95eXl62ESFkSpVqsiSJUtsz+3Zs0eOHDmih2lKEhgYaFu6a724GkXGAADwkjkfqpdi0KBBehLphQsX9MqW5cuXy6JFi/SQyejRo2X8+PESFhamQ8Qjjzyig4enrXSh5wMAAC8JH6dPn5Z77rlHTpw4ocOGKjimgsf111+vn3/nnXfE19dXFxdTvSEDBgyQ999/XzwN2QMAAOP4WDxs6Yda7aKCjZr/4ewhmMZP/6CvY+tWl5/HX+PU1wYAwMxSK/D5bcq9XTwsbwEAYCrmDB9GNwAAABMzZfggfQAAYBxThg+yBwAAxjFn+GDOBwAAhjFn+DC6AQAAmJg5wwfpAwAAw5gzfND3AQCAYcwZPsgeAAAYhvABAADcypThAwAAGMeU4YOltgAAGMeU4QMAABjHlOEjj44PAAAMY8rwwVJbAACMY8rwAQAAjGPK8MF8UwAAjGPK8NGtSW2jmwAAgGmZKnx0blxLX3cquAYAAO5nqvBRp0ag0U0AAMD0TBU+fMTH6CYAAGB6pgofVkw4BQDAOKYMHwAAwDjmCh8Foy7s7QIAgHHMFT4AAIDhTBU+mG4KAIDxTBU+rBh0AQDAOKYMHwAAwDimCh8+Pgy8AABgNFOFDysWuwAAYBxThQ/6PQAAMJ6pwocVHR8AABjHlOEDAAAYx1Thg/mmAAAYz1Thw4ry6gAAGMdU4YOODwAAjGeq8AEAAIxH+AAAAG5lqvBBhVMAAIxnqvBhxXxTAACMY6rwkZWTp68zsnONbgoAAKZlqvDxw/YT+vqtxXuNbgoAAKZlqvABAACMZ6rwcW+vGH19d9doo5sCAIBpmSp8BAf662tfVr0AAGAYU4UP34LMkcdyFwAADGOy8JGfPvLIHgAAGMZU4cOvoOsjj/QBAIBhTBU+rFM9GHYBAMA4pgofDLsAAGA8U4UPv4LwYaHnAwAAw5hy2CWX8AEAgGFMFT4YdgEAwHgmCx/510w4BQDAOKZcasucDwAAjGOq8OFTMOySy7gLAACGMVX4YM4HAADGM1n4yL9m2AUAAOOYK3xYy6uTPQAAMIy5wgdzPgAAMJzJwkf+NUttAQDwkvAxefJk6dy5s9SoUUPq1q0rw4YNkz179jgcc+211+pVJfaXBx98UDxrqa3RLQEAwLwqFD5WrFghY8aMkXXr1snixYslOztb+vfvL+np6Q7H3XfffXLixAnb5Y033hBPWmpLzwcAAMbxr8jBCxcudLj/6aef6h6QTZs2SZ8+fWyPBwcHS2RkpHjqsAtzPgAA8NI5HykpKfo6LCzM4fGvvvpKwsPDpU2bNjJhwgS5ePFiia+RmZkpqampDhdXTzil4wMAAC/p+bCXl5cn48aNk549e+qQYXXXXXdJo0aNpH79+rJt2zZ56qmn9LyQ2bNnlziP5MUXXxS3rnYhfQAA4H3hQ8392LFjh6xevdrh8fvvv992u23btlKvXj3p16+fHDhwQJo2bVrkdVTPyPjx4233Vc9HVFSUuAKrXQAA8NLwMXbsWJk/f76sXLlSGjZsWOqxXbt21df79+8vNnwEBgbqiztXu+Qx5wMAAO8IH6os+SOPPCJz5syR5cuXS0xMTJlfk5CQoK9VD4inVDhl2AUAAC8JH2qo5euvv5Z58+bpWh8nT57Uj4eGhkpQUJAeWlHPDx48WGrXrq3nfDz22GN6JUy7du3EaH62CqdGtwQAAPOqUPiYPn26rZCYvRkzZsjIkSMlICBAfv75Z3n33Xd17Q81d2P48OHy3HPPiSdg2AUAAC8cdimNChuqEJmnYrULAADGM9XeLvR8AABgPJOFj/xrej4AADCOqcKHdW8XyqsDAGAcU4UP62oXhl0AADCOucIHdT4AADCcqcKHdbULHR8AABjHVOGD1S4AABjPZOEj/5phFwAAjGOq8GErMkbPBwAAhjFV+GDYBQAA45kqfFBeHQAA45krfNh6PoxuCQAA5mXKImP0fAAAYBxThQ9f62oX5nwAAGAYU/Z8KEw6BQDAGOYKHwVzPhSGXgAAMIYpJ5wqDL0AAGAM0w670PEBAIAxTFnnQ2HYBQAAY5grfNh9twfPpBnZFAAATMtU4cPfLn3sPUX4AADACKZd7VK/ZlVD2wIAgFmZKnworeuH6OvMHGqsAwBgBNOFD3+//G85N5cJpwAAGMF04cOvYOQlhzofAAAYwnThwzrpNI+ltgAAGMJ04cM66ZSeDwAAjGG68OFfMO6Sm8eEUwAAjGDeng8mnAIAYAjThQ//gvDBxnIAABjDdOFj3cFz+nr2lmNGNwUAAFMyXfhIy8zR1xsS80MIAABwL9OFDwAAYCzCBwAAcCvCBwAAcCvCBwAAcCvCBwAAcCvThY8Jg1oY3QQAAEzNdOFj14lUo5sAAICpmS58ZGTnGt0EAABMzXTho2oVP9tti4US6wAAuJvpwkeA3+VvOTOHnW0BAHA304WPWzo2tN0+mZJhaFsAADAj04WPrk1q224v33Pa0LYAAGBGpgsf9hqHVzO6CQAAmI6pw8c7P+8zugkAAJiOqcPH1qPJRjcBAADTMXX4AAAA7kf4AAAAbmX68EGhMQAA3MuU4aNmcBXb7dRLOYa2BQAAszFl+LinWyPb7fHfJBjaFgAAzMaU4cPfrsT6kt0UGgMAwJ1MGT7sS6wr2bns8QIAgLuYMnzUrxnkcL/ZswskIzvXsPYAAGAmpgwfxXnoy01GNwEAAFMgfBRYtueMrN53VpIvZhndFAAAKjXCh50/f7xebvznGqObAQBApWba8DGmb9NiHz9y7qLMWJPoMAeE+SAAADiPacNHn2Z1Snzuxe93yfvL9uvbv51IlRYTF8pL3+9yY+sAAKi8TBs+4qJqlvr81KX54eOtn/bq60/WJLqlXQAAVHYVCh+TJ0+Wzp07S40aNaRu3boybNgw2bNnj8MxGRkZMmbMGKldu7ZUr15dhg8fLqdOnRJP4+/rU+Yxw6atkZ9/O+WwD4x1CEb1iHy36Xfb3jA5uXmy9kASQzQAAJTBXypgxYoVOlioAJKTkyPPPPOM9O/fX3bt2iXVqlXTxzz22GPyww8/yLfffiuhoaEyduxYufnmm2XNGs+ayOlXjvCRcDTZ4X7MhB/19YonrpVB763St0Oq+ktOnkUe/mqz7bhDU25wensBAKgsfCx/YFvXM2fO6B4QFUr69OkjKSkpUqdOHfn666/llltu0cfs3r1bWrZsKWvXrpVu3bqV+Zqpqak6tKjXCgkJEVeav+24LNh+Un7YfuKKX+PeXjHy0WrHIZkvR3eVsGoBcv5ilvSMDS/X65y5kCnn0rOkeWSNK24LAABGqcjnd4V6PgpTJ1DCwsL09aZNmyQ7O1vi4+Ntx7Ro0UKio6NLDB+ZmZn6Yt94d/lTu/r6cv/RZBk67cp6ZuYmHCt2ya7VjJGdpW+Lurb7alimahU/hwC09Wiy/HtVoq1XpVHt/F6k4qis6ONTdq8NAACVbsJpXl6ejBs3Tnr27Clt2rTRj508eVICAgKkZk3HyZwRERH6uZLmkaikZL1ERUWJp00+Lc3ZtNKLko369Ffb3jFvLtqtV868vzx/Mqsy9usttuBR3FCPcjo1Q2ZuOCJPfLtVrv37cknLzLni9gIAYLQr7vlQcz927Nghq1ev/kMNmDBhgowfP96h58OIAOJKau8Ye28s3KMvf7sutsix/1p5UAa3rSd//mi9DkXPDG4pt3ywVtcfsZqz5Zj8pVsjycuzyL9WHZSFO07qHpZa1QJKbIN1dI1eEwCAV/Z8qEmk8+fPl2XLlknDhpd3iI2MjJSsrCxJTnb8612tdlHPFScwMFCPDdlfjHB/nyb6eujV9eXbB7u75ZzW5bz2dh5PlUnzdsj6xHM6iCj2wUOZOHeHfLomUU9ynbJgt+4tefXH34q81qnUDPlx+wk91HP9OyvlwS836SGedQeT5Pfzjq8JAIBHTjhVhz7yyCMyZ84cWb58uTRr1szheeuE05kzZ+oltopaiqvmfXjihNPC39vhpIvSqHawnL+YLR1eXize5v/6XyUje8ZI9cD8Di31PahJrNe3ipDFu4oud35hSCt9fOFg4+/nI08NbOEwN6WkSbLh1QPoTQEASEU+v30rOtTy5Zdf6tUsqtaHmsehLpcuXdLPq5OOHj1aD6OoXhE1AXXUqFHSvXv3cgUPI6kP0Mbh1fR1reAqDs/d0K6eeIO//7RX2jy/SKYt2697NlTwUIoLHsoLhaq2/nronHyx7rDMWHNIz01Jzcgu8VyqxknnV3/WPS8AALis56Okv3BnzJghI0eOtBUZe/zxx3Xvh1rFMmDAAHn//fdLHHbxlJ6PwtQOt1OX7pPXbmorsXWr2x4/n54lZ9Iypf87K6Uy2P3yQFm6+7QeurEO81j1aFpbxvaNlR6x4XL6QoZ8sfaw3NElWhrUDJK4F3+SlEv54YS6JgCA1Ap8fv+hOh+u4CnhoyyNn/5BzEIt//3bzC2y9ff8pdV/aldP5m+7XBvl5/HXSMNaQWUO0wAAKi/ChxuYKXyUt2LskwOaS/Wq/rLlSLJMubmtHvYJrx4ovuWoJgsA8G5uKzJmZh+P6CRvL94rD13bVNfquK1TQ7m9c7T8sv+sfLjyoF4xUy+0qnyz8ai89uPuIh/UQVX8KlW9jtw8i0y2m/9x4EyaDiFKdFiw1KjqL7d2bFhkgisAwHzo+XDRB7F17xi14VxsoTofB18bLFm5eXrCpirDrnoILmRUniBSmq3P95fQIMcJvQAA78ewi4dJz8wRS8FOumrObqB//tyIzJxc8ff1FZVT1GTPg2fS5aVhrWX/6TS5YWp+8Ta15PXbTUf1c8VRX5vnUe9g2f59TyfpEF1TagUHMCQDAJUE4aMSmJdwTA9bTPxTK92LctP7a2zDGFYBfr6y99VBuvz6Xz7eIHtOXRBv848728tVETXYUA8AvBzhoxJStTveXLRHT+Dc+Fy87D99QSJDg2wFxdTbuHr/WR1CvBHLdQHAuzHhtBK6r3cTPYG1R9NwfT+2bo0iNVg6Nw6TxrWDpUVkiIzs2Vju+Nc62/Pv3XG17DqRv2Pwhysc63l4grUHkqR709pGNwMA4Ab0fFQyarM56zwKVSpdVSy171lYtvu03mlXWf9MPzmZkiFDp60RT0DvBwB4L3o+TMx+Auej8c1k85Hzcluny7sEX9u8jrw8tLW0qh8iESFV9WXVk33l202/y9Ql+8TTZOXkyfrEJN2rQxEzAKgc6PmAzaWsXHlp/k655qo6ElK1itz10Xrp1KiWngz61fojxX5NcICfXMzKdcr5p93VQTo1riVfrjss/VtFStuGobbeG7W/jnre3tLdpyThaIqM69eMVTMAYDAmnMLpvlp/WJ6ds6PI43d0jpLBbetJUICfHDidJk/P3u60c345uqv8+eP1tvtrJ1wn9UKDilSZ/eDPHWRgG+/Y/A8AKiuX7WoL87q7ayPZ+eIA3StSWJ+r6uhhEbXpnDPZBw+l++Sl8suBs0WOO56c4dTzAgBci/CBcqsW6C+f/bWLnhhqXeJ7bfO6DsdseKafLj2fOHmwS9rw2S+H5OCZND0XxOql+bvkWPIll5wPAOB8DLvgipy5kCl7T12QHk1r62W+7t58r0VkDdl98nJRNVUxdfbDPV12PgBA6Rh2gcvVqREoPWPDSwwe9p4Y0Fy+H9tL7u3lvE3l7IOHsvlIsjw6a4t8u/Go084BAHANwgdcRs0FaVgrSO7tHaNXrtgbElff6eebl3Bcnvhum9NfFwDgXNT5gMt8Nqqz3vTOusOv/fjek6o3ZOtxl5zXOtyjVsu0rFdD94p0aRwmocGOu+n+fv6iNKgZVK7eGwCA8xA+4DLqQ93P7nM9z256UVRYsOx+eaC0mLjQ9tjcMT1lmBOrrRZeLdMrNlz6tqgro3vFyMerE+Xl+bvkgT5NZMLglk47JwCgbIQPuE3hqc32FUsD/X11L4UrqY331CW8eoAOHsqHKw8SPgDAzZjzAbe5vXN+mfcuMWFFnvP18ZEqvu753/HRWQkO9z1swRcAVHqED7hNy3ohsum5eJl5X7ciz4XXCHAokX5T+wby1q1xbmlX7zeWybn0LDlwJo0gAgBuQJ0PGGrVvjPy95/2yuvD20qLyBD5ZHWinLqQIRMGtdRBYO2BJEnLzJH7v9jklvaodtze2bmVWgHADFLZ2wWVSW6eRQa8u1IOnU2XHLV8xsV+/FtvPf9kzNebpXa1QHl5WBuXnxMAvF1FPr+ZcAqPp5bqLny0txxKSpf4t1e6/HyDp67Sm9X9uP2kvt+hUU0JqxYozSNqSGRoVZefHwAqO+Z8wCv4+/k6rJZx9XyQDYnnbbcf+89WGfHJBun1+lJ9f8lvp/RwkXL6QobkuaE3BgAqE3o+4DXsP+OHtW8gr/34mySlZ7nkXJ+sSSzymBrySb6YJaM/26jvfzqqs4yc8as0Ca8mtaoF6MJpXZvUdkl7AKAyoecDXiM6LNhhKMaIyqQLd+QPxSgqeCgHz6bLpsPn5fZ/rXN7ewDAG9HzAa8RFOAnW5/vL1UKyqYG2JdPdZOnZ293+zkBoLKh5wNeJTSoigQH5GfmaXd30NVK374tTq6KqG500wAA5UTPB7xW++ha8uuz8Xr4ZUDrSMnKyZOFO0/KO4v3yuSb29rmZrjT3lMX5KoI15aJBwBvR50PVDrqf2kVSE6kXJLjyRmSeDZd/u/brW47/wPXNNFF0gDATFIr8PnNsAsqHetE1HqhQdKxUS25pWNDt57/wxUH3Xo+APA2hA/ABdYfTDK6CQDgsQgfMC1Vn8NV1LLbp77b5rLXBwBvRviAKRQeenno2qbyj7vau/Sc/9l4VBo//YOehKoqoialZbr0fADgLZhwClNQJdBPpGbI8eRLsnzPaXm031US4O8rMzcckd/PX5Rpyw64vA0hVf1lw7PxsvnIeencOEyq+JH9AVQe7GoLXAHVS+FOM0Z1lr7N67r1nADgKuxqC/xBo3vFyMeri+7v4kyjZvwqPWNrS9M61fW+MY/2ayYRIeyaC6Dyo98XKPD+3R309Zu3tJOJf2pV5Plx8c2cfs41+5Pk87WH5ev1R+SRmVuKHS6avvwAq2cAVCqED6DA4Lb1ZO8rg+TWTlH6/rwxPR2eHxd/lXzw544uO/+GxHNyOjVDfth2QnJy8/RjP2w/Ia8v3M2mdQAqFcIHYEdNQrWKi6opLSIdS6XHtyw6R+Pm9g2cdv5B762SMV9vlhlrDsnFrBzZd+qC014bADwF4QMoRW6e43xsfz9f+futcQ6PPXBNU6edLyk9S19/veGItJq0SKYu3e+01wYAT8GEU6AUucUsBlM1Q9Tlye+2SlJalkt21FX70RS2bM9pOXMhU24rGBYCAG9F+ABKoSZ8luSNWxx7QFxNrY5R1H41aoUMAHgrhl2AUgxqW09fx5RRiv2loa3d1CLRvR8A4M3o+QBKoZbXqkmnvWLDSz3unu6N9VCMr4+PDge931jmsjbl79lbtq1HkyU7N086NQ5zWVsA4EoQPoBSBPr7ydCry7eaJTgg/9cpKixY/Hx9HCarvjKsjTw3d4dT2qSW3UaHBct3D3aXRTtPytqDSbo42bXN68pnvxySJwc218MyQ6et0cdve6G/hFSt4pRzA4AzED4AF5h1fze59YO1tvuBdkt4neHIuYvy7pJ9ujiZlVqeq/yaeE7vIWOVnJ5N+ADgUZjzAbhAp0a1HO77+5V3sKT8Vuw5U+zjFzJzpOWkhbb7eZ61fRMAED4AV/DxuRw2rm8VIZEhQUWO2fnigD90jmPJl8p1HOEDgKdh2AVwkUXj+khGdq6ulFrc5tHVAv2ld7NwWbXvrEvbUcpqYQAwBD0fgIs0j6yhg0fhnhB77qjXseNYiq5XooIQAHgCwgfgJs/d0LLUvWRcZdx/EuSuj9ZJ6+cXSfLFLPl87SH539bjLj8vAJSEYRfATe7t3URqVPWXp/673fZYtYLlua627uA524qY95bs07dvjKvvlnMDQGGED8CNbukYJRezcqVzQeEvXx/Hgmbv/pwfDFzlTBrVUQEYj2EXwI1U8bFRPWOkTYNQfX9Y+/wCZl1jwmRc/FVy4LXBLj3/qn1nyrVvDQC4EuEDMJCqhrr9hf4y875utnAy7GrXDYccPXepyI69qlDZoPdWycmUDJedFwDsMewCGKxGoeqj79x+tYy9LlYyc/Lkx+0nZNqyAy457+/nL8ncLcdsc0BeX7hbnxsAXI3wAXgYtSw3tm4N21LctQeSZPORZKefp+/flzvcn7PlmMTWrS5j+sY6/VwAYI9hF8CDVa3iJ7Mf7um28725aI/t9u6TqQ5zRADAWej5ALxc1Sq+kpGd5/TXHfjuKtvtr+7tKj1jwyv09aqmiCoB37p+/uRaALjino+VK1fKkCFDpH79+rp7eO7cuQ7Pjxw5Uj9ufxk4cGBFTwOgGPZLc61Fyu7oHO3Uc+w6nlrksbs/Wm+7nZ2bH3Q2Hjon87eVXKys2+QlcsPU1bLlyHmntg+ACcNHenq6xMXFybRp00o8RoWNEydO2C4zZ878o+0ETO2DP3eQ2tUC5Mt7u8rnf+0iYdUCdJGwdRP6OX3juA9XHpDTF4qufEnPzJEF209Is2cXyLyEY3LLB2tl7Ndb9PBMcay9MSv2MnQD4A8OuwwaNEhfShMYGCiRkZEVfWkAJRjYpp4MaB1p2yNm88Trbc/lFqrX0bZBqNzWOUomzt1xReeal3BcXwo7lZohD321Wd9+dFaC7fHfz12SFpEh+vahs+ny5H+3yZ/a1bM97yPF72sDwLxcMuF0+fLlUrduXWnevLk89NBDkpSUVOKxmZmZkpqa6nABUFRJm9MV7vlQG9r9pVsjp5+/cMgpzvhvEmRD4jmZNG+n088PoPJwevhQQy6ff/65LFmyRF5//XVZsWKF7inJzS1+R83JkydLaGio7RIVFeXsJgGVWk6updgwUsXPuT0O17+z8orKt5eQmQCYmNNXu9xxxx22223btpV27dpJ06ZNdW9Iv379ihw/YcIEGT9+vO2+6vkggADlVzPYsUiZFGQRVSNk98kLLj+/Ot3mI+clOyfPoYJqRXpMAJiLy5faNmnSRMLDw2X//v3Fhg81P0RdAFyZsX2byf7TabJsT/7ETutHvSrVbrX75YHSYuJCl5z/vs83lvq8qqDasl6IDGgdUeLQEQBzcXmRsd9//13P+ahX7/IENADOExpcRWaM6mK7bykYdlEFyqzU7SWPXyO9m1WsVoezPPjlpjJDCgDzqHD4SEtLk4SEBH1REhMT9e0jR47o55544glZt26dHDp0SM/7GDp0qMTGxsqAAQNc0X4AhVh7Pl4f3lYa1grS19ZhmPiWEYa16+ffTht2bgBePuyyceNG6du3r+2+db7GiBEjZPr06bJt2zb57LPPJDk5WRci69+/v7z88ssMrQBuYp1iofaHWf3UdQ7PXdu8jjGNKnApK1cXHVu9/6yM6hkjj87aIrd0bCg3d2hoaLsAuJePxdpH6yHUhFO16iUlJUVCQvJrBwAo2/j/JMjsLcdk/iO9pE2Dkkuaq5LntYKrSKtJi8Tdrm8VIYt3ndK31fQP678+h6bc4HDcD9tOSGpGttzZxbnVWwF4xuc3e7sAlcRbt8XJS8PaSPXA0n+tG9QMEqNYg4di/2fPZ78c0nNVRvaM0ffHfJ1fzKxXbLhEhQW7v6EAXIpdbYFKQq0kKSt4FKdx7fwP97/2jJGdLxozN+v5/+2UF77fJVOX7LNNmFWSL2Yb0h4ArkX4AEyuf+tISZw8WCYNaSXVAv1lTN+mhrXl7cV75VTq5UJlzt63BoBnIHwAJhVSNb+XRK2Asa+/MfTqBrbbI3s0dnu7er2+9A+Fj7w8i54vAsBzMecDMKmVT/bVFUnbNnScnBpS9XLF1PDqAW5vV45dRVTrzX8u3afnftgHI4evyc2Tmb8ele5NwmTi3J2y9mCSLP+/a6VxeDV3NRtABRA+AJOqGRygL4VFhlaVJwc2l+AqfnJrpyj5+097HZ5XhVPdVzHdIluPJtvaYB8+MrJzZefxVGkfVVO+Wn9Ezxux99/Nv8vj/Zu7q6EAKoDwAaCIh6+Ntd3e9Fy8dHzlZ9v9FU/0ld5vLHNLO4ZPX+twP+VStmRm50rdkKpy/xebZOXeMzJhUAvZdSK11B4UAJ6FOR8ASmW/R0yLyBoO8zA+uqeTW9sS9+JP0uW1JZJ8MUsHD2Xygt26B6S4uR8APBPhA0CpfO3Cx99vjXPYpfYagyqmXv3SYof7amO9wthNF/BchA8ApbLfhzasWoA0ql1N71LbuXEt8ff1kVs7emZp9I9WJ8rmI+cl5WK2nL6QIQfPFA0ohUu/f7DigOw8nuK2NgJmRXl1AKVKz8yR1s/nl2Jf8/R1ukKqGtJQq3PVEl31T0h6Vq60KTjGk/36bLzUqREoc7cc00Hjw790lOiwYP19NH76B9tx1nLvKrR8ufaw3NElWupXsDLs5B9/k9MXMuXt2+IcljIDlRXl1QE4TXCAn7SPrql7BuqFVC0yFFNcZdUO0TVl85Fk8TRqeEaFj3H/yd+Ve/RnG4sdsrF6+MvNsvHweZm/7YQs/b9ry3z9389f1CuI1M/jw5UH9WP392mie4oAXEb4AFAqFS5mP9RD78ViHzoKm/SnVvLS/F16GObNW+Nkx7EU+WjVQZmbcFw8xZ3/XqeDlFVpwUNRwUM5eDbd9pgalvH18SkSKA4npcs1by7XwWOHXZl6tSQYgCPCB4ByBZCyRg5G9WysJ6DG1M4v7KV21p0yvJ1D+GhUO1gOJ110dXNLteUKe2TU3JEfd5yQCbO36/t7Xhkogf5+8vOuU9I4PFjWJ57Tj6dl5sgFuwqr5Zn3qoauVC+M8vGITgzToNJjwikAp1AfmE3rVHfoHalaxc9hSEbVCPEG2bl5smpf/lJeq7iXfrIFDyUjK082Hjon936+UeLfXql7Q6w+XJE/5FJSifgNief0a6m6JUpSepYs3X1aX86lZ7nouwI8Bz0fAFyqRlV/3RvgTZo9u6DMY1So2Pr75ZUx9n0VKkzYjivo+vhi3WE5cDpNnh/SSm77ML94msppr97Ulg30YDqEDwAuVVk/V99avMfh/tN2vSL2oya5BT+AiXN36OsBrSNtzx1KujyX5PLXMuSCyo/wAcAwfa6qY6tUWliAv69k5eSJp/py3ZFyHaeyh5ovYj/p1SrBA1cEAe7AnA8AbvXKsDa2203sdp2dN6an7XZ49UCJCAkUbzVzw+VgooZUnp69rdjjVH2UK6Hqj/x6KH+Ca+G5JLtPFi01D3gawgcAlxrQOsIhaPy5WyPZ+Fy8LuR1W6co/VjHRrUkLuryEtjXh7eVyIKaIkqXxmHirUNNqsz7gh0ny/iCyzdV8bOLWaXPkeny6hK59YO1su5gku2xEymX9FySge+uuvKGA25C+ADgUk8Pailv3tJO/vNAd4eeDaVV/RAdRP5zfzeHrwmq4idv3Xq19IoNly9Gd5G7u0WXeR5VzMsb57wUrgPyr5UHpdWk/Gqx1gLUJe1Ts3rfWdvto+cuFXuMGvL5fO0hSUrLrGjTAZdhzgcAlwoK8JNbC3o4imMNIvYah1fT5cy/vLer7cP35fm75Gxalu5JOX8xWw8xWO18cYBUC/SX06kZHlXUTBn16a+lPr/2QJK0rl+0Auqjs7bIvITjeifhU6kZsmhcH12ddfqKA7ZjsvPy58RMW7Zf3lzkOAHWavw3CbJk92mZvfmYzLUb2gKMRPgA4DEWP9ZH174ovI+Kn6+PbHzuejmefEkiQqrqZa0ZOblyOjVTcvIsOngoMeHVxduUFE5U8FB2n7ygr7u8tkTiW0bIz7+dcqgnck/3xiUGD0UFDyXhaPkmt6qemMe/3Sr9WtSVmzt45qaB8H6EDwAeo1lEjVKftw8lwQH+0jjcXP+E2QcPK9XrURF7T12QWRuOSot6NeSGtvV0Ibj//HpUusSESWzd6vLlusPyw7YT+kL4gKuY6zcXQKVWvWr5/0lrXDtYDhlc6t0Zki8WrYiqJp6qCbv/2+o4BLX/9AXp/85K2/0lv53Sy52fnZNfg0RNAk62WxZsncg6ad5OXT6/R9PwMtuj9vQ5eu6iDGpbz/bY+fQs3fOizqV6sQDCB4BK464u0bJ09ym5rkWE3NKhoS6JXljdGoGy8sm++i9+NYzz+DdbZa3dqhFvY1/W3cp+Pow9VQbe3qKdp2xDVoqqq1I4Gzz93+2yYu8ZWbzrlHw/tpfuMVl/8Jy0bRAqocFVbMe9vnC3rNhzRnadyF/qq+aXXF2wgmnotDVy5NxFXd11VM+Ycn9vaidlNWcIlQ+rXQBUGuqD6qt7u8noXjH6g1GtslETNmcVrKZREzs3PBuvg4d1GOejEZ3Em/n/0Z4Eu4U0vd9YKlOX7nfoVVE9H1ZD/rlarn7xJ/nzx+tl6LTVDi8zffkBW/CwDu9YqeChvPj9rhKboY63n5cye/Pv0nLSQl2WHpUPPR8AKi21ysa60kYNKRTH/i//0njCjrzF+aOre+wX8Z5KdVyO2/W1JRJjVwjOvjCaGrLKzMnVO/sWu4y3AmX1D55Jsw0HqaXXagXU+G+22srS/6VboxK/9kjSRfH38ykySRmejZ4PACiHh69tKl1jvKvYWXnM2XKsxOcyc/Jsq22K0/y5hfLmot3S8ZWfizxX2mZ5anhHLQFWvRvKLR/kb7SnqKGwwr5Ye0ju+vc6SS+0QeH+02nS581l0mPKUtsGfvAOhA8Apndzhwb51+0byCcjO+lVIIX5iI9MvbO9XhVSkq8K6pKYybRll+uO2FOTXfeUEFzu/midrjti7d04Z7cLsMoshYPExHk75ZcDSfLJ6kSHx+PfXmG7rZZcl0b1zpxMyRBXW38wSeYllBzokI9hFwCm99pNbWVIXH3p3qS2ng+iJqxOUytinv7h8kE+omuMfPNAd9vjf7suVrYcTZZV+87K/8b2lHYNL5eINzsVFga8u1IeKFR5Nu7Fn3QtF6vCgUBFiP/7Nj+UFJaa4bgSpzw9LaqXZcuR83L7v/I39Nvx4gCpXsZQmypv/8GKgzKwdaSuwlsWtbonpGoVPc/Iep4WkSHSPLL0peNmRs8HANNTgaNv87q2iajF8SmhLslno7rI7pcHEjxK8OHKgw737YOHssduYqryz6X7ZXYJQ0Gqd0MNvRS32/GT322zlaO399zc7bZAoPx+vux5O+/9vE+mLtkng6eWvU+OCk+931hWZGWVCiSu8vrC3fJSKZN3vQHhAwBKMPvhHvpaLSjp3zqyyPPNIqqLr69PqaGlsAWP9nZqG73dA19sLLOQmlXqpRxp/fwiueq5BXqJdOFhnkHvrZLDSen6g7/v35frgmnfbMyfV2JVUJHeNhQzbtYWhw36lJ3Hy78zsP0KnW82HrXdzi1rU58rpCrQqpVFn6xJ1GX3y6J6fb4vVO/FEzDsAgAl6BBdSxInD9Z/cVfxu/y32k+P9ZETKRm6a72wdg1DZdvvKQ6PvXVrnC5ZrlQkqJhBRnbRXozSCphZ/bdgsqo9NTn2mjeX2+4/Nze/eJq9ifN2yD3dG8mWI8l6GbGqdaJWDFlXQ02YvV1W7z/rUEG2f6sIUeVUYusWN4xiceh9sSppAqx1n6KOjWrpob6Kss80xfUAFXbT+7/o68a1q0nbhqHiKej5AIBS+Pj4OAQP5aqIGnLNVXWKPd66GZ690KDLxbjszbPb6G389Vf94bZWdoWHaK7EpsPn5dFZCfLpL4d08LDvBVEf5jM3HHE4Xu2bc/07K3WBtl/2n5UpC3Y77ERcUgdHSfNff9h+Qp/7kZlb9DCR/VCRGsL5aNVB2VrOfXgq0rmSmJQunoSeDwBwohrFTGbs0KiW7bb9h419OfihV9eXtxfvdUMLURw1ZGOtyFqSuz5ar68vZeVIfKsIaVUvRIeJ4mxITJJtvyfLY9df5dDbtctuSCdmwo/6WvWuqZB74z9Xy+kL+TVTVDXZ4noq7CfWWooppqKWH6sJu6qMfbBddVg110b1unhKeXvCBwA4kfoQOfDaYLn3s19l2Z4ztp4PFUrUB0d0WLD+kFN/ZdcLrerwtY9ff5W8VUoAUR8mFwuKfCnqw8++qiiunPrQ/2lXyfNN7H229rC+lHWMdQPER66LlX2n06RZ3erywYqiS5PPpmXJhYxsW/BQVu0/U2b4UGGisL/N3FLs/xOqWNu8Lcfku4fy5zEZjWEXAHAy9dflpCGtdbiY9KdW+v7GifGyedL14u/nK7Mf6iHzH+kl/r6X/wkOquInj/RrJgGFhnjs/fsex1Lw88b21BVBrdSHGzzLvtMXZMrC3XrZcZNn8ns6Clu086Rc99blmiUl7dlTeMLssGlr5LcTqQ4ra5LSi6k2W2Dj4fPiKej5AAAXUGXJf3n6Ot0Toqgy5FZqhYwS4OsjL97YWi5l50rdkPxekGdvaCnP/2+nHoYZ0q6+nEjNcCgv/vP4PlKjahXdla/moqhS5HtfGSQHzqRJtQB/XfFTua1TQ73SY1x8Mz25cn1ikm1yp6rW+uTAFo51TOAS87cVPyxjr7iJsdklTCa17/lIzcjRw0XKP+9qL/1bRVZoHoiRCB8A4CLW4FGaET0aO9xXKzF6NK2tw4vqJSmsuBUXAf6+0rJe/sqbMX2b6nByb68YGdkjRm+sp5qRlZuny6ErxX0+qeNKK6UO93pr8V7ZdOS8ZGbnyReju8j+M2m6wuvf+jUr9vixX2/Rz5WVPdQqHGv4NRLhAwA8LLCo4mVX6okBLWy37atz2ve8WP86rhVcRc5fzC/6tXBcH3pCPMzygjlDS3aflge+2KRvF65bYm/RjpPFFlorXGF21gPdpHV9Y5fdMucDAEymQc38IZ7IUMedYK29J+WhemZuvII6Fag4a/Aoz1JkNXm1NBcyc+SGqatl0+FzYiTCBwCYxIxRneWvPWPkji7R+v60u9pLtyZhtg3x2jUo+tfwh3/pKO2jiy5B7RlbW2+0t/ixPvo1C6tfaCVPWZ4Y0LxCx+OPGT798k7CRiB8AIBJqP1rJg1pZSua1qROdZl1f3fpGRuu74/pG6tX24zs0VhPlv36vq4yoHWkxJWyb40aIlKvqSa+2ouoYPi4qX0DubtrfihS3r4tzna7QU3HHhp4P8IHAECLrh0sO18aIC/c2Frq1wySHk3zQ0nXmLAixxadWuD4wNQ72lfo3P5+PvLqTW0lpKDwWrcmtW3PtS7HzrLwLoQPAIBN4VLyysA2kfL+3R1k5RN9ZXDbyGJX6bxz+9X6emzfWL1tfVRYsKx5+jr570Pdy3Vea82TtRP6yboJ/XT4KW55qTM80KeJU18PFcdqFwBAmStwBretp29Pu6uDpGXm6OW89no3q6Prjahlv/bDJeUdMrGW/a4W6K8v9rJySw4f8S0jSt0Jtziq5PmHKw9W6GvgXIQPAECFgkjh4GFlHzyKM2NkZ70bcN8WdaT75KUOz5VWesK+4NagNpGyIfGcJKXnr+r49z0dJTvXontH1IZst/9rXZnfQ6BdO1VVWFX6HO7FsAsAwKXmjukp/7izvfRtUVfu6hot9UKDJGHS9bLiiWttxxS34Vlcwd4mqtqr1bt3XC1PDWrhEIZU6FEVX7s2qS2Hptwgo3vFlLjh399vjXMo/vbUwMuvBfeh5wMA4FJqI73CO8bWDA7QF1UGXgUHtQFbYbMf7imZOblS1d9PdhxPkaujauliaeHVA0o938Q/tZIHrmkiXV5dYts3Z/uLAxyOia1bXQ4npUv3prX165VVHwPORfgAABjmjVsuL6ktLH9b+PyPqVeGta3Q69atUVVu7tBAZm8+pveyKWzho731cE1QgJ98NKKz3qTNWt7+84Idafe8MlDOpWfJfZ9vlB3H2D3YmQgfAACv0qyY/W2K8/rwdroAWqtiKreqfXOsFeftS5K/MKS1XubbrmGo7mVRQ0RzHu4pFzJypP87K5zaQ3JH5yg5eDZdz2FRggP85GJW7h96TTXk9PHqRPF0zPkAAHgVtYx33pieeulvWcuG2zQILXMjNfu1NOpYtbKnYa1gh9cJqxYgQ69uUOzX7355oGx/oX+52z+qZ2P5fmwvmTK8ndi3rM0V7reidi4ube6MJyJ8AAC8TlxUTV0UzRnKuxzYvgS89TO+d7NwPWdFrQBSy5ALe3Jgc2lcqJ3PD2ktbQsm09oHn17N8ou6KfdXoBbJ8A4NbbfVXNomdaqJpyN8AABMLSKkqsy8r5vMf6RXqcepkKECiNpQb9Nz18urN7WRf955OXDc0C6/Foq9e3s1kX/e1cG2DPnZwS0dnrduzte0TjXJzbscRZ4Z3LLE5cf/vqeT3NklqqBNvronyMrPx0d+/FvvIgHkmwe6S+LkwTKieyNpWCuoQj01rsCcDwCA6alVL+Wh9r+xurtro1KPVfNGVOhQQz+qAFtx7uoSLU3Cq0nrBqHy70KFz9Y/Ey+dX/3ZIUB0KSh1f32rCL1MWAUie74+Pvox9fyHK/Jfb/+rg/QcF+XFoW3khRstDsuNjUDPBwAATvLcDfk9G20bhMr/xpbek2KdY9IjNlxCg6ro5b/26tQI1IHDqn5Nx8361FLlwuGjTYPiJ9faMzp4KPR8AADgJPf2bqKHUuqGVGxXX0V93dm0TOnYqJZYqZ6O2Q/3kJSL2Q6TYAtTQy3bjyXrXYgVH4eprJ6H8AEAgBNdSfCw9oKo8FJYh+jLYaQkreqH6Iu3YNgFAIBKpm2DK1u267HhY+XKlTJkyBCpX7++HjeaO3euw/OqWMukSZOkXr16EhQUJPHx8bJv3z5nthkAAJRicNtIeWN4O1k4rrdUivCRnp4ucXFxMm3atGKff+ONN2Tq1KnywQcfyPr166VatWoyYMAAycjIcEZ7AQBAGVTnwG2do6RFZEjlmPMxaNAgfSmO6vV499135bnnnpOhQ4fqxz7//HOJiIjQPSR33HHHH28xAADwak6d85GYmCgnT57UQy1WoaGh0rVrV1m7dq0zTwUAALyUU1e7qOChqJ4Oe+q+9bnCMjMz9cUqNZWdAwEAqMwMX+0yefJk3TtivURF5ZeMBQAAlZNTw0dkZH5xk1OnTjk8ru5bnytswoQJkpKSYrscPXrUmU0CAACVOXzExMTokLFkyRKHYRS16qV798slYu0FBgZKSEiIwwUAAFReFZ7zkZaWJvv373eYZJqQkCBhYWESHR0t48aNk1deeUWaNWumw8jEiRN1TZBhw4Y5u+0AAMAM4WPjxo3St29f2/3x48fr6xEjRsinn34qTz75pK4Fcv/990tycrL06tVLFi5cKFWrXlm5WQAAULn4WFRxDg+ihmnUxFM1/4MhGAAAvENFPr8NX+0CAADMhfABAADcivABAADcivABAAC8t7y6M1jnv1JmHQAA72H93C7POhaPCx8XLlzQ15RZBwDA+6jPcbXqxauW2ubl5cnx48elRo0a4uPj4/RUpkKNKuHOMl7Pw/vj2Xh/PBfvjWczy/tjsVh08FCFRX19fb2r50M1uGHDhi49B2XcPRvvj2fj/fFcvDeezQzvT2gZPR5WTDgFAABuRfgAAABuZarwoXbQff755/U1PA/vj2fj/fFcvDeejffHCyacAgCAys1UPR8AAMB4hA8AAOBWhA8AAOBWhA8AAOBWpgkf06ZNk8aNG0vVqlWla9eusmHDBqObVCmtXLlShgwZoivcqQq1c+fOdXhezW+eNGmS1KtXT4KCgiQ+Pl727dvncMy5c+fk7rvv1sV4atasKaNHj5a0tDSHY7Zt2ya9e/fW76eqHPjGG2+45fvzZpMnT5bOnTvr6sF169aVYcOGyZ49exyOycjIkDFjxkjt2rWlevXqMnz4cDl16pTDMUeOHJEbbrhBgoOD9es88cQTkpOT43DM8uXLpUOHDnp2f2xsrHz66adu+R692fTp06Vdu3a2QlTdu3eXBQsW2J7nvfEcU6ZM0f++jRs3zvYY708FWUxg1qxZloCAAMsnn3xi2blzp+W+++6z1KxZ03Lq1Cmjm1bp/Pjjj5Znn33WMnv2bLWKyjJnzhyH56dMmWIJDQ21zJ0717J161bLjTfeaImJibFcunTJdszAgQMtcXFxlnXr1llWrVpliY2Ntdx5552251NSUiwRERGWu+++27Jjxw7LzJkzLUFBQZYPP/zQrd+rtxkwYIBlxowZ+meWkJBgGTx4sCU6OtqSlpZmO+bBBx+0REVFWZYsWWLZuHGjpVu3bpYePXrYns/JybG0adPGEh8fb9myZYt+v8PDwy0TJkywHXPw4EFLcHCwZfz48ZZdu3ZZ/vGPf1j8/PwsCxcudPv37E3+97//WX744QfL3r17LXv27LE888wzlipVquj3S+G98QwbNmywNG7c2NKuXTvLo48+anuc96diTBE+unTpYhkzZoztfm5urqV+/fqWyZMnG9quyq5w+MjLy7NERkZa3nzzTdtjycnJlsDAQB0gFPULp77u119/tR2zYMECi4+Pj+XYsWP6/vvvv2+pVauWJTMz03bMU089ZWnevLmbvrPK4fTp0/pnvWLFCtt7oT7svv32W9sxv/32mz5m7dq1+r76B9PX19dy8uRJ2zHTp0+3hISE2N6PJ5980tK6dWuHc91+++06/KBi1P/nH330Ee+Nh7hw4YKlWbNmlsWLF1uuueYaW/jg/am4Sj/skpWVJZs2bdLd+/b7x6j7a9euNbRtZpOYmCgnT550eC/UPgBqGMz6XqhrNdTSqVMn2zHqePWerV+/3nZMnz59JCAgwHbMgAED9BDC+fPn3fo9ebOUlBR9HRYWpq/V70l2drbD+9OiRQuJjo52eH/atm0rERERDj97tXHWzp07bcfYv4b1GH7fyi83N1dmzZol6enpeviF98YzqGEVNWxS+GfI+1NxHrexnLOdPXtW/yLbv+GKur97927D2mVGKngoxb0X1ufUtRoLtefv768/IO2PiYmJKfIa1udq1arl0u+jMlC7R6vx6p49e0qbNm1sPzsV6FT4K+39Ke79sz5X2jHqH9lLly7puT4o3vbt23XYUPMH1LyBOXPmSKtWrSQhIYH3xmAqDG7evFl+/fXXIs/xu1NxlT58ACj+L7gdO3bI6tWrjW4K7DRv3lwHDdUr9d1338mIESNkxYoVRjfL9I4ePSqPPvqoLF68WE9yxx9X6YddwsPDxc/Pr8isY3U/MjLSsHaZkfXnXdp7oa5Pnz7t8LyaDa5WwNgfU9xr2J8DJRs7dqzMnz9fli1bJg0bNrQ9rn52apgyOTm51PenrJ99SceoFRyV6S83V1B/PasVDh07dtSrk+Li4uS9997jvTGYGlZR/y6pVSiqJ1ZdVCicOnWqvq16J3h/KsbXDL/M6hd5yZIlDl3O6r7q3oT7qKES9ctl/16o7kQ1l8P6Xqhr9Qusftmtli5dqt8zNTfEeoxa0qvGWK3UXyTqr0aGXEqm5gCr4KG68tXPtPDQlfo9qVKlisP7o+bRqOWB9u+PGhqwD4jqZ6/+cVTDA9Zj7F/Degy/bxWn/r/PzMzkvTFYv3799M9W9UpZL2pemioJYL3N+1NBFpMstVUrKj799FO9muL+++/XS23tZx3DebPB1TIydVH/e7399tv69uHDh21LbdXPft68eZZt27ZZhg4dWuxS2/bt21vWr19vWb16tZ5dbr/UVs0sV0tt//KXv+hliOr9VcvTWGpbuoceekgvc16+fLnlxIkTtsvFixcdlguq5bdLly7VywW7d++uL4WXC/bv318v11VLAOvUqVPscsEnnnhCz/ifNm1apV0u6ExPP/20XnmUmJiofzfUfbXK66efftLP8954FvvVLgrvT8WYInwoar20+h9D1ftQS29VDQk437Jly3ToKHwZMWKEbbntxIkTdXhQgbBfv366poG9pKQkHTaqV6+ul6GNGjVKhxp7qkZIr1699Gs0aNBAhxqUrrj3RV1U7Q8rFQIffvhhvcRT/SN400036YBi79ChQ5ZBgwbp2iqqTsHjjz9uyc7OLvL/wdVXX61/35o0aeJwDhTvr3/9q6VRo0b6Z6Y+lNTvhjV4KLw3nh0+eH8qxkf9p6K9JQAAAFeq0s/5AAAAnoXwAQAA3IrwAQAA3IrwAQAA3IrwAQAA3IrwAQAA3IrwAQAA3IrwAQAA3IrwAQAA3IrwAQAA3IrwAQAA3IrwAQAAxJ3+H6lQy45oqEJ+AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe76faa0-3933-416d-b923-993c67cfcf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tale to the world the worlf the wall the wall.\n",
      "\n",
      "LADY CAPULET:\n",
      "I will not the death the way the trumpetted thee.\n",
      "\n",
      "Nurse:\n",
      "Nurse, I will not the way the way, then I was they say,\n",
      "then they shall the way they are they was a woffer.\n",
      "\n",
      "Nurse:\n",
      "I do not thee war, then they warrant they way.\n",
      "\n",
      "LADY CAPULET:\n",
      "I will say thee to the way thee was they was a wone.\n",
      "\n",
      "Nurse:\n",
      "I do not thee to the way then thee way thee way.\n",
      "\n",
      "Nurse:\n",
      "I do you are to the way the way they was a word.\n",
      "\n",
      "Nurse:\n",
      "I do not thee to-morrow that I was they was a word.\n",
      "\n",
      "LADY CAPULET:\n",
      "I do not thee to the way to the way to thee.\n",
      "\n",
      "Nurse:\n",
      "I do not thee to the way to the world of the world.\n",
      "\n",
      "Nurse:\n",
      "I do not the way thee way the way the way thee.\n",
      "\n",
      "LADY CAPULET:\n",
      "I do not thee to the world the world the world.\n",
      "\n",
      "Nurse:\n",
      "I do not thee to the way the way the worther.\n",
      "\n",
      "LADY CAPULET:\n",
      "I do thee to the worthy the world of the cheeks.\n",
      "\n",
      "Nurse:\n",
      "I think thee the heart the world of the world,\n",
      "And then they have the to the heart they had they had they\n",
      "And the truly of the world of the world of the state.\n",
      "\n",
      "\n",
      "Nurse:\n",
      "I do thee to the worth of the worth of the words.\n",
      "\n",
      "\n",
      "LANTIS:\n",
      "I would thee to the worth of the words the world,\n",
      "And the wall the worth of the state of the state.\n",
      "\n",
      "\n",
      "LARTCA:\n",
      "I the day thee to the worth of the words the world.\n",
      "\n",
      "\n",
      "LARTCA:\n",
      "The trust the state of the state of the state.\n",
      "\n",
      "\n",
      "Nurse:\n",
      "O the stab of the state of the state,\n",
      "And the state of the state of the state.\n",
      "\n",
      "\n",
      "Nurse:\n",
      "O the stabb'd the state of the state.\n",
      "\n",
      "\n",
      "LARTIS:\n",
      "I would the doon of the state, and the words of the stand of the stander.\n",
      "\n",
      "\n",
      "LARTCA:\n",
      "I the day the worthy the state of the state.\n",
      "\n",
      "\n",
      "Nurse:\n",
      "The day of the state of the state of the state.\n",
      "\n",
      "\n",
      "LARTCA:\n",
      "I do the lost of the worth of the state.\n",
      "\n",
      "\n",
      "LARTIS:\n",
      "I would the worthy the state of the words of the words.\n",
      "\n",
      "\n",
      "LARTIS:\n",
      "The day the state of the state of the state.\n",
      "\n",
      "\n",
      "PAULINA:\n",
      "I do see the state the worth of the state.\n",
      "\n",
      "\n",
      "PAULINA:\n",
      "I have say to the state of the words of the worse.\n",
      "\n",
      "\n",
      "PAULINA:\n",
      "I have she shall be the state of the state.\n",
      "\n",
      "\n",
      "PAULINA:\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "def decode_sequence_char(\n",
    "    model, stoi, itos, prompt, max_new_tokens=100, block_size=256,\n",
    "    use_fenchel=False, tau=1.0, fenchel_iters=3, temperature=1.0\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    max_ctx = int(block_size)\n",
    "\n",
    "    # ?1 assume space token exists; fallback to 0 if missing\n",
    "    space_id = stoi.get(' ', 0)\n",
    "\n",
    "    # encode prompt\n",
    "    start_ids = torch.tensor([encode_chars(prompt, stoi)], dtype=torch.long, device=device)\n",
    "\n",
    "    # prepend the pad once; from now on the window just slides\n",
    "\n",
    "    idx = start_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = idx[:, -max_ctx:]  # rotating buffer: last pad_len+block_size tokens\n",
    "        logits, _ = model(context, None)\n",
    "        last_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    # drop the initial pad when returning the string\n",
    "    out_tokens = idx[0].tolist()\n",
    "    return decode_chars(out_tokens, itos)\n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "\n",
    "prompt = \" \"\n",
    "generated = decode_sequence_char(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=2024,\n",
    "    block_size=1024,\n",
    "    use_fenchel=False,\n",
    "    tau=1.5,\n",
    "    fenchel_iters=2,\n",
    "    temperature=0.0001\n",
    ")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e045fae-f129-499c-a0f6-0376e15fef27",
   "metadata": {},
   "source": [
    "This is Not equivalent to learning a Markov model. the synthetic composition of the aux target breaks that equivalence on multiple fronts, so the transformer canâ€”and evidently doesâ€”learn supra-Markov structure even though Z itself is built from n-gram machinery.\n",
    "\n",
    "why itâ€™s not Markov-equivalent:\n",
    "\n",
    "teacher â‰  single-order chain. Z is a composite of multiple orders (2â€“64) with B-tree intersection/union, top-K capping, and Îµ fills from global frequency. thereâ€™s no single conditional table that reproduces those distributions; theyâ€™re a curated prior, not a proper n-gram MLE.\n",
    "\n",
    "objective â‰  local count fitting. we optimize a target with a deep network. nothing forces it to compute conditionals the way an n-gram would. the network can use any cues in its receptive field (including >64 tokens) to match Z statistically across the corpus.\n",
    "\n",
    "multi-depth CE (same Z, different skew). supervising every block with depth-wise sharpening introduces constraints on internal representations (curriculum/distillation effect) that Markov models donâ€™t have. it shapes a hierarchy, not just the final conditional.\n",
    "\n",
    "support prior, not ground truth. by constraining support to plausible continuations, Z acts like an energy manifold (valid vs. invalid regions). the model is trained to align with that manifold; it is not estimating n-gram probabilities per se.\n",
    "\n",
    "global coupling through parameter sharing. transformers share parameters across positions and contexts; learning to match Z in one regime generalizes to others in ways that exceed local-count estimators.\n",
    "\n",
    "observed behavior contradicts Markov collapse. the low-temperature generations degrade into structured attractors (speaker scaffolds, grammatical frames), not trivial loops; thatâ€™s a hallmark of a smooth, global field learned over sequences, not a brittle n-gram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72509ed-bff1-43e2-a08a-4aad880fb293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
