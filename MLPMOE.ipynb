{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7741bdf-98ab-4400-a0d1-4b831244ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "# --- GPT with auxiliary reverse-embedding loss from zb ---\n",
    "import math, inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# assumes Block and LayerNorm are defined elsewhere (as in your current setup)\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig,\n",
    "                 aux_scale: float = 1.0,\n",
    "                 noise_constituent: float = 1e-4,\n",
    "                 noise_final: float = 1e-4):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None and config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        # per-block orthonormal maps for blocks 0..3\n",
    "        need_blocks = 4\n",
    "        if config.n_layer < need_blocks:\n",
    "            raise ValueError(f\"need at least {need_blocks} transformer blocks for aux; got {config.n_layer}\")\n",
    "        self.aux_blocks = list(range(need_blocks))  # [0,1,2,3] fixed\n",
    "        self.aux_maps = nn.ModuleList()\n",
    "        for _ in self.aux_blocks:\n",
    "            lin = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            nn.init.orthogonal_(lin.weight)  # square => orthonormal rows & columns\n",
    "            self.aux_maps.append(lin)\n",
    "\n",
    "        # noise/scales\n",
    "        self.aux_scale_default = float(aux_scale)\n",
    "        self.noise_constituent = float(noise_constituent)\n",
    "        self.noise_final = float(noise_final)\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # reverse-embedding for one lane (list length T of [idxs, probs])\n",
    "    def _rev_embed_lane(self, lane_seq, device):\n",
    "        T = len(lane_seq)\n",
    "        if T == 0:\n",
    "            return torch.empty(0, self.config.n_embd, device=device)\n",
    "        idxs = torch.tensor([pair[0] for pair in lane_seq], device=device, dtype=torch.long)      # (T, K)\n",
    "        probs = torch.tensor([pair[1] for pair in lane_seq], device=device, dtype=torch.float32)  # (T, K)\n",
    "        probs = probs / (probs.sum(dim=-1, keepdim=True) + 1e-12)\n",
    "\n",
    "        E = self.transformer.wte.weight  # (V, D)\n",
    "        emb = E.index_select(0, idxs.reshape(-1)).reshape(*idxs.shape, E.size(1))  # (T, K, D)\n",
    "        if self.noise_constituent > 0:\n",
    "            emb = emb + torch.randn_like(emb) * self.noise_constituent\n",
    "        rev = torch.einsum('tkd,tk->td', emb, probs)  # (T, D)\n",
    "        if self.noise_final > 0:\n",
    "            rev = rev + torch.randn_like(rev) * self.noise_final\n",
    "        return rev\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 66\n",
    "    n_layer: int = 4          # explicitly 4 per your instruction\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig,\n",
    "                 aux_scale: float = 1.0,           # (12) fixed\n",
    "                 noise_constituent: float = 1e-6,    # (7) fixed\n",
    "                 noise_final: float = 1e-4):         # (7) fixed\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None and config.block_size is not None\n",
    "\n",
    "        self.config = config\n",
    "        self.aux_scale = float(aux_scale)\n",
    "        self.noise_constituent = float(noise_constituent)\n",
    "        self.noise_final = float(noise_final)\n",
    "\n",
    "        # core transformer\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        #self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        # per-block orthonormal linears (square D√óD, columns orthonormal)\n",
    "        self.aux_maps = nn.ModuleList()\n",
    "        for _ in range(4):\n",
    "            lin = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            nn.init.orthogonal_(lin.weight)\n",
    "            self.aux_maps.append(lin)\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # -------- reverse-embedding helpers (batchified, same dtype/device as x) --------\n",
    "    def _extract_idx_prob(self, zb, pair_offset):\n",
    "        \"\"\"\n",
    "        zb: Python list of length B; zb[b] is list length T;\n",
    "            zb[b][t] is length-8 list per spec:\n",
    "              [idxs_bi, probs_bi, idxs_4, probs_4, idxs_8, probs_8, idxs_16, probs_16]\n",
    "        pair_offset: 0 for bigram, 2 for m4, 4 for m8, 6 for m16.\n",
    "        returns: (idxs, probs, K) with shapes (B, T, K)\n",
    "        \"\"\"\n",
    "        B = len(zb)\n",
    "        T = len(zb[0])\n",
    "        # infer K from the first timestep\n",
    "        K = len(zb[0][0][pair_offset])\n",
    "        idxs = torch.empty((B, T, K), dtype=torch.long)\n",
    "        probs = torch.empty((B, T, K), dtype=torch.float32)\n",
    "        for b in range(B):\n",
    "            seq = zb[b]\n",
    "            for t in range(T):\n",
    "                idxs[b, t] = torch.tensor(seq[t][pair_offset], dtype=torch.long)\n",
    "                probs[b, t] = torch.tensor(seq[t][pair_offset + 1], dtype=torch.float32)\n",
    "        return idxs, probs, K\n",
    "\n",
    "    def _rev_embed_batch(self, idxs, probs, dtype, device):\n",
    "        \"\"\"\n",
    "        idxs:  (B, T, K) long\n",
    "        probs: (B, T, K) float (will be cast to dtype)\n",
    "        returns rev: (B, T, D) in `dtype` on `device`\n",
    "        \"\"\"\n",
    "        E = self.transformer.wte.weight.to(dtype=dtype)   # (V, D)\n",
    "        B, T, K = idxs.shape\n",
    "        # gather embeddings: (B,T,K,D)\n",
    "        emb = E.index_select(0, idxs.reshape(-1)).reshape(B, T, K, E.size(1))\n",
    "        # noise per constituent\n",
    "        if self.noise_constituent > 0:\n",
    "            emb = emb + torch.randn_like(emb) * self.noise_constituent\n",
    "        # weighted sum\n",
    "        probs = probs.to(dtype=dtype, device=device)\n",
    "        rev = (emb * probs.unsqueeze(-1)).sum(dim=2)      # (B, T, D)\n",
    "        # final noise\n",
    "        if self.noise_final > 0:\n",
    "            rev = rev + torch.randn_like(rev) * self.noise_final\n",
    "        return rev\n",
    "\n",
    "    # --- replace ONLY the forward in GPT with this version ---\n",
    "    def forward(self, idx, targets=None, zb=None):\n",
    "        \"\"\"\n",
    "        idx: (B, T) Long\n",
    "        targets: (B, T) Long or None\n",
    "        zb: list of 4 tuples, each (idxs, probs) as numpy arrays with shape (B, T, 32)\n",
    "            order: 0=bigram, 1=4gram, 2=8gram, 3=16gram\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size\n",
    "    \n",
    "        # embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "    \n",
    "        aux_loss = None\n",
    "        dtype = x.dtype\n",
    "        ignore = {0: 1, 1: 3, 2: 7, 3: 15}  # warmup ignores\n",
    "    \n",
    "        # block loop with vectorized aux\n",
    "        for bidx, block in enumerate(self.transformer.h):\n",
    "            x = block(x)  # (B, T, D)\n",
    "    \n",
    "            if zb is not None and bidx < 4:\n",
    "                idxs_np, probs_np = zb[bidx]  # numpy arrays (B, T, K)\n",
    "                # to tensors on the right device/dtype\n",
    "                idxs  = torch.from_numpy(idxs_np).to(device=device, dtype=torch.long)\n",
    "                probs = torch.from_numpy(probs_np).to(device=device, dtype=dtype)\n",
    "    \n",
    "                # gather embeddings: E[idxs] -> (B,T,K,D)\n",
    "                E = self.transformer.wte.weight.to(dtype=dtype)\n",
    "                BTK = idxs.reshape(-1)\n",
    "                emb = E.index_select(0, BTK).reshape(B, T, probs.size(-1), E.size(1))\n",
    "    \n",
    "                # tiny noise per constituent, then weighted sum -> (B,T,D)\n",
    "                if self.noise_constituent > 0:\n",
    "                    emb = emb + torch.randn_like(emb) * self.noise_constituent\n",
    "                rev = (emb * probs.unsqueeze(-1)).sum(dim=2)\n",
    "                if self.noise_final > 0:\n",
    "                    rev = rev + torch.randn_like(rev) * self.noise_final\n",
    "                # per-block projector and masked MSE\n",
    "                mapped = self.aux_maps[bidx](rev)  # (B,T,D)\n",
    "\n",
    "                mask = (torch.arange(T, device=device).expand(B, T) >= ignore[bidx]).unsqueeze(-1)  # (B,T,1)\n",
    "                diff2 = (x - mapped) ** 2\n",
    "                diff2 = diff2 * mask  # bool -> broadcast\n",
    "                denom = (mask.sum() * diff2.size(-1)).clamp_min(1)\n",
    "                block_loss = diff2.sum() / denom\n",
    "    \n",
    "                aux_loss = block_loss if aux_loss is None else aux_loss + block_loss\n",
    "    \n",
    "        # head + CE\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "    \n",
    "        ce_loss = None\n",
    "        if targets is not None:\n",
    "            ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "                                      targets.view(-1), ignore_index=-100)\n",
    "    \n",
    "        # total\n",
    "        if ce_loss is None and aux_loss is None:\n",
    "            loss = None\n",
    "        elif aux_loss is None:\n",
    "            loss = ce_loss\n",
    "        elif ce_loss is None:\n",
    "            loss = self.aux_scale * aux_loss\n",
    "        else:\n",
    "            loss = ce_loss + self.aux_scale * aux_loss\n",
    "    \n",
    "        if targets is None:\n",
    "            logits = logits[:, [-1], :]\n",
    "    \n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d76ace-c3c1-4a68-adce-851c842dbbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading aochildes.txt...\n",
      "üì• Downloading cbt.txt...\n",
      "üì• Downloading children_stories.txt...\n",
      "üì• Downloading gutenberg.txt...\n",
      "üì• Downloading qed.txt...\n",
      "üì• Downloading simple_wikipedia.txt...\n",
      "üì• Downloading switchboard.txt...\n",
      "üì• Downloading wikipedia.txt...\n",
      "üì• Downloading shakespeare.txt...\n",
      "‚úÖ Done. Files saved to ./babylm_10m_cleaned\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"üì• Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"‚úÖ Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95a695ab-753c-4b35-8834-a1d4f59859bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Char tokenizer finalized.\n",
      "üßæ Train tokens: 1016242 | Val tokens: 99152\n",
      "üî§ Vocab size: 66\n",
      "Loaded 1016242 train tokens and 99152 val tokens | vocab=66\n",
      "Building 4-token Markov chain...\n",
      "Building 8-token Markov chain...\n",
      "Building 16-token Markov chain...\n",
      "Building bigram probability distribution...\n",
      "‚úÖ Markov and Bigram models saved.\n",
      "Chains: ['order=4', 'order=8', 'order=16']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Char tokenizer finalized.\")\n",
    "print(f\"üßæ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"üî§ Vocab size: {len(stoi)}\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# === Load data ===\n",
    "data_dir = \"./babylm_char_tokenized\"\n",
    "train_path = os.path.join(data_dir, \"train.bin\")\n",
    "val_path   = os.path.join(data_dir, \"val.bin\")\n",
    "meta_path  = os.path.join(data_dir, \"meta.pkl\")\n",
    "\n",
    "train_ids = np.fromfile(train_path, dtype=np.uint16)\n",
    "val_ids   = np.fromfile(val_path,   dtype=np.uint16)\n",
    "\n",
    "with open(meta_path, \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "vocab_size = meta[\"vocab_size\"]\n",
    "\n",
    "print(f\"Loaded {len(train_ids)} train tokens and {len(val_ids)} val tokens | vocab={vocab_size}\")\n",
    "\n",
    "# === Build Markov Models ===\n",
    "def build_markov_chain(data, window):\n",
    "    \"\"\"\n",
    "    Builds a Markov chain of given window size.\n",
    "    Returns: dict mapping tuple(context) -> Counter(next_token)\n",
    "    \"\"\"\n",
    "    chain = defaultdict(Counter)\n",
    "    for i in range(len(data) - window):\n",
    "        context = tuple(data[i : i + window])\n",
    "        nxt = data[i + window]\n",
    "        chain[context][nxt] += 1\n",
    "    return chain\n",
    "\n",
    "windows = [4, 8, 16]\n",
    "markov_models = {}\n",
    "\n",
    "for w in windows:\n",
    "    print(f\"Building {w}-token Markov chain...\")\n",
    "    markov_models[w] = build_markov_chain(train_ids, w)\n",
    "\n",
    "# === Build Bigram Continuation Probabilities ===\n",
    "import numpy as np\n",
    "\n",
    "def build_bigram_distribution_fixed(data, vocab_size, top_k=16, seed=1337, epsilon=1e-6):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # counts\n",
    "    bigram_counts = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    a = data[:-1]\n",
    "    b = data[1:]\n",
    "    np.add.at(bigram_counts, (a, b), 1)\n",
    "\n",
    "    out_idx = np.empty((vocab_size, top_k), dtype=np.int32)\n",
    "    out_p   = np.empty((vocab_size, top_k), dtype=np.float32)\n",
    "\n",
    "    all_ids = np.arange(vocab_size, dtype=np.int32)\n",
    "\n",
    "    for tok in range(vocab_size):\n",
    "        counts = bigram_counts[tok]\n",
    "        total = counts.sum()\n",
    "\n",
    "        if total == 0:\n",
    "            # no observations ‚Äî choose k random unique tokens and make them uniform\n",
    "            idx = rng.choice(vocab_size, size=top_k, replace=False)\n",
    "            p = np.full(top_k, 1.0 / top_k, dtype=np.float32)\n",
    "            out_idx[tok] = idx\n",
    "            out_p[tok] = p\n",
    "            continue\n",
    "\n",
    "        probs_full = counts.astype(np.float64) / float(total)\n",
    "        observed = np.flatnonzero(counts)\n",
    "\n",
    "        if observed.size >= top_k:\n",
    "            # get top_k among observed only (fast top-k)\n",
    "            obs_p = probs_full[observed]\n",
    "            kth = np.argpartition(obs_p, -top_k)[-top_k:]\n",
    "            idx = observed[kth]\n",
    "            p = probs_full[idx].astype(np.float32)\n",
    "            # normalize in case of numerical drift\n",
    "            s = p.sum()\n",
    "            p = p / s if s > 0 else np.full(top_k, 1.0 / top_k, dtype=np.float32)\n",
    "        else:\n",
    "            # take all observed, randomly fill the rest from unobserved\n",
    "            need = top_k - observed.size\n",
    "            mask = np.ones(vocab_size, dtype=bool)\n",
    "            mask[observed] = False\n",
    "            pool = all_ids[mask]\n",
    "            # sample without replacement to avoid duplicates\n",
    "            extra = rng.choice(pool, size=need, replace=False)\n",
    "            idx = np.concatenate([observed, extra])\n",
    "\n",
    "            p = probs_full[idx].astype(np.float32)\n",
    "            # give a tiny positive mass to the extras that were unobserved (counts==0)\n",
    "            unobs = (counts[idx] == 0)\n",
    "            if unobs.any():\n",
    "                p = p + unobs.astype(np.float32) * epsilon\n",
    "            p = p / p.sum()\n",
    "\n",
    "        # ensure a consistent ordering (optional): sort descending prob\n",
    "        order = np.argsort(-p)\n",
    "        out_idx[tok] = idx[order]\n",
    "        out_p[tok]   = p[order]\n",
    "\n",
    "    # return as simple dict-of-tuples for backward compatibility\n",
    "    bigram_db = {int(t): (out_idx[t], out_p[t]) for t in range(vocab_size)}\n",
    "    return bigram_db\n",
    "\n",
    "print(\"Building bigram probability distribution...\")\n",
    "bigram_db = build_bigram_distribution_fixed(train_ids, vocab_size)\n",
    "\n",
    "# === Save ===\n",
    "model_dir = \"./markov_bigram_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(model_dir, \"markov_models.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(markov_models, f)\n",
    "\n",
    "with open(os.path.join(model_dir, \"bigram_db.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(bigram_db, f)\n",
    "\n",
    "print(\"‚úÖ Markov and Bigram models saved.\")\n",
    "print(f\"Chains: {[f'order={w}' for w in windows]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcae90-1cdc-4ef5-8907-75961e9a5ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57eff45-c482-4e59-ace0-5c35e65c879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd4dd64a-8f8e-471e-97f2-7ffd1df4c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "# === Load tokenizer metadata ===\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "\n",
    "# === Load mmap edata (char-level tokens, uint16) ===\n",
    "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
    "\n",
    "# === Replacement dataloader that uses SAVED bigram + markov models and yields (X, Y, Z) ===\n",
    "import os, pickle, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# expects `vocab_size` and `device` already defined in the outer scope\n",
    "# expects saved models at ./markov_bigram_models/{bigram_db.pkl, markov_models.pkl}\n",
    "\n",
    "\n",
    "\n",
    "class ZPack:\n",
    "    __slots__ = (\"blocks\",)\n",
    "    def __init__(self, blocks):\n",
    "        # blocks = [(idxs_np, probs_np), ...] length 4, each np arrays shape (B,T,32)\n",
    "        self.blocks = blocks\n",
    "    def __getitem__(self, i):\n",
    "        return self.blocks[i]  # allows model to access zb[bidx] -> (idxs_np, probs_np)\n",
    "    # no __len__ and no Sequence inheritance => collate treats this as an opaque object\n",
    "\n",
    "class GPUBatchDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size, batch_size, device,\n",
    "                 model_dir=\"./markov_bigram_models\", jitter=63, p_aligned=0.5, pad_len=0,\n",
    "                 top_k=16, seed=1337):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = int(block_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.device = device\n",
    "        self.pad_len = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = max(1, self.total // self.sample_len)\n",
    "        self.jitter = int(jitter)\n",
    "        self.p_aligned = float(p_aligned)\n",
    "        self.top_k = int(top_k)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        with open(os.path.join(model_dir, \"bigram_db.pkl\"), \"rb\") as f:\n",
    "            self.bigram_db = pickle.load(f)\n",
    "        with open(os.path.join(model_dir, \"markov_models.pkl\"), \"rb\") as f:\n",
    "            self.markov_models = pickle.load(f)  # {4:..., 8:..., 16:...}\n",
    "\n",
    "        assert isinstance(vocab_size, int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def _sample_block(self):\n",
    "        base_block = self.rng.integers(0, self.n_blocks)\n",
    "        start = base_block * self.sample_len\n",
    "        if self.rng.random() > self.p_aligned:\n",
    "            j = self.rng.integers(0, self.jitter + 1)\n",
    "            start = min(start + j, self.total)\n",
    "        return start\n",
    "\n",
    "    def _finalize_topk_from_counts(self, counter, top_k=16, epsilon=1e-6):\n",
    "        rng = self.rng\n",
    "        if not counter:\n",
    "            idxs = rng.choice(vocab_size, size=top_k, replace=False)\n",
    "            probs = np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "            return idxs.astype(np.int64), probs\n",
    "        items = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        obs_idxs = np.fromiter((t for t, _ in items), dtype=np.int64, count=len(items))\n",
    "        obs_cnts = np.fromiter((c for _, c in items), dtype=np.float64, count=len(items))\n",
    "        if len(obs_idxs) >= top_k:\n",
    "            idxs = obs_idxs[:top_k]\n",
    "            probs = (obs_cnts[:top_k] / obs_cnts[:top_k].sum()).astype(np.float32)\n",
    "            return idxs, probs\n",
    "        need = top_k - len(obs_idxs)\n",
    "        mask = np.ones(vocab_size, dtype=bool); mask[obs_idxs] = False\n",
    "        extras = rng.choice(np.nonzero(mask)[0], size=need, replace=False).astype(np.int64)\n",
    "        idxs = np.concatenate([obs_idxs, extras])\n",
    "        probs = np.concatenate([obs_cnts, np.full(need, epsilon, dtype=np.float64)]).astype(np.float32)\n",
    "        probs = probs / probs.sum()\n",
    "        return idxs, probs\n",
    "\n",
    "    def _finalize_topk_from_bigram(self, entry, top_k=16, epsilon=1e-6):\n",
    "        rng = self.rng\n",
    "        if entry is None:\n",
    "            idxs = rng.choice(vocab_size, size=top_k, replace=False).astype(np.int64)\n",
    "            probs = np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "            return idxs, probs\n",
    "        idxs, probs = entry\n",
    "        idxs = np.asarray(idxs, dtype=np.int64)\n",
    "        probs = np.asarray(probs, dtype=np.float32)\n",
    "        if idxs.shape[0] > top_k:\n",
    "            order = np.argsort(-probs)[:top_k]\n",
    "            idxs, probs = idxs[order], probs[order]\n",
    "        elif idxs.shape[0] < top_k:\n",
    "            need = top_k - idxs.shape[0]\n",
    "            mask = np.ones(vocab_size, dtype=bool); mask[idxs] = False\n",
    "            extras = rng.choice(np.nonzero(mask)[0], size=need, replace=False).astype(np.int64)\n",
    "            idxs = np.concatenate([idxs, extras])\n",
    "            probs = np.concatenate([probs, np.full(need, epsilon, dtype=np.float32)])\n",
    "        probs = probs / probs.sum()\n",
    "        return idxs, probs\n",
    "\n",
    "    def _dist_bigram(self, tok):\n",
    "        entry = self.bigram_db.get(int(tok), None)\n",
    "        return self._finalize_topk_from_bigram(entry, top_k=self.top_k)\n",
    "\n",
    "    def _dist_markov(self, ctx_tuple, backoff_tok):\n",
    "        counter = None\n",
    "        if ctx_tuple is not None:\n",
    "            chain = self.markov_models.get(len(ctx_tuple), {})\n",
    "            counter = chain.get(ctx_tuple, None)\n",
    "        if counter:\n",
    "            return self._finalize_topk_from_counts(counter, top_k=self.top_k)\n",
    "        return self._dist_bigram(backoff_tok)\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        B, T, K = self.batch_size, self.block_size, self.top_k\n",
    "        X = np.empty((B, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((B, T), dtype=np.int64)\n",
    "\n",
    "        # preallocate Z blocks\n",
    "        bi_idx  = np.empty((B, T, K), dtype=np.int64); bi_p  = np.empty((B, T, K), dtype=np.float32)\n",
    "        m4_idx  = np.empty((B, T, K), dtype=np.int64); m4_p  = np.empty((B, T, K), dtype=np.float32)\n",
    "        m8_idx  = np.empty((B, T, K), dtype=np.int64); m8_p  = np.empty((B, T, K), dtype=np.float32)\n",
    "        m16_idx = np.empty((B, T, K), dtype=np.int64); m16_p = np.empty((B, T, K), dtype=np.float32)\n",
    "\n",
    "        for i in range(B):\n",
    "            start = self._sample_block()\n",
    "            X[i] = self.data[start : start + self.sample_len]\n",
    "            Y[i] = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + T]\n",
    "\n",
    "            for j in range(T):\n",
    "                tok_now = int(X[i, j])\n",
    "\n",
    "                # bigram\n",
    "                idxs, probs = self._dist_bigram(tok_now)\n",
    "                bi_idx[i, j, :] = idxs; bi_p[i, j, :] = probs\n",
    "\n",
    "                # contexts for markov\n",
    "                ctx4  = tuple(int(x) for x in X[i, j-3 :  j+1]) if j >= 3  else None\n",
    "                ctx8  = tuple(int(x) for x in X[i, j-7 :  j+1]) if j >= 7  else None\n",
    "                ctx16 = tuple(int(x) for x in X[i, j-15:  j+1]) if j >= 15 else None\n",
    "\n",
    "                idxs, probs = self._dist_markov(ctx4,  tok_now);  m4_idx[i, j, :]  = idxs; m4_p[i, j, :]  = probs\n",
    "                idxs, probs = self._dist_markov(ctx8,  tok_now);  m8_idx[i, j, :]  = idxs; m8_p[i, j, :]  = probs\n",
    "                idxs, probs = self._dist_markov(ctx16, tok_now);  m16_idx[i, j, :] = idxs; m16_p[i, j, :] = probs\n",
    "\n",
    "        # wrap Z so collate doesn't decompose it\n",
    "        Z = ZPack([\n",
    "            (bi_idx,  bi_p),\n",
    "            (m4_idx,  m4_p),\n",
    "            (m8_idx,  m8_p),\n",
    "            (m16_idx, m16_p),\n",
    "        ])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
    "            torch.from_numpy(Y).to(self.device, non_blocking=True),\n",
    "            Z,  # opaque: collate will return [Z] for batch_size=1, so training uses zb=zb[0]\n",
    "        )\n",
    "\n",
    "# instantiate (unchanged outer config)\n",
    "model_dir = \"./markov_bigram_models\"\n",
    "train_dataset = GPUBatchDataset(\n",
    "    np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r'),\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    model_dir=model_dir,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "efacb560-8e61-4e4e-9399-3c1611b476ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_keep_z(batch):\n",
    "    # batch: list of N items; each item is (X, Y, Z)\n",
    "    # X: (B, T_x) tensor on device\n",
    "    # Y: (B, T)   tensor on device\n",
    "    # Z: [(idxs, probs)] * 4, each np arrays (B, T, 32)\n",
    "    Xs, Ys, Zs = zip(*batch)  # tuples of length N\n",
    "\n",
    "    # stack X/Y across the outer dataloader batch (keeps them on the same device)\n",
    "    X = torch.cat(Xs, dim=0)\n",
    "    Y = torch.cat(Ys, dim=0)\n",
    "\n",
    "    # merge Z by concatenating along batch axis (axis=0) for each of the 4 blocks\n",
    "    merged_blocks = []\n",
    "    for b in range(4):\n",
    "        idxs_list  = [Z[b][0] for Z in Zs]  # list of np arrays (B_i, T, 32)\n",
    "        probs_list = [Z[b][1] for Z in Zs]\n",
    "        idxs  = np.concatenate(idxs_list,  axis=0)  # (sum B_i, T, 32)\n",
    "        probs = np.concatenate(probs_list, axis=0)  # (sum B_i, T, 32)\n",
    "        merged_blocks.append((idxs, probs))\n",
    "\n",
    "    return X, Y, merged_blocks\n",
    "\n",
    "# --- use the custom collate in your DataLoader (keep batch_size=1 as you have) ---\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_keep_z\n",
    ")\n",
    "\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb, zb in train_loader:\n",
    "        # xb: (B, T), yb: (B, T), zb: list of 4 tuples (np arrays (B,T,32))\n",
    "        logits, loss = model(xb, yb, zb)   # model unchanged\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        print(loss.item())\n",
    "    return total_loss / len(train_loader)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f66f46f4-d5f4-4085-ae51-a92758adb506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1.67M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "config =  GPTConfig(\n",
    "    block_size,\n",
    "    vocab_size,\n",
    "    n_layer=8,      \n",
    "    n_head = 8,\n",
    "    n_embd =128)\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c43d3e5c-ba7e-4a1e-9b59-1b593316e2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1799936\n"
     ]
    }
   ],
   "source": [
    "print(sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6451f-1367-40a2-9fb9-85a5085ef5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de2187f4-6a37-4a77-9a37-b0dae6751d8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.229501247406006\n",
      "3.7600677013397217\n",
      "3.7490367889404297\n",
      "3.6579651832580566\n",
      "3.9314305782318115\n",
      "3.6007657051086426\n",
      "3.4505062103271484\n",
      "3.635887861251831\n",
      "3.3901374340057373\n",
      "3.4784457683563232\n",
      "3.4064905643463135\n",
      "3.3360023498535156\n",
      "3.3552255630493164\n",
      "3.3451287746429443\n",
      "3.2399630546569824\n",
      "3.1685311794281006\n",
      "3.2302069664001465\n",
      "3.282038927078247\n",
      "3.180192708969116\n",
      "3.176995038986206\n",
      "3.2507495880126953\n",
      "3.2394912242889404\n",
      "3.1054329872131348\n",
      "3.0508878231048584\n",
      "3.0057408809661865\n",
      "3.0497231483459473\n",
      "3.0181965827941895\n",
      "2.997053623199463\n",
      "2.940744400024414\n",
      "2.954636812210083\n",
      "2.9175171852111816\n",
      "2.9824867248535156\n",
      "2.9058568477630615\n",
      "2.8636674880981445\n",
      "2.832852840423584\n",
      "2.815890073776245\n",
      "2.804941415786743\n",
      "2.809286117553711\n",
      "2.772695779800415\n",
      "2.856433391571045\n",
      "2.7632880210876465\n",
      "2.8036773204803467\n",
      "2.7886061668395996\n",
      "2.788189172744751\n",
      "2.745576858520508\n",
      "2.7569684982299805\n",
      "2.7444798946380615\n",
      "2.7276852130889893\n",
      "2.7317850589752197\n",
      "2.7010409832000732\n",
      "2.676767587661743\n",
      "2.6793293952941895\n",
      "2.7404181957244873\n",
      "2.6671271324157715\n",
      "2.6537039279937744\n",
      "2.6549489498138428\n",
      "2.6391642093658447\n",
      "2.673917055130005\n",
      "2.6390743255615234\n",
      "2.657876491546631\n",
      "2.630915403366089\n",
      "2.6534242630004883\n",
      "2.634068727493286\n",
      "2.6555681228637695\n",
      "2.6245315074920654\n",
      "2.632847547531128\n",
      "2.645146369934082\n",
      "2.6418020725250244\n",
      "2.620267868041992\n",
      "2.6742050647735596\n",
      "2.645493507385254\n",
      "2.6287460327148438\n",
      "2.627411365509033\n",
      "2.636709690093994\n",
      "2.6187827587127686\n",
      "2.5982325077056885\n",
      "2.6180005073547363\n",
      "2.5991694927215576\n",
      "2.575549364089966\n",
      "2.672506332397461\n",
      "2.5795257091522217\n",
      "2.5813896656036377\n",
      "2.6079821586608887\n",
      "2.592820644378662\n",
      "2.566300392150879\n",
      "2.5975406169891357\n",
      "2.609086751937866\n",
      "2.5358617305755615\n",
      "2.6260247230529785\n",
      "2.5771501064300537\n",
      "2.5781965255737305\n",
      "2.585775375366211\n",
      "2.579756736755371\n",
      "2.564527750015259\n",
      "2.5701050758361816\n",
      "2.598593235015869\n",
      "2.5625417232513428\n",
      "2.5797035694122314\n",
      "2.5414981842041016\n",
      "2.5821733474731445\n",
      "2.535881519317627\n",
      "2.558065414428711\n",
      "2.596575975418091\n",
      "2.559028148651123\n",
      "2.573357105255127\n",
      "2.567936420440674\n",
      "2.544656753540039\n",
      "2.552626132965088\n",
      "2.559187412261963\n",
      "2.547576904296875\n",
      "2.536019802093506\n",
      "2.5775325298309326\n",
      "2.5615618228912354\n",
      "2.5268192291259766\n",
      "2.5777535438537598\n",
      "2.5527966022491455\n",
      "2.556820869445801\n",
      "2.5771899223327637\n",
      "2.5388197898864746\n",
      "2.56105637550354\n",
      "2.556094169616699\n",
      "2.533507823944092\n",
      "2.5373728275299072\n",
      "2.5299465656280518\n",
      "2.555432081222534\n",
      "2.542607545852661\n",
      "2.5323264598846436\n",
      "2.5292840003967285\n",
      "2.565187931060791\n",
      "2.5558104515075684\n",
      "2.533736228942871\n",
      "2.526221990585327\n",
      "2.5225954055786133\n",
      "2.557003974914551\n",
      "2.524386167526245\n",
      "2.551583766937256\n",
      "2.541599750518799\n",
      "2.585693836212158\n",
      "2.5378029346466064\n",
      "2.5369315147399902\n",
      "2.5443389415740967\n",
      "2.555492877960205\n",
      "2.5506632328033447\n",
      "2.5494484901428223\n",
      "2.535825252532959\n",
      "2.519587755203247\n",
      "2.5555596351623535\n",
      "2.581674814224243\n",
      "2.5079002380371094\n",
      "2.5101230144500732\n",
      "2.519238233566284\n",
      "2.5498082637786865\n",
      "2.533762216567993\n",
      "2.5334229469299316\n",
      "2.5092625617980957\n",
      "2.519409656524658\n",
      "2.504753589630127\n",
      "2.5248215198516846\n",
      "2.531902551651001\n",
      "2.5399510860443115\n",
      "2.513042688369751\n",
      "2.5207459926605225\n",
      "2.516953706741333\n",
      "2.512699604034424\n",
      "2.5400264263153076\n",
      "2.51936936378479\n",
      "2.4943830966949463\n",
      "2.50154972076416\n",
      "2.510713577270508\n",
      "2.5167429447174072\n",
      "2.5404231548309326\n",
      "2.5223491191864014\n",
      "2.4996628761291504\n",
      "2.498303174972534\n",
      "2.5087828636169434\n",
      "2.538052558898926\n",
      "2.490529775619507\n",
      "2.5244855880737305\n",
      "2.482926845550537\n",
      "2.5243728160858154\n",
      "2.5183119773864746\n",
      "2.4969708919525146\n",
      "2.4914348125457764\n",
      "2.4970760345458984\n",
      "2.5246260166168213\n",
      "2.5409727096557617\n",
      "2.510484218597412\n",
      "2.51027774810791\n",
      "2.521735429763794\n",
      "2.500293493270874\n",
      "2.538196086883545\n",
      "2.5144596099853516\n",
      "2.525162696838379\n",
      "2.5132250785827637\n",
      "2.5297441482543945\n",
      "2.526404619216919\n",
      "2.5293092727661133\n",
      "2.534655809402466\n",
      "2.508542060852051\n",
      "2.5103633403778076\n",
      "2.5028676986694336\n",
      "2.5298843383789062\n",
      "2.5100464820861816\n",
      "2.5272903442382812\n",
      "2.5222861766815186\n",
      "2.5023396015167236\n",
      "2.489865303039551\n",
      "2.5159380435943604\n",
      "2.4917423725128174\n",
      "2.5063536167144775\n",
      "2.4905078411102295\n",
      "2.5275046825408936\n",
      "2.504065752029419\n",
      "2.5450565814971924\n",
      "2.474066734313965\n",
      "2.48817777633667\n",
      "2.4680449962615967\n",
      "2.483316659927368\n",
      "2.519399881362915\n",
      "2.4882822036743164\n",
      "2.478013515472412\n",
      "2.5119662284851074\n",
      "2.495896816253662\n",
      "2.52349853515625\n",
      "2.5156099796295166\n",
      "2.5442636013031006\n",
      "2.5191123485565186\n",
      "2.528315305709839\n",
      "2.5062096118927\n",
      "2.4963738918304443\n",
      "2.548579692840576\n",
      "2.499319076538086\n",
      "2.4832847118377686\n",
      "2.5061779022216797\n",
      "2.4800221920013428\n",
      "2.4971160888671875\n",
      "2.4617581367492676\n",
      "2.4906485080718994\n",
      "2.484239101409912\n",
      "2.486633062362671\n",
      "2.4746639728546143\n",
      "2.516648769378662\n",
      "2.4944491386413574\n",
      "2.527064323425293\n",
      "2.489901542663574\n",
      "2.5117764472961426\n",
      "2.510701894760132\n",
      "2.46816086769104\n",
      "2.533696174621582\n",
      "2.464813709259033\n",
      "2.5285065174102783\n",
      "2.5032358169555664\n",
      "2.5419485569000244\n",
      "2.490549325942993\n",
      "2.504387855529785\n",
      "2.521620035171509\n",
      "2.5098397731781006\n",
      "2.4783225059509277\n",
      "2.5052788257598877\n",
      "2.470820426940918\n",
      "2.5005645751953125\n",
      "2.4988186359405518\n",
      "2.4923512935638428\n",
      "2.489126205444336\n",
      "2.4895219802856445\n",
      "2.5207884311676025\n",
      "2.5194029808044434\n",
      "2.504420757293701\n",
      "2.4844350814819336\n",
      "2.4958765506744385\n",
      "2.5117897987365723\n",
      "2.481234073638916\n",
      "2.4792587757110596\n",
      "2.5132431983947754\n",
      "2.52565336227417\n",
      "2.495140314102173\n",
      "2.460858106613159\n",
      "2.529881238937378\n",
      "2.490785598754883\n",
      "2.4804775714874268\n",
      "2.4634757041931152\n",
      "2.497194528579712\n",
      "2.47544264793396\n",
      "2.462155818939209\n",
      "2.481722354888916\n",
      "2.4884727001190186\n",
      "2.461679697036743\n",
      "2.5082356929779053\n",
      "2.4654157161712646\n",
      "2.4692301750183105\n",
      "2.4755048751831055\n",
      "2.499152660369873\n",
      "2.5022668838500977\n",
      "2.4715347290039062\n",
      "2.4919426441192627\n",
      "2.475855827331543\n",
      "2.471992254257202\n",
      "2.458888053894043\n",
      "2.5027389526367188\n",
      "2.466803789138794\n",
      "2.512394905090332\n",
      "2.483386516571045\n",
      "2.4783830642700195\n",
      "2.493638038635254\n",
      "2.4717490673065186\n",
      "2.4952280521392822\n",
      "2.4893991947174072\n",
      "2.508880615234375\n",
      "2.4754724502563477\n",
      "2.484679937362671\n",
      "2.479576349258423\n",
      "2.5339670181274414\n",
      "2.4950478076934814\n",
      "2.4671554565429688\n",
      "2.5334839820861816\n",
      "2.5026674270629883\n",
      "2.5118634700775146\n",
      "2.510526418685913\n",
      "2.478316307067871\n",
      "2.5002217292785645\n",
      "2.4892945289611816\n",
      "2.465822219848633\n",
      "2.5342328548431396\n",
      "2.5239813327789307\n",
      "2.483661651611328\n",
      "2.544515371322632\n",
      "2.5025384426116943\n",
      "2.5000319480895996\n",
      "2.477113723754883\n",
      "2.512805700302124\n",
      "2.5077502727508545\n",
      "2.551377296447754\n",
      "2.511904001235962\n",
      "2.483276128768921\n",
      "2.487791061401367\n",
      "2.4763784408569336\n",
      "2.504856586456299\n",
      "2.5028419494628906\n",
      "2.507242441177368\n",
      "2.5268564224243164\n",
      "2.4960811138153076\n",
      "2.4875433444976807\n",
      "2.528266429901123\n",
      "2.4984660148620605\n",
      "2.5054893493652344\n",
      "2.5293960571289062\n",
      "2.5037903785705566\n",
      "2.503234624862671\n",
      "2.507732391357422\n",
      "2.4836442470550537\n",
      "2.504389524459839\n",
      "2.4941964149475098\n",
      "2.487306594848633\n",
      "2.4991753101348877\n",
      "2.5060272216796875\n",
      "2.505793809890747\n",
      "2.4776079654693604\n",
      "2.4770030975341797\n",
      "2.4814438819885254\n",
      "2.4922940731048584\n",
      "2.4974822998046875\n",
      "2.4963998794555664\n",
      "2.5031490325927734\n",
      "2.5028951168060303\n",
      "2.4720351696014404\n",
      "2.4586684703826904\n",
      "2.4714291095733643\n",
      "2.4545106887817383\n",
      "2.4968063831329346\n",
      "2.5075063705444336\n",
      "2.484598398208618\n",
      "2.4883365631103516\n",
      "2.451380968093872\n",
      "2.4830384254455566\n",
      "2.5145375728607178\n",
      "2.4581081867218018\n",
      "2.4795472621917725\n",
      "2.4975922107696533\n",
      "2.4792983531951904\n",
      "2.511625289916992\n",
      "2.499119997024536\n",
      "2.4738991260528564\n",
      "2.4646999835968018\n",
      "2.4958839416503906\n",
      "2.478440761566162\n",
      "2.485149383544922\n",
      "2.502892255783081\n",
      "2.442657232284546\n",
      "2.475849151611328\n",
      "2.5072181224823\n",
      "2.4690749645233154\n",
      "2.52510142326355\n",
      "2.4880313873291016\n",
      "2.466902256011963\n",
      "2.460840940475464\n",
      "2.4601352214813232\n",
      "2.4508252143859863\n",
      "2.5033957958221436\n",
      "2.464900255203247\n",
      "2.470120668411255\n",
      "2.46581768989563\n",
      "2.451151132583618\n",
      "2.4635398387908936\n",
      "2.4754199981689453\n",
      "2.4736244678497314\n",
      "2.4996185302734375\n",
      "2.476655960083008\n",
      "2.5062179565429688\n",
      "2.503119707107544\n",
      "2.4567134380340576\n",
      "2.4853453636169434\n",
      "2.496659278869629\n",
      "2.503734588623047\n",
      "2.5051372051239014\n",
      "2.4869821071624756\n",
      "2.478421688079834\n",
      "2.4892537593841553\n",
      "2.5029571056365967\n",
      "2.495966911315918\n",
      "2.4624457359313965\n",
      "2.4894795417785645\n",
      "2.479990005493164\n",
      "2.4973888397216797\n",
      "2.499613046646118\n",
      "2.474256992340088\n",
      "2.8619589805603027\n",
      "2.5375542640686035\n",
      "2.6148488521575928\n",
      "2.6463918685913086\n",
      "2.556483268737793\n",
      "2.5705835819244385\n",
      "2.532149076461792\n",
      "2.5382049083709717\n",
      "2.535881280899048\n",
      "2.5541229248046875\n",
      "2.610442876815796\n",
      "2.611003875732422\n",
      "2.546968698501587\n",
      "2.5442283153533936\n",
      "2.5451676845550537\n",
      "2.5594120025634766\n",
      "2.633005142211914\n",
      "2.5523533821105957\n",
      "2.5156586170196533\n",
      "2.530527114868164\n",
      "2.5418455600738525\n",
      "2.545067310333252\n",
      "2.5547821521759033\n",
      "2.5126802921295166\n",
      "2.527224063873291\n",
      "2.543905735015869\n",
      "2.5592610836029053\n",
      "2.5432586669921875\n",
      "2.5367579460144043\n",
      "2.5171968936920166\n",
      "2.703028678894043\n",
      "2.5501904487609863\n",
      "2.6080455780029297\n",
      "2.5470130443573\n",
      "2.5294418334960938\n",
      "2.54162859916687\n",
      "2.5918490886688232\n",
      "2.57368540763855\n",
      "2.5672073364257812\n",
      "2.537393569946289\n",
      "2.5306484699249268\n",
      "2.5552115440368652\n",
      "2.515103578567505\n",
      "2.6031978130340576\n",
      "2.5813372135162354\n",
      "2.592076301574707\n",
      "2.5270888805389404\n",
      "2.5438594818115234\n",
      "2.5530478954315186\n",
      "2.5419576168060303\n",
      "2.5113377571105957\n",
      "2.5204882621765137\n",
      "2.5343446731567383\n",
      "2.5173821449279785\n",
      "2.552952766418457\n",
      "2.4874649047851562\n",
      "2.516552686691284\n",
      "2.532789945602417\n",
      "2.483334541320801\n",
      "2.4931390285491943\n",
      "2.4974002838134766\n",
      "2.517392873764038\n",
      "2.4988856315612793\n",
      "2.4951565265655518\n",
      "2.526320219039917\n",
      "2.513326406478882\n",
      "2.497584104537964\n",
      "2.479996681213379\n",
      "2.4704768657684326\n",
      "2.5011305809020996\n",
      "2.466108560562134\n",
      "2.502232074737549\n",
      "2.511549949645996\n",
      "2.503962755203247\n",
      "2.5111989974975586\n",
      "2.517380714416504\n",
      "2.493122100830078\n",
      "2.501450777053833\n",
      "2.5421512126922607\n",
      "2.5148587226867676\n",
      "2.497141122817993\n",
      "2.464388847351074\n",
      "2.4839251041412354\n",
      "2.4941482543945312\n",
      "2.5034992694854736\n",
      "2.519927740097046\n",
      "2.5054259300231934\n",
      "2.469830274581909\n",
      "2.462614059448242\n",
      "2.491948366165161\n",
      "2.481713056564331\n",
      "2.4452810287475586\n",
      "2.4864606857299805\n",
      "2.4525606632232666\n",
      "2.4639313220977783\n",
      "2.490506649017334\n",
      "2.486929416656494\n",
      "2.441455841064453\n",
      "2.449089288711548\n",
      "2.429295301437378\n",
      "2.4538352489471436\n",
      "2.4976212978363037\n",
      "2.492680072784424\n",
      "2.462949275970459\n",
      "2.487860918045044\n",
      "2.4942548274993896\n",
      "2.457071542739868\n",
      "2.4678115844726562\n",
      "2.471449375152588\n",
      "2.4767727851867676\n",
      "2.4478671550750732\n",
      "2.4629323482513428\n",
      "2.4695990085601807\n",
      "2.4730305671691895\n",
      "2.4619526863098145\n",
      "2.488835096359253\n",
      "2.5050442218780518\n",
      "2.5148351192474365\n",
      "2.4838757514953613\n",
      "2.4185547828674316\n",
      "2.4891343116760254\n",
      "2.460601329803467\n",
      "2.47062087059021\n",
      "2.480907440185547\n",
      "2.4638960361480713\n",
      "2.507405996322632\n",
      "2.4883813858032227\n",
      "2.45377779006958\n",
      "2.50832200050354\n",
      "2.455286741256714\n",
      "2.4725699424743652\n",
      "2.4522387981414795\n",
      "2.4682388305664062\n",
      "2.4442577362060547\n",
      "2.471916675567627\n",
      "2.4605562686920166\n",
      "2.5031402111053467\n",
      "2.4920597076416016\n",
      "2.4747562408447266\n",
      "2.4600229263305664\n",
      "2.4556422233581543\n",
      "2.4434189796447754\n",
      "2.4540677070617676\n",
      "2.460390567779541\n",
      "2.4364774227142334\n",
      "2.472034215927124\n",
      "2.4398515224456787\n",
      "2.4607560634613037\n",
      "2.467365026473999\n",
      "2.4404947757720947\n",
      "2.4898390769958496\n",
      "2.459507942199707\n",
      "2.489739179611206\n",
      "2.4374027252197266\n",
      "2.4496216773986816\n",
      "2.463524103164673\n",
      "2.4499611854553223\n",
      "2.475109577178955\n",
      "2.4946136474609375\n",
      "2.468564748764038\n",
      "2.452531576156616\n",
      "2.4530391693115234\n",
      "2.475700855255127\n",
      "2.430607557296753\n",
      "2.437177896499634\n",
      "2.4600648880004883\n",
      "2.455045223236084\n",
      "2.449989080429077\n",
      "2.4689090251922607\n",
      "2.478977918624878\n",
      "2.4761316776275635\n",
      "2.4500951766967773\n",
      "2.461428165435791\n",
      "2.4558920860290527\n",
      "2.4576525688171387\n",
      "2.470937490463257\n",
      "2.458561420440674\n",
      "2.4824039936065674\n",
      "2.466064929962158\n",
      "2.477773666381836\n",
      "2.4520134925842285\n",
      "2.458402395248413\n",
      "2.452378988265991\n",
      "2.4824295043945312\n",
      "2.4540677070617676\n",
      "2.4864797592163086\n",
      "2.451059341430664\n",
      "2.441209077835083\n",
      "2.448249578475952\n",
      "2.433232069015503\n",
      "2.471363067626953\n",
      "2.450340509414673\n",
      "2.4546713829040527\n",
      "2.437375783920288\n",
      "2.453784704208374\n",
      "2.446676254272461\n",
      "2.4288768768310547\n",
      "2.434621572494507\n",
      "2.4470932483673096\n",
      "2.444725513458252\n",
      "2.4286022186279297\n",
      "2.4485228061676025\n",
      "2.4463088512420654\n",
      "2.49015474319458\n",
      "2.490738868713379\n",
      "2.466233491897583\n",
      "2.462009906768799\n",
      "2.481329917907715\n",
      "2.4620466232299805\n",
      "2.4619109630584717\n",
      "2.441633701324463\n",
      "2.4550280570983887\n",
      "2.447685956954956\n",
      "2.4745283126831055\n",
      "2.420293092727661\n",
      "2.4570305347442627\n",
      "2.4592697620391846\n",
      "2.465353012084961\n",
      "2.4835188388824463\n",
      "2.4423723220825195\n",
      "2.455388069152832\n",
      "2.4375457763671875\n",
      "2.4612951278686523\n",
      "2.4602229595184326\n",
      "2.4325501918792725\n",
      "2.445793390274048\n",
      "2.464007616043091\n",
      "2.426706314086914\n",
      "2.4744489192962646\n",
      "2.427809715270996\n",
      "2.4540462493896484\n",
      "2.4494025707244873\n",
      "2.4701719284057617\n",
      "2.48639178276062\n",
      "2.435673952102661\n",
      "2.4624812602996826\n",
      "2.437910318374634\n",
      "2.4156243801116943\n",
      "2.4315266609191895\n",
      "2.4709367752075195\n",
      "2.466919183731079\n",
      "2.4473934173583984\n",
      "2.4511821269989014\n",
      "2.4685704708099365\n",
      "2.461353302001953\n",
      "2.428138494491577\n",
      "2.471545457839966\n",
      "2.4254984855651855\n",
      "2.443925380706787\n",
      "2.4785244464874268\n",
      "2.445671319961548\n",
      "2.4591901302337646\n",
      "2.4718501567840576\n",
      "2.442249298095703\n",
      "2.464143753051758\n",
      "2.4429433345794678\n",
      "2.4613935947418213\n",
      "2.48380970954895\n",
      "2.4572410583496094\n",
      "2.4818243980407715\n",
      "2.4637556076049805\n",
      "2.482053518295288\n",
      "2.4765594005584717\n",
      "2.4577298164367676\n",
      "2.4701364040374756\n",
      "2.4467334747314453\n",
      "2.474364995956421\n",
      "2.4656448364257812\n",
      "2.4457125663757324\n",
      "2.43101167678833\n",
      "2.463881731033325\n",
      "2.419537305831909\n",
      "2.4660470485687256\n",
      "2.470505714416504\n",
      "2.419083595275879\n",
      "2.4331259727478027\n",
      "2.4239206314086914\n",
      "2.4381368160247803\n",
      "2.41184401512146\n",
      "2.3959715366363525\n",
      "2.4404919147491455\n",
      "2.462340831756592\n",
      "2.455929756164551\n",
      "2.458632230758667\n",
      "2.44148850440979\n",
      "2.43105411529541\n",
      "2.460616111755371\n",
      "2.462522506713867\n",
      "2.4357521533966064\n",
      "2.402815818786621\n",
      "2.4654414653778076\n",
      "2.4207825660705566\n",
      "2.444018840789795\n",
      "2.4080958366394043\n",
      "2.4219770431518555\n",
      "2.445411205291748\n",
      "2.3963537216186523\n",
      "2.4301881790161133\n",
      "2.4166295528411865\n",
      "2.427797794342041\n",
      "2.404975414276123\n",
      "2.460084915161133\n",
      "2.421786069869995\n",
      "2.4757070541381836\n",
      "2.444960832595825\n",
      "2.444063186645508\n",
      "2.4563660621643066\n",
      "2.425710439682007\n",
      "2.4579994678497314\n",
      "2.4447410106658936\n",
      "2.4180243015289307\n",
      "2.437764883041382\n",
      "2.4475226402282715\n",
      "2.4172379970550537\n",
      "2.42368483543396\n",
      "2.431817054748535\n",
      "2.4645156860351562\n",
      "2.401599884033203\n",
      "2.4013888835906982\n",
      "2.4114627838134766\n",
      "2.4211654663085938\n",
      "2.4094796180725098\n",
      "2.4260261058807373\n",
      "2.3799614906311035\n",
      "2.412524461746216\n",
      "2.4479820728302\n",
      "2.414710760116577\n",
      "2.4097907543182373\n",
      "2.3847222328186035\n",
      "2.4086802005767822\n",
      "2.405527114868164\n",
      "2.4359889030456543\n",
      "2.423201560974121\n",
      "2.4018189907073975\n",
      "2.386544704437256\n",
      "2.435582399368286\n",
      "2.3937246799468994\n",
      "2.4225351810455322\n",
      "2.4184815883636475\n",
      "2.412330389022827\n",
      "2.424995183944702\n",
      "2.4199283123016357\n",
      "2.4111552238464355\n",
      "2.404242753982544\n",
      "2.3975040912628174\n",
      "2.364901304244995\n",
      "2.390497922897339\n",
      "2.426374673843384\n",
      "2.3934485912323\n",
      "2.407641887664795\n",
      "2.4655895233154297\n",
      "2.399183750152588\n",
      "2.431417465209961\n",
      "2.440937042236328\n",
      "2.4376449584960938\n",
      "2.4515037536621094\n",
      "2.4040982723236084\n",
      "2.4764254093170166\n",
      "2.463327646255493\n",
      "2.525604248046875\n",
      "2.4710185527801514\n",
      "2.4416871070861816\n",
      "2.4742138385772705\n",
      "2.472069025039673\n",
      "2.4890990257263184\n",
      "2.402445077896118\n",
      "2.426435947418213\n",
      "2.4851536750793457\n",
      "2.4808244705200195\n",
      "2.474560022354126\n",
      "2.4517905712127686\n",
      "2.5037035942077637\n",
      "2.488131523132324\n",
      "2.4448840618133545\n",
      "2.521707534790039\n",
      "2.483612537384033\n",
      "2.490227460861206\n",
      "2.4669570922851562\n",
      "2.479085683822632\n",
      "2.445098638534546\n",
      "2.452791929244995\n",
      "2.445507287979126\n",
      "2.41786789894104\n",
      "2.47031569480896\n",
      "2.4132256507873535\n",
      "2.4230101108551025\n",
      "2.430511951446533\n",
      "2.4505245685577393\n",
      "2.4473822116851807\n",
      "2.465135335922241\n",
      "2.452413320541382\n",
      "2.5212955474853516\n",
      "2.531700372695923\n",
      "2.5462474822998047\n",
      "2.441427707672119\n",
      "2.445713758468628\n",
      "2.4760689735412598\n",
      "2.47563099861145\n",
      "2.488131046295166\n",
      "2.487699031829834\n",
      "2.4123239517211914\n",
      "2.4880728721618652\n",
      "2.4500551223754883\n",
      "2.477226734161377\n",
      "2.421546697616577\n",
      "2.4670214653015137\n",
      "2.4157328605651855\n",
      "2.404928684234619\n",
      "2.4203343391418457\n",
      "2.39361572265625\n",
      "2.424332857131958\n",
      "2.436997175216675\n",
      "2.458136558532715\n",
      "2.4512693881988525\n",
      "2.5085740089416504\n",
      "2.459895610809326\n",
      "2.417929172515869\n",
      "2.473712205886841\n",
      "2.426072359085083\n",
      "2.422481060028076\n",
      "2.452059745788574\n",
      "2.400331974029541\n",
      "2.4161312580108643\n",
      "2.3852405548095703\n",
      "2.4124515056610107\n",
      "2.3976261615753174\n",
      "2.397953987121582\n",
      "2.4007086753845215\n",
      "2.3758180141448975\n",
      "2.429189443588257\n",
      "2.403813600540161\n",
      "2.441622495651245\n",
      "2.4197819232940674\n",
      "2.359415054321289\n",
      "2.369260787963867\n",
      "2.4323670864105225\n",
      "2.354104518890381\n",
      "2.3568079471588135\n",
      "2.3425185680389404\n",
      "2.3838014602661133\n",
      "2.3348467350006104\n",
      "2.40899658203125\n",
      "2.3857994079589844\n",
      "2.365412712097168\n",
      "2.412811517715454\n",
      "2.3723185062408447\n",
      "2.358431816101074\n",
      "2.365967035293579\n",
      "2.3434770107269287\n",
      "2.3471686840057373\n",
      "2.33966064453125\n",
      "2.395310401916504\n",
      "2.3741343021392822\n",
      "2.3770384788513184\n",
      "2.36698055267334\n",
      "2.340836524963379\n",
      "2.374610662460327\n",
      "2.3665337562561035\n",
      "2.3682444095611572\n",
      "2.386279582977295\n",
      "2.3606812953948975\n",
      "2.2614893913269043\n",
      "2.367445468902588\n",
      "2.3018741607666016\n",
      "2.344468116760254\n",
      "2.376164197921753\n",
      "2.324071168899536\n",
      "2.3409032821655273\n",
      "2.433147430419922\n",
      "2.3594675064086914\n",
      "2.300645351409912\n",
      "2.347421646118164\n",
      "2.324044942855835\n",
      "2.3148341178894043\n",
      "2.2920281887054443\n",
      "2.336087465286255\n",
      "2.292940616607666\n",
      "2.272533655166626\n",
      "2.2753090858459473\n",
      "2.289076805114746\n",
      "2.2979273796081543\n",
      "2.297724723815918\n",
      "2.2807857990264893\n",
      "2.301450252532959\n",
      "2.307274341583252\n",
      "2.335901975631714\n",
      "2.310016393661499\n",
      "2.309051752090454\n",
      "2.328688383102417\n",
      "2.311603307723999\n",
      "2.2611420154571533\n",
      "2.271106719970703\n",
      "2.251763343811035\n",
      "2.2711100578308105\n",
      "2.2653067111968994\n",
      "2.3088631629943848\n",
      "2.2777180671691895\n",
      "2.2973783016204834\n",
      "2.308849573135376\n",
      "2.2879796028137207\n",
      "2.213268280029297\n",
      "2.291350841522217\n",
      "2.2135703563690186\n",
      "2.25183367729187\n",
      "2.2575149536132812\n",
      "2.267885208129883\n",
      "2.2822983264923096\n",
      "2.29085636138916\n",
      "2.3048739433288574\n",
      "2.2985377311706543\n",
      "2.298107624053955\n",
      "2.2457993030548096\n",
      "2.290388584136963\n",
      "2.2547051906585693\n",
      "2.278446912765503\n",
      "2.2956032752990723\n",
      "2.2826731204986572\n",
      "2.2080812454223633\n",
      "2.2762253284454346\n",
      "2.2501213550567627\n",
      "2.231238603591919\n",
      "2.221062660217285\n",
      "2.2649548053741455\n",
      "2.222165107727051\n",
      "2.2655580043792725\n",
      "2.247701406478882\n",
      "2.2428371906280518\n",
      "2.22525691986084\n",
      "2.2483649253845215\n",
      "2.256474494934082\n",
      "2.2178585529327393\n",
      "2.186959981918335\n",
      "2.2817888259887695\n",
      "2.2340381145477295\n",
      "2.220951795578003\n",
      "2.2164695262908936\n",
      "2.2376549243927\n",
      "2.2415077686309814\n",
      "2.2121572494506836\n",
      "2.188495397567749\n",
      "2.2285780906677246\n",
      "2.224403142929077\n",
      "2.22383189201355\n",
      "2.246866226196289\n",
      "2.183849811553955\n",
      "2.2124855518341064\n",
      "2.205082416534424\n",
      "2.270298719406128\n",
      "2.233456611633301\n",
      "2.2397055625915527\n",
      "2.2085437774658203\n",
      "2.1896114349365234\n",
      "2.227673053741455\n",
      "2.2182140350341797\n",
      "2.1883816719055176\n",
      "2.2293243408203125\n",
      "2.2257256507873535\n",
      "2.184657096862793\n",
      "2.233593225479126\n",
      "2.2209489345550537\n",
      "2.226296901702881\n",
      "2.2385900020599365\n",
      "2.2396607398986816\n",
      "2.2064785957336426\n",
      "2.2446396350860596\n",
      "2.22800350189209\n",
      "2.1510918140411377\n",
      "2.2441487312316895\n",
      "2.164642095565796\n",
      "2.2212748527526855\n",
      "2.2105345726013184\n",
      "2.235395908355713\n",
      "2.199822425842285\n",
      "2.2481586933135986\n",
      "2.226522922515869\n",
      "2.1905343532562256\n",
      "2.2339985370635986\n",
      "2.2101266384124756\n",
      "2.1842191219329834\n",
      "2.152491807937622\n",
      "2.1584317684173584\n",
      "2.181697368621826\n",
      "2.1450719833374023\n",
      "2.2118632793426514\n",
      "2.233637571334839\n",
      "2.114349365234375\n",
      "2.224550247192383\n",
      "2.1253607273101807\n",
      "2.161715269088745\n",
      "2.163821220397949\n",
      "2.2117719650268555\n",
      "2.204378128051758\n",
      "2.193472385406494\n",
      "2.1809463500976562\n",
      "2.1555745601654053\n",
      "2.170541763305664\n",
      "2.144062042236328\n",
      "2.170855760574341\n",
      "2.191706418991089\n",
      "2.189166307449341\n",
      "2.1721394062042236\n",
      "2.1170637607574463\n",
      "2.2002906799316406\n",
      "2.1542019844055176\n",
      "2.213535785675049\n",
      "2.1629953384399414\n",
      "2.199061155319214\n",
      "2.2007875442504883\n",
      "2.2242157459259033\n",
      "2.184890031814575\n",
      "2.1448049545288086\n",
      "2.1534621715545654\n",
      "2.169940233230591\n",
      "2.1873912811279297\n",
      "2.1463162899017334\n",
      "2.165692090988159\n",
      "2.166520595550537\n",
      "2.187943696975708\n",
      "2.153087854385376\n",
      "2.152935743331909\n",
      "2.167551040649414\n",
      "2.1429715156555176\n",
      "2.0537784099578857\n",
      "2.15386962890625\n",
      "2.130657434463501\n",
      "2.1948399543762207\n",
      "2.18772292137146\n",
      "2.115755319595337\n",
      "2.1821939945220947\n",
      "2.1496365070343018\n",
      "2.121117115020752\n",
      "2.168597459793091\n",
      "2.16721773147583\n",
      "2.145678758621216\n",
      "2.2036616802215576\n",
      "2.182307720184326\n",
      "2.10593581199646\n",
      "2.1820766925811768\n",
      "2.105901002883911\n",
      "2.1077046394348145\n",
      "2.0914864540100098\n",
      "2.1435599327087402\n",
      "2.1214799880981445\n",
      "2.140958786010742\n",
      "2.1752915382385254\n",
      "2.1778910160064697\n",
      "2.187039613723755\n",
      "2.182792901992798\n",
      "2.1059839725494385\n",
      "2.1786770820617676\n",
      "2.1591081619262695\n",
      "2.135416269302368\n",
      "2.1269452571868896\n",
      "2.138282537460327\n",
      "2.08215069770813\n",
      "2.1422903537750244\n",
      "2.164963722229004\n",
      "2.1205315589904785\n",
      "2.176236629486084\n",
      "2.1160354614257812\n",
      "2.069427490234375\n",
      "2.1827495098114014\n",
      "2.1314897537231445\n",
      "2.1589128971099854\n",
      "2.0842056274414062\n",
      "2.0530807971954346\n",
      "2.180525779724121\n",
      "2.14003324508667\n",
      "2.147118330001831\n",
      "2.1074161529541016\n",
      "2.150069236755371\n",
      "2.068181276321411\n",
      "2.1526498794555664\n",
      "2.0401597023010254\n",
      "2.0475964546203613\n",
      "2.143446207046509\n",
      "2.168630361557007\n",
      "2.1311964988708496\n",
      "2.1056344509124756\n",
      "2.073270082473755\n",
      "2.086702585220337\n",
      "2.087157964706421\n",
      "2.077049493789673\n",
      "2.114905595779419\n",
      "2.108633041381836\n",
      "2.107710599899292\n",
      "2.0953927040100098\n",
      "2.0866804122924805\n",
      "2.1048924922943115\n",
      "2.116956949234009\n",
      "2.0765304565429688\n",
      "2.075836420059204\n",
      "2.0840811729431152\n",
      "2.091808319091797\n",
      "2.1367809772491455\n",
      "2.060274362564087\n",
      "2.094108819961548\n",
      "2.095095634460449\n",
      "2.1263058185577393\n",
      "2.084833860397339\n",
      "2.0669078826904297\n",
      "2.0869388580322266\n",
      "2.0756194591522217\n",
      "2.0620360374450684\n",
      "2.0531747341156006\n",
      "2.040548086166382\n",
      "2.0232224464416504\n",
      "1.9699405431747437\n",
      "2.1530954837799072\n",
      "2.0347232818603516\n",
      "2.0255842208862305\n",
      "2.03175950050354\n",
      "2.0771491527557373\n",
      "2.0164549350738525\n",
      "2.040053367614746\n",
      "2.088379144668579\n",
      "2.0648951530456543\n",
      "2.0835297107696533\n",
      "2.0174102783203125\n",
      "2.0747387409210205\n",
      "2.0513925552368164\n",
      "2.023503065109253\n",
      "2.075169324874878\n",
      "2.0230488777160645\n",
      "2.049251079559326\n",
      "2.0367391109466553\n",
      "2.0632572174072266\n",
      "2.1108996868133545\n",
      "2.0295941829681396\n",
      "1.994724154472351\n",
      "1.9779446125030518\n",
      "2.010382890701294\n",
      "2.043513774871826\n",
      "2.03733491897583\n",
      "1.9897398948669434\n",
      "2.016779899597168\n",
      "2.029094696044922\n",
      "2.0186221599578857\n",
      "1.9916353225708008\n",
      "2.049736976623535\n",
      "1.9552046060562134\n",
      "2.0028023719787598\n",
      "1.9684661626815796\n",
      "2.0207207202911377\n",
      "2.040471076965332\n",
      "2.02609920501709\n",
      "2.071568489074707\n",
      "2.026536226272583\n",
      "2.01703143119812\n",
      "1.9379714727401733\n",
      "2.0624656677246094\n",
      "1.9883832931518555\n",
      "2.021362066268921\n",
      "1.9814974069595337\n",
      "2.015119791030884\n",
      "1.9464408159255981\n",
      "2.011906147003174\n",
      "2.0044260025024414\n",
      "2.030428409576416\n",
      "1.9417434930801392\n",
      "1.996705412864685\n",
      "2.011028289794922\n",
      "1.9480491876602173\n",
      "2.0436556339263916\n",
      "1.9365715980529785\n",
      "2.0077896118164062\n",
      "1.9322309494018555\n",
      "2.0387165546417236\n",
      "1.96172297000885\n",
      "2.0529801845550537\n",
      "2.0208539962768555\n",
      "1.9901337623596191\n",
      "2.030336380004883\n",
      "1.9468284845352173\n",
      "1.973629355430603\n",
      "1.9894227981567383\n",
      "1.975705623626709\n",
      "1.8982393741607666\n",
      "1.9000974893569946\n",
      "1.9830243587493896\n",
      "1.9759315252304077\n",
      "1.9702041149139404\n",
      "1.9694695472717285\n",
      "1.9875421524047852\n",
      "1.9004179239273071\n",
      "1.958074927330017\n",
      "2.009931802749634\n",
      "1.9553883075714111\n",
      "1.9572370052337646\n",
      "1.957532286643982\n",
      "1.9099267721176147\n",
      "1.9566528797149658\n",
      "1.9789602756500244\n",
      "1.9496657848358154\n",
      "1.935619831085205\n",
      "1.9658021926879883\n",
      "1.9069488048553467\n",
      "1.890798807144165\n",
      "1.9636061191558838\n",
      "1.98677659034729\n",
      "1.9486525058746338\n",
      "1.9438787698745728\n",
      "1.9079028367996216\n",
      "1.9691557884216309\n",
      "1.938112497329712\n",
      "1.9433228969573975\n",
      "1.9466149806976318\n",
      "1.9027588367462158\n",
      "1.9357068538665771\n",
      "1.862336277961731\n",
      "1.9286586046218872\n",
      "1.8873118162155151\n",
      "1.9645445346832275\n",
      "1.8877532482147217\n",
      "1.8825207948684692\n",
      "1.8961610794067383\n",
      "1.8872158527374268\n",
      "1.8915852308273315\n",
      "1.9297319650650024\n",
      "1.8849914073944092\n",
      "1.9039506912231445\n",
      "1.9140307903289795\n",
      "1.9297375679016113\n",
      "1.9643758535385132\n",
      "1.8187798261642456\n",
      "1.9268126487731934\n",
      "1.925171136856079\n",
      "1.9067331552505493\n",
      "1.8923314809799194\n",
      "1.9164106845855713\n",
      "1.9597218036651611\n",
      "1.8991717100143433\n",
      "1.914103627204895\n",
      "1.9452533721923828\n",
      "1.8844926357269287\n",
      "1.916271686553955\n",
      "1.8592844009399414\n",
      "1.8675322532653809\n",
      "1.874518632888794\n",
      "1.889488697052002\n",
      "1.888490080833435\n",
      "2.0113348960876465\n",
      "1.8669219017028809\n",
      "1.801755428314209\n",
      "1.8809679746627808\n",
      "1.909077763557434\n",
      "1.9497804641723633\n",
      "1.8882694244384766\n",
      "1.893785834312439\n",
      "1.887040376663208\n",
      "1.9094021320343018\n",
      "1.8418010473251343\n",
      "1.9107460975646973\n",
      "1.9639631509780884\n",
      "1.8268767595291138\n",
      "1.8429621458053589\n",
      "1.8847615718841553\n",
      "1.8094841241836548\n",
      "1.888609528541565\n",
      "1.872825026512146\n",
      "1.8893535137176514\n",
      "1.9177069664001465\n",
      "1.9418916702270508\n",
      "1.8419320583343506\n",
      "1.8810065984725952\n",
      "1.846190094947815\n",
      "1.9352747201919556\n",
      "1.8306604623794556\n",
      "1.8540534973144531\n",
      "1.7908713817596436\n",
      "1.850422978401184\n",
      "1.8715205192565918\n",
      "1.8782858848571777\n",
      "1.8364633321762085\n",
      "1.857749342918396\n",
      "1.7471362352371216\n",
      "1.9076236486434937\n",
      "1.8384313583374023\n",
      "1.872096061706543\n",
      "1.887388825416565\n",
      "1.8721156120300293\n",
      "1.8692915439605713\n",
      "1.8089486360549927\n",
      "1.8487094640731812\n",
      "1.8524445295333862\n",
      "1.8269554376602173\n",
      "1.8666460514068604\n",
      "1.8238478899002075\n",
      "1.8326938152313232\n",
      "1.8417901992797852\n",
      "1.7543715238571167\n",
      "1.8461416959762573\n",
      "1.8170822858810425\n",
      "1.863904356956482\n",
      "1.8702504634857178\n",
      "1.866972804069519\n",
      "1.858968734741211\n",
      "1.8862521648406982\n",
      "1.8223834037780762\n",
      "1.8297497034072876\n",
      "1.839223027229309\n",
      "1.846571922302246\n",
      "1.8419634103775024\n",
      "1.8334088325500488\n",
      "1.8497803211212158\n",
      "1.7914518117904663\n",
      "1.85255765914917\n",
      "1.878471851348877\n",
      "1.8375369310379028\n",
      "1.86070716381073\n",
      "1.7927098274230957\n",
      "1.84014892578125\n",
      "1.7994604110717773\n",
      "1.7959998846054077\n",
      "1.8334131240844727\n",
      "1.8704819679260254\n",
      "1.8518917560577393\n",
      "1.892931580543518\n",
      "1.8621160984039307\n",
      "1.7991911172866821\n",
      "1.837507724761963\n",
      "1.8266206979751587\n",
      "1.7798200845718384\n",
      "1.8151772022247314\n",
      "1.8602161407470703\n",
      "1.8127987384796143\n",
      "1.7367922067642212\n",
      "1.8060377836227417\n",
      "1.819912314414978\n",
      "1.765942096710205\n",
      "1.7863261699676514\n",
      "1.7857201099395752\n",
      "1.807968258857727\n",
      "1.7647863626480103\n",
      "1.7949036359786987\n",
      "1.840366244316101\n",
      "1.7804806232452393\n",
      "1.7347151041030884\n",
      "1.7859594821929932\n",
      "1.7753676176071167\n",
      "1.7735601663589478\n",
      "1.8054577112197876\n",
      "1.7901030778884888\n",
      "1.769459843635559\n",
      "1.721876621246338\n",
      "1.7988018989562988\n",
      "1.7928894758224487\n",
      "1.7803162336349487\n",
      "1.8009206056594849\n",
      "1.7946094274520874\n",
      "1.7945277690887451\n",
      "1.7831889390945435\n",
      "1.8357716798782349\n",
      "1.8239132165908813\n",
      "1.7786030769348145\n",
      "1.73148775100708\n",
      "1.7667114734649658\n",
      "1.756786823272705\n",
      "1.7688989639282227\n",
      "1.76505708694458\n",
      "1.7567479610443115\n",
      "1.7773629426956177\n",
      "1.772918462753296\n",
      "1.753970980644226\n",
      "1.778632640838623\n",
      "1.7607756853103638\n",
      "1.760275959968567\n",
      "1.7986624240875244\n",
      "1.7995736598968506\n",
      "1.804317593574524\n",
      "1.7507081031799316\n",
      "1.6778618097305298\n",
      "1.7935514450073242\n",
      "1.747755527496338\n",
      "1.7940640449523926\n",
      "1.8136481046676636\n",
      "1.7282902002334595\n",
      "1.7609658241271973\n",
      "1.8483712673187256\n",
      "1.7282209396362305\n",
      "1.7445942163467407\n",
      "1.8143571615219116\n",
      "1.7015583515167236\n",
      "1.7763055562973022\n",
      "1.7250722646713257\n",
      "1.7295711040496826\n",
      "1.6992716789245605\n",
      "1.7978737354278564\n",
      "1.6996549367904663\n",
      "1.710251808166504\n",
      "1.7221275568008423\n",
      "1.7496951818466187\n",
      "1.7840733528137207\n",
      "1.7604269981384277\n",
      "1.827921748161316\n",
      "1.7340714931488037\n",
      "1.7575242519378662\n",
      "1.6802011728286743\n",
      "1.7421181201934814\n",
      "1.7468743324279785\n",
      "1.7249637842178345\n",
      "1.7628809213638306\n",
      "1.710048794746399\n",
      "1.7780296802520752\n",
      "1.7535723447799683\n",
      "1.727008581161499\n",
      "1.667096495628357\n",
      "1.7526873350143433\n",
      "1.7284361124038696\n",
      "1.6871551275253296\n",
      "1.7146480083465576\n",
      "1.77047598361969\n",
      "1.7646452188491821\n",
      "1.7661322355270386\n",
      "1.6540088653564453\n",
      "1.7363414764404297\n",
      "1.6813387870788574\n",
      "1.695658564567566\n",
      "1.7529466152191162\n",
      "1.6757692098617554\n",
      "1.7015448808670044\n",
      "1.7732694149017334\n",
      "1.7024569511413574\n",
      "1.7172397375106812\n",
      "1.6968798637390137\n",
      "1.7480000257492065\n",
      "1.7619990110397339\n",
      "1.707172155380249\n",
      "1.6922037601470947\n",
      "1.7635653018951416\n",
      "1.7212162017822266\n",
      "1.7502334117889404\n",
      "1.6695035696029663\n",
      "1.7156192064285278\n",
      "1.666609764099121\n",
      "1.7068371772766113\n",
      "1.7298356294631958\n",
      "1.6842315196990967\n",
      "1.7166807651519775\n",
      "1.7362343072891235\n",
      "1.7615681886672974\n",
      "1.7373865842819214\n",
      "1.6952558755874634\n",
      "1.7573816776275635\n",
      "1.703443169593811\n",
      "1.6457328796386719\n",
      "1.727166771888733\n",
      "1.719059705734253\n",
      "1.7894152402877808\n",
      "1.7410759925842285\n",
      "1.681075930595398\n",
      "1.7204786539077759\n",
      "1.7207443714141846\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb, zb \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# xb: (B, T), yb: (B, T), zb: list of 4 tuples (np arrays (B,T,32))\u001b[39;00m\n\u001b[32m     38\u001b[39m     logits, loss = model(xb, yb, zb)   \u001b[38;5;66;03m# model unchanged\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     41\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe76faa0-3933-416d-b923-993c67cfcf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeond, thee, they your and a at\n",
      "The father, him man, be the fear me a perofe hight him\n",
      "And be the been him devered him with him with\n",
      "And the the can and see for so more the change,\n",
      "The Lestent the we then the worth that a hears\n",
      "As the eare of this stre be the death\n",
      "The see the him prevences me plesent strande,\n",
      "And thy man to the be and my so,\n",
      "The hou dot the so the should the the bed\n",
      "And his lover the good the on him.\n",
      "\n",
      "LARCE:\n",
      "O, my look will the father of your stander\n",
      "But with the him suplis and the long sound me hate\n",
      "And the so be the his more reforer wit\n",
      "And and not an be the with theing the and well;\n",
      "And my more thy best and he some the on be\n",
      "And and the was and and and the king strue forter,\n",
      "That see the so my death she be thou form speant\n",
      "With see deater and the my loven ever to dels,\n",
      "And the we be not of the more well Pape,\n",
      "On the this the with the feath the come not,\n",
      "Your love the his hearted the the more forth,\n",
      "Which the see beat and see in and not to she\n",
      "And me dear, and my less this on the for her,\n",
      "And\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "def decode_sequence_char(\n",
    "    model, stoi, itos, prompt, max_new_tokens=100, block_size=256,\n",
    "    use_fenchel=False, tau=1.0, fenchel_iters=3, temperature=1.0\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    max_ctx = int(block_size)\n",
    "\n",
    "    # ?1 assume space token exists; fallback to 0 if missing\n",
    "    space_id = stoi.get(' ', 0)\n",
    "\n",
    "    # encode prompt\n",
    "    start_ids = torch.tensor([encode_chars(prompt, stoi)], dtype=torch.long, device=device)\n",
    "\n",
    "    # prepend the pad once; from now on the window just slides\n",
    "\n",
    "    idx = start_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = idx[:, -max_ctx:]  # rotating buffer: last pad_len+block_size tokens\n",
    "        logits, _ = model(context, None)\n",
    "        last_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    # drop the initial pad when returning the string\n",
    "    out_tokens = idx[0].tolist()\n",
    "    return decode_chars(out_tokens, itos)\n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "\n",
    "prompt = \"Romeo\"\n",
    "generated = decode_sequence_char(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=1024,\n",
    "    block_size=1024,\n",
    "    use_fenchel=False,\n",
    "    tau=1.5,\n",
    "    fenchel_iters=2,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee122bcb-2bba-41b4-8e62-8a1783c1f574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08025ccd-88ac-46f0-bbb6-7b5b3749ce6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056acdf-22c3-4e83-bdaa-dd6d2fbbeb83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
