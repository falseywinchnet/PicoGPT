{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7741bdf-98ab-4400-a0d1-4b831244ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "# --- GPT with auxiliary reverse-embedding loss from zb ---\n",
    "import math, inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# assumes Block and LayerNorm are defined elsewhere (as in your current setup)\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig,\n",
    "                 aux_scale: float = 1.0,\n",
    "                 noise_constituent: float = 1e-4,\n",
    "                 noise_final: float = 1e-4):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None and config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        # per-block orthonormal maps for blocks 0..3\n",
    "        need_blocks = 4\n",
    "        if config.n_layer < need_blocks:\n",
    "            raise ValueError(f\"need at least {need_blocks} transformer blocks for aux; got {config.n_layer}\")\n",
    "        self.aux_blocks = list(range(need_blocks))  # [0,1,2,3] fixed\n",
    "        self.aux_maps = nn.ModuleList()\n",
    "        for _ in self.aux_blocks:\n",
    "            lin = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            nn.init.orthogonal_(lin.weight)  # square => orthonormal rows & columns\n",
    "            self.aux_maps.append(lin)\n",
    "\n",
    "        # noise/scales\n",
    "        self.aux_scale_default = float(aux_scale)\n",
    "        self.noise_constituent = float(noise_constituent)\n",
    "        self.noise_final = float(noise_final)\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # reverse-embedding for one lane (list length T of [idxs, probs])\n",
    "    def _rev_embed_lane(self, lane_seq, device):\n",
    "        T = len(lane_seq)\n",
    "        if T == 0:\n",
    "            return torch.empty(0, self.config.n_embd, device=device)\n",
    "        idxs = torch.tensor([pair[0] for pair in lane_seq], device=device, dtype=torch.long)      # (T, K)\n",
    "        probs = torch.tensor([pair[1] for pair in lane_seq], device=device, dtype=torch.float32)  # (T, K)\n",
    "        probs = probs / (probs.sum(dim=-1, keepdim=True) + 1e-12)\n",
    "\n",
    "        E = self.transformer.wte.weight  # (V, D)\n",
    "        emb = E.index_select(0, idxs.reshape(-1)).reshape(*idxs.shape, E.size(1))  # (T, K, D)\n",
    "        if self.noise_constituent > 0:\n",
    "            emb = emb + torch.randn_like(emb) * self.noise_constituent\n",
    "        rev = torch.einsum('tkd,tk->td', emb, probs)  # (T, D)\n",
    "        if self.noise_final > 0:\n",
    "            rev = rev + torch.randn_like(rev) * self.noise_final\n",
    "        return rev\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 66\n",
    "    n_layer: int = 4          # explicitly 4 per your instruction\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "\n",
    "    # -------- reverse-embedding helpers (batchified, same dtype/device as x) --------\n",
    "def soft_ce(logits: torch.Tensor, target_probs: torch.Tensor, ignore_mask =None):\n",
    "        \"\"\"\n",
    "        logits: (B, T, V)\n",
    "        target_probs: (B, T, V) row-normalized\n",
    "        ignore_mask: (B, T) bool, True where we KEEP, False to ignore\n",
    "        \"\"\"\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        loss = -(target_probs * logp).sum(dim=-1)  # (B, T)\n",
    "        if ignore_mask is not None:\n",
    "            loss = loss * ignore_mask\n",
    "            denom = torch.clamp(ignore_mask.sum(), min=1)\n",
    "            return loss.sum() / denom\n",
    "        else:\n",
    "            return loss.mean()\n",
    "\n",
    "def sharpen_distribution(idx: torch.Tensor, p: torch.Tensor, V: int, alpha: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: (B, T, K) long\n",
    "        p:   (B, T, K) float\n",
    "        return dense (B, T, V) probs with sharpening exponent alpha (alpha>1 => more peaked)\n",
    "        \"\"\"\n",
    "        B, T, K = idx.shape\n",
    "        out = torch.full((B, T, V), 0.0, dtype=p.dtype, device=p.device)\n",
    "        # apply exponent (temperature-like). alpha==1 means unchanged\n",
    "        q = torch.clamp(p, min=1e-12) ** alpha\n",
    "        q = q / q.sum(dim=-1, keepdim=True)\n",
    "        out.scatter_add_(dim=-1, index=idx, src=q)\n",
    "        return out\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class StochasticEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    A swap-in replacement for nn.Embedding that sometimes \"smudges\" inputs by\n",
    "    replacing a hard index lookup with a small weighted mixture over up to K tokens.\n",
    "\n",
    "    Key behavior:\n",
    "    - With probability `mix_prob` (per position), use a mixture; otherwise do a standard idx->embedding.\n",
    "    - The mixture *always* includes the original idx and keeps it as the most likely token.\n",
    "    - The remaining mixture members (up to `max_blend`) are:\n",
    "        * If Z is provided: taken from the previous position's Z-index candidates, with randomized sharpening.\n",
    "        * Else: random distinct tokens (excluding idx).\n",
    "    - Weights are *uneven* and decay gradually (geometric), then normalized.\n",
    "    - Position t=0 is always pure (hard idx).\n",
    "    - During eval mode, mixing is disabled by default (can be enabled with `enable_in_eval=True`).\n",
    "\n",
    "    Expected Z format (optional):\n",
    "        z_prev = (Z_idx, Z_p)\n",
    "        shapes: Z_idx: (B, T, Kz), Z_p: (B, T, Kz)\n",
    "        For input at time t, we *key* into Z at t-1 (previous position).\n",
    "        If Z is provided for the whole sequence, you can pass it directly and this layer\n",
    "        will handle the t-1 alignment internally.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings: vocab size\n",
    "        embedding_dim:  embedding size\n",
    "        mix_prob:       probability to use a mixture at a given (b, t) position\n",
    "        max_blend:      up to this many *additional* indices beyond `idx` (<=16 as requested)\n",
    "        alpha_range:    (low, high) range for random sharpening exponent applied to Z probs\n",
    "        geom_decay_range: range for random geometric ratio r in (low, high) to create uneven, decaying weights\n",
    "        ensure_idx_top_margin: multiplicative margin to ensure idx remains strictly the top prob\n",
    "        enable_in_eval: if True, still perform mixing during model.eval(); else mixing only in training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        mix_prob: float = 0.3,\n",
    "        max_blend: int = 16,\n",
    "        alpha_range: Tuple[float, float] = (0.8, 2.0),\n",
    "        geom_decay_range: Tuple[float, float] = (0.55, 0.9),\n",
    "        ensure_idx_top_margin: float = 1.05,\n",
    "        enable_in_eval: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(num_embeddings, embedding_dim, **kwargs)\n",
    "        if max_blend < 0 or max_blend > 16:\n",
    "            raise ValueError(\"max_blend must be in [0, 16].\")\n",
    "        self.mix_prob = float(mix_prob)\n",
    "        self.max_blend = int(max_blend)\n",
    "        self.alpha_range = alpha_range\n",
    "        self.geom_decay_range = geom_decay_range\n",
    "        self.ensure_idx_top_margin = float(ensure_idx_top_margin)\n",
    "        self.enable_in_eval = bool(enable_in_eval)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _rand_geom_weights(self, k: int, device: torch.device, dtype: torch.dtype) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create a strictly decreasing, uneven weight vector of length k via a random geometric ratio.\n",
    "        Returns a normalized vector (sums to 1).\n",
    "        \"\"\"\n",
    "        r = torch.empty(1, device=device, dtype=dtype).uniform_(*self.geom_decay_range).item()  # scalar ratio\n",
    "        # weights ~ [1, r, r^2, ...]\n",
    "        exps = torch.arange(k, device=device, dtype=dtype)\n",
    "        w = torch.pow(torch.tensor(r, device=device, dtype=dtype), exps)\n",
    "        w = w / (w.sum() + 1e-12)\n",
    "        return w\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _apply_sharpening(self, probs: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sharpen/widen probabilities: q_i ‚àù p_i^alpha. alpha>1 => more peaked, alpha<1 => flatter.\n",
    "        \"\"\"\n",
    "        q = torch.clamp(probs, min=1e-12) ** alpha\n",
    "        q = q / (q.sum() + 1e-12)\n",
    "        return q\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        idx: torch.Tensor,\n",
    "        z_prev: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # (Z_idx, Z_p), each (B,T,Kz)\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: (B, T) Long\n",
    "        z_prev: optional tuple (Z_idx, Z_p), both shaped (B, T, Kz).\n",
    "                For current position t, we look at z_prev[:, t-1] (previous position).\n",
    "        Returns:\n",
    "            embeddings: (B, T, D)\n",
    "        \"\"\"\n",
    "        if idx.dtype != torch.long:\n",
    "            raise TypeError(\"idx must be torch.long\")\n",
    "\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "        D = self.embedding_dim\n",
    "        E = self.weight  # (V, D)\n",
    "\n",
    "        # Early exit if mixing disabled (e.g., eval mode and not enabled)\n",
    "        mixing_active = self.training or self.enable_in_eval\n",
    "        if not mixing_active or self.mix_prob <= 0.0 or self.max_blend == 0:\n",
    "            # Just standard embedding, except we always keep t=0 pure anyway (same outcome here).\n",
    "            return super().forward(idx)\n",
    "\n",
    "        # We will construct output one position group at a time for clarity.\n",
    "        out = torch.empty(B, T, D, device=device, dtype=E.dtype)\n",
    "\n",
    "        # Sample where to mix (Bernoulli per token position), but force t==0 to False.\n",
    "        mix_mask = torch.rand(B, T, device=device) < self.mix_prob\n",
    "        if T > 0:\n",
    "            mix_mask[:, 0] = False  # first element always x (pure)\n",
    "        if model.training == False:\n",
    "            mix_mask[:,:]= False#always pure when eval\n",
    "\n",
    "        # Helper to do a pure lookup fast:\n",
    "        if (~mix_mask).any():\n",
    "            out[~mix_mask] = E[idx[~mix_mask]]\n",
    "\n",
    "        # If we have no positions to mix, we are done.\n",
    "        if not mix_mask.any():\n",
    "            return out\n",
    "\n",
    "        # If Z is provided, we will use previous position (t-1) candidates to key the mixture.\n",
    "        have_z = z_prev is not None\n",
    "        if have_z:\n",
    "            Z_idx, Z_p = z_prev\n",
    "            assert Z_idx.shape[:2] == (B, T) and Z_p.shape[:2] == (B, T), \"Z shapes must be (B,T, Kz)\"\n",
    "            assert Z_idx.shape == Z_p.shape, \"Z_idx and Z_p must have identical shapes\"\n",
    "            # We'll access Z at t-1 for each position t; t=0 is never mixed per mask anyway.\n",
    "\n",
    "        # Process mixing positions, possibly in a light loop (readability > micro-optimizations)\n",
    "        mix_positions = mix_mask.nonzero(as_tuple=False)  # list of (b, t) rows\n",
    "        V = E.size(0)\n",
    "\n",
    "        for b, t in mix_positions.tolist():\n",
    "            cur_idx = idx[b, t].item()\n",
    "\n",
    "            # Build candidate set: always include cur_idx first\n",
    "            cand = [cur_idx]\n",
    "            cand_weights = None  # will be set later (unnormalized)\n",
    "\n",
    "            if have_z and t > 0:\n",
    "                # Use Z keyed from previous position\n",
    "                z_i = Z_idx[b, t - 1]  # (Kz,)\n",
    "                z_p = Z_p[b, t - 1]    # (Kz,)\n",
    "\n",
    "                # Randomly sharpen/widen Z\n",
    "                alpha = float(torch.empty(1, device=device).uniform_(*self.alpha_range).item())\n",
    "                z_q = self._apply_sharpening(z_p.to(torch.float32), alpha)  # (Kz,)\n",
    "\n",
    "                # Take top candidates from z (excluding cur_idx), up to max_blend\n",
    "                # We'll sort by z_q descending to preserve a \"gradual reduction\" feel.\n",
    "                top_vals, top_idx = torch.topk(z_q, k=min(self.max_blend + 4, z_q.numel()))  # oversample a bit\n",
    "                z_cands = []\n",
    "                for j in top_idx.tolist():\n",
    "                    tok = int(z_i[j].item())\n",
    "                    if tok != cur_idx and tok not in z_cands:\n",
    "                        z_cands.append(tok)\n",
    "                        if len(z_cands) >= self.max_blend:\n",
    "                            break\n",
    "\n",
    "                # If Z didn't yield enough distincts (rare), fill randomly\n",
    "                need = self.max_blend - len(z_cands)\n",
    "                if need > 0:\n",
    "                    # sample without replacement, excluding cur_idx and z_cands\n",
    "                    exclude = set([cur_idx] + z_cands)\n",
    "                    # Draw a bit more than needed and filter\n",
    "                    rand_pool = torch.randperm(V, device=device)[: max(need * 4, need + 8)].tolist()\n",
    "                    for tok in rand_pool:\n",
    "                        if tok not in exclude:\n",
    "                            z_cands.append(tok)\n",
    "                            exclude.add(tok)\n",
    "                            if len(z_cands) >= self.max_blend:\n",
    "                                break\n",
    "\n",
    "                cand.extend(z_cands)\n",
    "\n",
    "                # Create uneven, decaying weights aligned with sorted z_q order\n",
    "                # Start from geometric decay, then bias by z_q ranks a bit\n",
    "                k_total = len(cand)\n",
    "                geom_w = self._rand_geom_weights(k_total, device=device, dtype=torch.float32)\n",
    "\n",
    "                # Modulate the tail (excluding idx at position 0) by the (sorted) z_q values we used\n",
    "                # to keep a Z-keyed shape. Map z_cands in the order we picked them (already roughly top->down).\n",
    "                tail = geom_w[1:].clone()\n",
    "                if len(z_cands) > 0:\n",
    "                    # z shape weights based on normalized top_vals (already top->down)\n",
    "                    zshape = top_vals[: len(z_cands)].to(torch.float32)\n",
    "                    zshape = zshape / (zshape.sum() + 1e-12)\n",
    "                    # mix geometric decay with zshape to keep unevenness and Z-key\n",
    "                    tail = 0.5 * tail + 0.5 * (zshape * tail.sum())  # keep total tail mass\n",
    "\n",
    "                # Reassemble, ensure idx (slot 0) is strongest\n",
    "                cand_weights = torch.cat([geom_w[0:1], tail], dim=0)\n",
    "\n",
    "            else:\n",
    "                # No Z available (or t==0, but t==0 never mixes). Fall back to random candidates with decaying weights.\n",
    "                # Sample distinct random tokens excluding cur_idx\n",
    "                need = self.max_blend\n",
    "                exclude = {cur_idx}\n",
    "                rand_cands = []\n",
    "                # Draw a bit more than needed for uniqueness\n",
    "                for tok in torch.randperm(V, device=device)[: max(need * 4, need + 8)].tolist():\n",
    "                    if tok not in exclude:\n",
    "                        rand_cands.append(tok)\n",
    "                        exclude.add(tok)\n",
    "                        if len(rand_cands) >= need:\n",
    "                            break\n",
    "                cand.extend(rand_cands)\n",
    "                # Geometric decay weights (uneven)\n",
    "                cand_weights = self._rand_geom_weights(len(cand), device=device, dtype=torch.float32)\n",
    "\n",
    "            # Make sure the first element (the true idx) has the HIGHEST probability\n",
    "            # by granting it a small multiplicative margin, then renormalize.\n",
    "            cand_weights = cand_weights.clone()\n",
    "            cand_weights[0] = cand_weights[0] * self.ensure_idx_top_margin\n",
    "            cand_weights = cand_weights / (cand_weights.sum() + 1e-12)\n",
    "\n",
    "            # Turn the mixture into an embedding: sum_j p_j * E[cand_j]\n",
    "            cand_t = torch.tensor(cand, device=device, dtype=torch.long)\n",
    "            vecs = E.index_select(0, cand_t)           # (k, D)\n",
    "            probs = cand_weights.view(-1, 1).to(vecs)  # (k, 1)\n",
    "            mixed = (vecs * probs).sum(dim=0)          # (D,)\n",
    "            out[b, t] = mixed\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig,\n",
    "                 aux_scale: float = 1.0,           # (12) fixed\n",
    "                 noise_constituent: float = 1e-6,    # (7) fixed\n",
    "                 noise_final: float = 1e-4):         # (7) fixed\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None and config.block_size is not None\n",
    "\n",
    "        self.config = config\n",
    "        self.aux_scale = float(aux_scale)\n",
    "        self.noise_constituent = float(noise_constituent)\n",
    "        self.noise_final = float(noise_final)\n",
    "\n",
    "        # core transformer\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = StochasticEmbedding(\n",
    "            config.vocab_size, config.n_embd,\n",
    "            mix_prob=0.3,          # tune\n",
    "            max_blend=16,          # per your cap\n",
    "            alpha_range=(0.8, 2.0) # gentle->sharp randomization\n",
    "        ),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        #self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        # per-block orthonormal linears (square D√óD, columns orthonormal)\n",
    "        self.aux_maps = nn.ModuleList()\n",
    "        for _ in range(4):\n",
    "            lin = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            nn.init.orthogonal_(lin.weight)\n",
    "            self.aux_maps.append(lin)\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # --- Patch your GPT class: replace forward with the following ---\n",
    "    def forward(self, idx, targets=None, zb=None,\n",
    "                               aux_scale: float = 1.0,\n",
    "                               depth_alphas  = None,\n",
    "                               warmup_ignores  = None):\n",
    "        \"\"\"\n",
    "        idx: (B, T) Long\n",
    "        targets: (B, T) Long or None\n",
    "        zb: tuple (Z_idx, Z_p) each (B, T, K); single Z shared across blocks\n",
    "        depth_alphas: per-block sharpening exponents (len == n_layer). e.g. [0.8, 1.0, 1.5, 2.0, ...]\n",
    "                      alpha<1 widens early, >1 sharpens later ‚Äî approximates your 'gaussian‚Üípeaked student-t'\n",
    "        warmup_ignores: per-block number of initial positions to ignore for aux\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size\n",
    "    \n",
    "        # embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "        tok_emb = self.transformer.wte(idx, z_prev=zb)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "    \n",
    "        # defaults\n",
    "        L = len(self.transformer.h)\n",
    "        if depth_alphas is None:\n",
    "            # gentle ‚Üí sharp\n",
    "            depth_alphas = [0.8 + 1.2 * (i/(L-1)) for i in range(L)] if L > 1 else [1.0]\n",
    "        if warmup_ignores is None:\n",
    "            # ignore more in shallow blocks\n",
    "            warmup_ignores = [min(2**i - 1, T-1) for i in range(L)]  # 0,1,3,7,... capped\n",
    "    \n",
    "        # unpack Z\n",
    "        Z_idx, Z_p = zb if zb is not None else (None, None)\n",
    "    \n",
    "        aux_loss = None\n",
    "        for bidx, block in enumerate(self.transformer.h):\n",
    "            x = block(x)  # (B, T, D)\n",
    "            if Z_idx is not None and Z_p is not None:\n",
    "                V = self.lm_head.out_features\n",
    "                logits_b = self.lm_head(self.transformer.ln_f(x))  # (B, T, V)\n",
    "    \n",
    "                # per-depth skew of the SAME base Z\n",
    "                Z_dense = sharpen_distribution(Z_idx, Z_p, V, alpha=float(depth_alphas[bidx]))\n",
    "    \n",
    "                # warmup ignore mask (keep positions >= ignore)\n",
    "                keep = torch.arange(T, device=device).expand(B, T) >= int(warmup_ignores[bidx])\n",
    "                keep = keep.to(logits_b.dtype)\n",
    "                aux_b = soft_ce(logits_b, Z_dense, ignore_mask=(keep>0))\n",
    "                aux_loss = aux_b if aux_loss is None else aux_loss + aux_b\n",
    "    \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "    \n",
    "        ce_loss = None\n",
    "        if targets is not None:\n",
    "            ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "                                      targets.view(-1), ignore_index=-100)\n",
    "    \n",
    "        total = None\n",
    "        if ce_loss is None and aux_loss is None:\n",
    "            total = None\n",
    "        elif aux_loss is None:\n",
    "            total = ce_loss\n",
    "        elif ce_loss is None:\n",
    "            total = aux_scale * aux_loss\n",
    "        else:\n",
    "            total = ce_loss + aux_scale * aux_loss\n",
    "    \n",
    "        if targets is None:\n",
    "            logits = logits[:, [-1], :]\n",
    "    \n",
    "        return logits, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e1f1c-1f54-43b7-975c-4229c4ae1866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d76ace-c3c1-4a68-adce-851c842dbbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading aochildes.txt...\n",
      "üì• Downloading cbt.txt...\n",
      "üì• Downloading children_stories.txt...\n",
      "üì• Downloading gutenberg.txt...\n",
      "üì• Downloading qed.txt...\n",
      "üì• Downloading simple_wikipedia.txt...\n",
      "üì• Downloading switchboard.txt...\n",
      "üì• Downloading wikipedia.txt...\n",
      "üì• Downloading shakespeare.txt...\n",
      "‚úÖ Done. Files saved to ./babylm_10m_cleaned\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"üì• Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"‚úÖ Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95a695ab-753c-4b35-8834-a1d4f59859bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Char tokenizer finalized.\n",
      "üßæ Train tokens: 1016242 | Val tokens: 99152\n",
      "üî§ Vocab size: 66\n",
      "Loaded 1016242 train tokens and 99152 val tokens | vocab=66\n",
      "Building order-2 Markov...\n",
      "Building order-4 Markov...\n",
      "Building order-8 Markov...\n",
      "Building order-16 Markov...\n",
      "Building order-32 Markov...\n",
      "Building order-64 Markov...\n",
      "Building bigram db...\n",
      "‚úÖ Markov and Bigram models saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Char tokenizer finalized.\")\n",
    "print(f\"üßæ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"üî§ Vocab size: {len(stoi)}\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "data_dir = \"./babylm_char_tokenized\"\n",
    "train_path = os.path.join(data_dir, \"train.bin\")\n",
    "val_path   = os.path.join(data_dir, \"val.bin\")\n",
    "meta_path  = os.path.join(data_dir, \"meta.pkl\")\n",
    "train_ids = np.fromfile(train_path, dtype=np.uint16)\n",
    "val_ids   = np.fromfile(val_path,   dtype=np.uint16)\n",
    "\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "vocab_size = meta[\"vocab_size\"]\n",
    "\n",
    "def global_freqs(ids: np.ndarray, V: int) -> np.ndarray:\n",
    "    cnt = np.bincount(ids.astype(np.int64), minlength=V)\n",
    "    # normalize to probability (avoid zero)\n",
    "    p = cnt.astype(np.float64)\n",
    "    p = p / max(1.0, p.sum())\n",
    "    return p\n",
    "print(f\"Loaded {len(train_ids)} train tokens and {len(val_ids)} val tokens | vocab={vocab_size}\")\n",
    "p_global = global_freqs(train_ids, vocab_size)  # used for disciplined fill only\n",
    "\n",
    "def build_markov_chain(data: np.ndarray, window: int) -> Dict[Tuple[int, ...], Counter]:\n",
    "    chain = defaultdict(Counter)\n",
    "    for i in range(len(data) - window):\n",
    "        ctx = tuple(map(int, data[i:i+window]))\n",
    "        nxt = int(data[i+window])\n",
    "        chain[ctx][nxt] += 1\n",
    "    return chain\n",
    "\n",
    "ngram_orders = [2,4,8,16,32,64]\n",
    "markov_models: Dict[int, Dict[Tuple[int,...], Counter]] = {}\n",
    "for w in ngram_orders:\n",
    "    print(f\"Building order-{w} Markov...\")\n",
    "    markov_models[w] = build_markov_chain(train_ids, w)\n",
    "\n",
    "def build_bigram_db(data: np.ndarray, V: int, top_k=16, epsilon=1e-6, seed=1337):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    counts = np.zeros((V, V), dtype=np.int64)\n",
    "    a = data[:-1].astype(np.int64)\n",
    "    b = data[1:].astype(np.int64)\n",
    "    np.add.at(counts, (a, b), 1)\n",
    "    out = {}\n",
    "    all_ids = np.arange(V, dtype=np.int64)\n",
    "    for t in range(V):\n",
    "        row = counts[t]\n",
    "        tot = row.sum()\n",
    "        if tot == 0:\n",
    "            idx = rng.choice(V, size=top_k, replace=False)\n",
    "            p = np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "        else:\n",
    "            pr = row.astype(np.float64) / float(tot)\n",
    "            obs = np.flatnonzero(row)\n",
    "            if len(obs) >= top_k:\n",
    "                sel = np.argpartition(pr[obs], -top_k)[-top_k:]\n",
    "                idx = obs[sel]\n",
    "                p = pr[idx].astype(np.float32)\n",
    "                s = p.sum()\n",
    "                p = p/s if s > 0 else np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "            else:\n",
    "                need = top_k - len(obs)\n",
    "                mask = np.ones(V, dtype=bool); mask[obs] = False\n",
    "                extra = np.random.default_rng(seed+t).choice(np.nonzero(mask)[0], size=need, replace=False)\n",
    "                idx = np.concatenate([obs, extra])\n",
    "                p   = pr[idx].astype(np.float32)\n",
    "                # give epsilon to never-seen extras\n",
    "                unseen = (row[idx] == 0)\n",
    "                if unseen.any():\n",
    "                    p = p + unseen.astype(np.float32) * epsilon\n",
    "                p = p / p.sum()\n",
    "        order = np.argsort(-p)\n",
    "        out[t] = (idx[order].astype(np.int64), p[order])\n",
    "    return out\n",
    "\n",
    "print(\"Building bigram db...\")\n",
    "bigram_db = build_bigram_db(train_ids, vocab_size, top_k=64)  # collect a bit wider; we'll cap later\n",
    "\n",
    "# === Save ===\n",
    "model_dir = \"./markov_bigram_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(model_dir, \"markov_models.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(markov_models, f)\n",
    "\n",
    "with open(os.path.join(model_dir, \"bigram_db.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(bigram_db, f)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Markov and Bigram models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcae90-1cdc-4ef5-8907-75961e9a5ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57eff45-c482-4e59-ace0-5c35e65c879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd4dd64a-8f8e-471e-97f2-7ffd1df4c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "# === Replacement dataloader that uses SAVED bigram + markov models and yields (X, Y, Z) ===\n",
    "import os, pickle, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# expects `vocab_size` and `device` already defined in the outer scope\n",
    "# expects saved models at ./markov_bigram_models/{bigram_db.pkl, markov_models.pkl}\n",
    "\n",
    "class DisciplinedZ:\n",
    "    def __init__(self, markov_models: Dict[int, Dict[Tuple[int,...], Counter]],\n",
    "                 bigram_db: Dict[int, Tuple[np.ndarray, np.ndarray]],\n",
    "                 p_global: np.ndarray,\n",
    "                 vocab_size: int,\n",
    "                 top_k: int = 32,\n",
    "                 epsilon: float = 1e-6):\n",
    "        self.models = markov_models\n",
    "        self.bigram_db = bigram_db\n",
    "        self.p_global = p_global.astype(np.float64)\n",
    "        self.V = vocab_size\n",
    "        self.K = top_k\n",
    "        self.eps = float(epsilon)\n",
    "        # global sort for fill\n",
    "        self.global_order = np.argsort(-self.p_global)\n",
    "\n",
    "    def _cands_from_counter(self, ctr: Optional[Counter]) -> Optional[np.ndarray]:\n",
    "        if not ctr:\n",
    "            return None\n",
    "        return np.fromiter((int(t) for t,_ in ctr.items()), dtype=np.int64)\n",
    "\n",
    "    def _probs_from_counter(self, ctr: Optional[Counter]) -> Optional[Dict[int, float]]:\n",
    "        if not ctr:\n",
    "            return None\n",
    "        tot = sum(ctr.values())\n",
    "        if tot == 0:\n",
    "            return None\n",
    "        return {int(t): c/tot for t, c in ctr.items()}\n",
    "\n",
    "    def _bigram_top(self, tok: int, limit: int) -> np.ndarray:\n",
    "        idx, prob = self.bigram_db.get(int(tok), (None, None))\n",
    "        if idx is None:\n",
    "            return np.array([], dtype=np.int64)\n",
    "        return idx[:limit]\n",
    "\n",
    "    def _btree_candidates(self, contexts: Dict[int, Tuple[Tuple[int,...], Optional[Counter]]], backoff_tok: int) -> np.ndarray:\n",
    "        # collect candidate sets from each available context\n",
    "        sets = []\n",
    "        for n, (_, ctr) in contexts.items():\n",
    "            c = self._cands_from_counter(ctr)\n",
    "            if c is not None and c.size > 0:\n",
    "                sets.append(set(c.tolist()))\n",
    "        if len(sets) == 0:\n",
    "            # no ctx ‚Üí use bigram set as starting point\n",
    "            return self._bigram_top(backoff_tok, self.K)\n",
    "\n",
    "        # try full intersection; if empty, progressively intersect strongest contexts first\n",
    "        inter = set.intersection(*sets) if len(sets) > 1 else sets[0]\n",
    "        if len(inter) == 0:\n",
    "            # heuristic: sort by context order (longest first), intersect greedily\n",
    "            sets_sorted = sorted(sets, key=lambda s: -len(s))\n",
    "            inter = sets_sorted[0].copy()\n",
    "            for s in sets_sorted[1:]:\n",
    "                new_inter = inter.intersection(s)\n",
    "                if len(new_inter) > 0:\n",
    "                    inter = new_inter\n",
    "        if len(inter) == 0:\n",
    "            # last resort: union (still disciplined; no random injection)\n",
    "            union = set()\n",
    "            for s in sets:\n",
    "                union |= s\n",
    "            inter = union\n",
    "\n",
    "        arr = np.fromiter(inter, dtype=np.int64)\n",
    "        if arr.size == 0:\n",
    "            return self._bigram_top(backoff_tok, self.K)\n",
    "        return arr\n",
    "\n",
    "    def _score_candidates(self, cands: np.ndarray, contexts: Dict[int, Tuple[Tuple[int,...], Optional[Counter]]]) -> np.ndarray:\n",
    "        # score = sum over contexts of presence * local prob\n",
    "        # local prob from per-context normalized counts\n",
    "        scores = np.zeros(cands.size, dtype=np.float64)\n",
    "        idxmap = {int(t): i for i, t in enumerate(cands)}\n",
    "        for n, (_, ctr) in contexts.items():\n",
    "            probs = self._probs_from_counter(ctr)\n",
    "            if probs is None:\n",
    "                continue\n",
    "            for t, p in probs.items():\n",
    "                if t in idxmap:\n",
    "                    scores[idxmap[t]] += float(p)\n",
    "        # tiny floor to avoid zeros\n",
    "        scores = scores + (scores == 0) * self.eps\n",
    "        return scores\n",
    "\n",
    "    def build_Z_for_sequence(self, seq: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        seq: array of length L = block_size (+optional pad)\n",
    "        returns:\n",
    "          topk_idx: (L, K) int64\n",
    "          topk_p:   (L, K) float32  (row-normalized)\n",
    "        \"\"\"\n",
    "        L = len(seq)\n",
    "        topk_idx = np.zeros((L, self.K), dtype=np.int64)\n",
    "        topk_p   = np.zeros((L, self.K), dtype=np.float32)\n",
    "        for j in range(L):\n",
    "            back_tok = int(seq[j])\n",
    "            # collect contexts\n",
    "            contexts = {}\n",
    "            for n in ngram_orders:\n",
    "                if j - (n-1) < 0:\n",
    "                    continue\n",
    "                ctx = tuple(int(x) for x in seq[j-(n-1):j+1])\n",
    "                ctr = self.models[n].get(ctx, None)\n",
    "                contexts[n] = (ctx, ctr)\n",
    "\n",
    "            # disciplined candidate set\n",
    "            cands = self._btree_candidates(contexts, back_tok)\n",
    "\n",
    "            # cap K by candidate count\n",
    "            if cands.size >= self.K:\n",
    "                # score & take best K\n",
    "                scores = self._score_candidates(cands, contexts)\n",
    "                order = np.argsort(-scores)[:self.K]\n",
    "                idx = cands[order]\n",
    "                sc  = scores[order]\n",
    "            else:\n",
    "                # we must fill with globally-most-common tokens (no randoms), excluding existing\n",
    "                scores = self._score_candidates(cands, contexts) if cands.size > 0 else np.array([], dtype=np.float64)\n",
    "                missing = self.K - cands.size\n",
    "                mask = np.ones(vocab_size, dtype=bool)\n",
    "                mask[cands] = False\n",
    "                fill = []\n",
    "                for t in self.global_order:\n",
    "                    if mask[t]:\n",
    "                        fill.append(int(t))\n",
    "                        if len(fill) == missing:\n",
    "                            break\n",
    "                if cands.size == 0:\n",
    "                    idx = np.array(fill, dtype=np.int64)\n",
    "                    sc  = np.full(len(fill), self.eps, dtype=np.float64)\n",
    "                else:\n",
    "                    idx = np.concatenate([cands, np.array(fill, dtype=np.int64)])\n",
    "                    sc  = np.concatenate([scores, np.full(missing, self.eps, dtype=np.float64)])\n",
    "\n",
    "            # normalize to prob\n",
    "            p = sc.astype(np.float64)\n",
    "            p = p / p.sum() if p.sum() > 0 else np.full_like(p, 1.0/len(p))\n",
    "            topk_idx[j, :] = idx\n",
    "            topk_p[j, :]   = p.astype(np.float32)\n",
    "        return topk_idx, topk_p\n",
    "\n",
    "discZ = DisciplinedZ(markov_models, bigram_db, p_global, vocab_size, top_k=32, epsilon=1e-6)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class GPUDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size: int, batch_size: int, builder: DisciplinedZ, pad_len:int=0, jitter:int=63, p_aligned:float=0.5, seed:int=1337):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = int(block_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.pad_len    = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = max(1, self.total // self.sample_len)\n",
    "        self.jitter = int(jitter)\n",
    "        self.p_aligned = float(p_aligned)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.builder = builder\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def _sample_block(self):\n",
    "        base_block = self.rng.integers(0, self.n_blocks)\n",
    "        start = base_block * self.sample_len\n",
    "        if self.rng.random() > self.p_aligned:\n",
    "            j = self.rng.integers(0, self.jitter + 1)\n",
    "            start = min(start + j, self.total)\n",
    "        return start\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        B, T = self.batch_size, self.block_size\n",
    "        X = np.empty((B, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((B, T), dtype=np.int64)\n",
    "        Z_idx = np.empty((B, T, self.builder.K), dtype=np.int64)\n",
    "        Z_p   = np.empty((B, T, self.builder.K), dtype=np.float32)\n",
    "        for i in range(B):\n",
    "            start = self._sample_block()\n",
    "            xseq = self.data[start : start + self.sample_len].astype(np.int64)\n",
    "            yseq = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + T].astype(np.int64)\n",
    "            X[i] = xseq\n",
    "            Y[i] = yseq\n",
    "            idxs, probs = self.builder.build_Z_for_sequence(xseq[:T])\n",
    "            Z_idx[i] = idxs\n",
    "            Z_p[i]   = probs\n",
    "        # torch tensors\n",
    "        X = torch.from_numpy(X[:, :T]).to(device)\n",
    "        Y = torch.from_numpy(Y).to(device)\n",
    "        Z_idx = torch.from_numpy(Z_idx).to(device)\n",
    "        Z_p   = torch.from_numpy(Z_p).to(device)\n",
    "        return X, Y, (Z_idx, Z_p)\n",
    "\n",
    "def collate_identity(batch):\n",
    "    Xs, Ys, Zs = zip(*batch)\n",
    "    X = torch.cat(Xs, dim=0)\n",
    "    Y = torch.cat(Ys, dim=0)\n",
    "    Zi = torch.cat([z[0] for z in Zs], dim=0)\n",
    "    Zp = torch.cat([z[1] for z in Zs], dim=0)\n",
    "    return X, Y, (Zi, Zp)\n",
    "\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "GPU_DATASET = GPUDataset(\n",
    "    np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r'),\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    builder=discZ,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    GPU_DATASET,\n",
    "    batch_size=1,            # keep outer loader at 1; inner dataset batches on GPU\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_identity\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efacb560-8e61-4e4e-9399-3c1611b476ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1.67M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "config =  GPTConfig(\n",
    "    block_size,\n",
    "    vocab_size,\n",
    "    n_layer=8,      \n",
    "    n_head = 8,\n",
    "    n_embd =128)\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, yb, zb in train_loader:\n",
    "        # xb: (B, T), yb: (B, T), zb: (Z_idx, Z_p) with shapes (B,T,K)\n",
    "        logits, loss = model(xb, None, zb, aux_scale=1.0)  # tweak aux_scale as desired\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total += loss.item()\n",
    "        print(loss.item())\n",
    "        losses.append(loss.item())\n",
    "    return total / len(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c43d3e5c-ba7e-4a1e-9b59-1b593316e2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1799936\n"
     ]
    }
   ],
   "source": [
    "print(sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ae6451f-1367-40a2-9fb9-85a5085ef5bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.65013599395752\n",
      "12.176746368408203\n",
      "12.367919921875\n",
      "11.763010025024414\n",
      "11.893775939941406\n",
      "12.134939193725586\n",
      "12.21752643585205\n",
      "11.834877967834473\n",
      "11.724566459655762\n",
      "11.790558815002441\n",
      "11.804945945739746\n",
      "11.331621170043945\n",
      "12.233490943908691\n",
      "12.015226364135742\n",
      "11.570435523986816\n",
      "11.75013542175293\n",
      "11.632621765136719\n",
      "12.035614967346191\n",
      "12.792713165283203\n",
      "12.174797058105469\n",
      "11.654851913452148\n",
      "11.91908073425293\n",
      "12.080265045166016\n",
      "11.939265251159668\n",
      "11.8851318359375\n",
      "11.771059036254883\n",
      "11.993128776550293\n",
      "11.767162322998047\n",
      "11.994684219360352\n",
      "12.469624519348145\n",
      "11.34873104095459\n",
      "12.72722339630127\n",
      "12.149290084838867\n",
      "11.922815322875977\n",
      "11.966263771057129\n",
      "12.623263359069824\n",
      "11.872671127319336\n",
      "12.3560791015625\n",
      "12.013833045959473\n",
      "11.993875503540039\n",
      "12.048503875732422\n",
      "11.892399787902832\n",
      "12.561891555786133\n",
      "11.384661674499512\n",
      "12.219773292541504\n",
      "11.618828773498535\n",
      "11.852543830871582\n",
      "11.436677932739258\n",
      "11.816597938537598\n",
      "11.426911354064941\n",
      "12.41243839263916\n",
      "11.745960235595703\n",
      "11.7672119140625\n",
      "11.860519409179688\n",
      "12.248421669006348\n",
      "11.916812896728516\n",
      "12.624561309814453\n",
      "12.42310619354248\n",
      "12.0638427734375\n",
      "12.423076629638672\n",
      "11.83372688293457\n",
      "11.578659057617188\n",
      "11.738483428955078\n",
      "11.937301635742188\n",
      "11.735671043395996\n",
      "11.805646896362305\n",
      "12.254130363464355\n",
      "11.86366081237793\n",
      "12.534537315368652\n",
      "11.88818359375\n",
      "11.675519943237305\n",
      "12.439544677734375\n",
      "12.196422576904297\n",
      "11.67396068572998\n",
      "11.945942878723145\n",
      "12.215991973876953\n",
      "11.698416709899902\n",
      "11.783453941345215\n",
      "12.192117691040039\n",
      "12.067956924438477\n",
      "11.718581199645996\n",
      "12.031042098999023\n",
      "11.672802925109863\n",
      "12.171646118164062\n",
      "12.343902587890625\n",
      "12.192755699157715\n",
      "11.921016693115234\n",
      "12.237465858459473\n",
      "11.851390838623047\n",
      "12.001554489135742\n",
      "12.288473129272461\n",
      "11.513739585876465\n",
      "11.457408905029297\n",
      "11.826972007751465\n",
      "12.258798599243164\n",
      "11.900764465332031\n",
      "11.835925102233887\n",
      "12.324193000793457\n",
      "11.611340522766113\n",
      "11.654996871948242\n",
      "11.555222511291504\n",
      "11.928112983703613\n",
      "12.185140609741211\n",
      "11.964204788208008\n",
      "11.889870643615723\n",
      "11.532302856445312\n",
      "12.216346740722656\n",
      "12.304802894592285\n",
      "12.072237968444824\n",
      "11.720148086547852\n",
      "11.8074951171875\n",
      "11.963554382324219\n",
      "11.719188690185547\n",
      "12.22806167602539\n",
      "11.90219783782959\n",
      "11.924175262451172\n",
      "11.88329029083252\n",
      "12.025193214416504\n",
      "12.25564956665039\n",
      "11.787705421447754\n",
      "11.731088638305664\n",
      "11.663126945495605\n",
      "12.0857515335083\n",
      "12.18949031829834\n",
      "11.762568473815918\n",
      "11.747797012329102\n",
      "12.068860054016113\n",
      "11.978458404541016\n",
      "11.955842018127441\n",
      "12.042223930358887\n",
      "11.673913955688477\n",
      "11.72221851348877\n",
      "11.769258499145508\n",
      "11.642995834350586\n",
      "12.010491371154785\n",
      "11.701549530029297\n",
      "11.903742790222168\n",
      "12.162325859069824\n",
      "12.112715721130371\n",
      "11.903934478759766\n",
      "11.879161834716797\n",
      "12.102062225341797\n",
      "11.804536819458008\n",
      "11.429586410522461\n",
      "11.548261642456055\n",
      "12.024773597717285\n",
      "11.869584083557129\n",
      "12.006918907165527\n",
      "12.018653869628906\n",
      "11.839035034179688\n",
      "11.733126640319824\n",
      "11.824631690979004\n",
      "11.793689727783203\n",
      "11.904916763305664\n",
      "11.293691635131836\n",
      "12.264334678649902\n",
      "11.64312744140625\n",
      "11.806065559387207\n",
      "12.074687957763672\n",
      "11.875529289245605\n",
      "11.786165237426758\n",
      "11.480644226074219\n",
      "11.811022758483887\n",
      "11.506104469299316\n",
      "12.02861213684082\n",
      "11.544821739196777\n",
      "11.978713989257812\n",
      "11.925127983093262\n",
      "12.14362907409668\n",
      "12.441213607788086\n",
      "11.69724178314209\n",
      "12.148456573486328\n",
      "12.09890365600586\n",
      "12.24202823638916\n",
      "12.58225154876709\n",
      "12.2294282913208\n",
      "12.279966354370117\n",
      "11.852267265319824\n",
      "11.71518611907959\n",
      "11.600272178649902\n",
      "11.750974655151367\n",
      "11.714853286743164\n",
      "11.379375457763672\n",
      "12.077827453613281\n",
      "11.74060344696045\n",
      "12.330409049987793\n",
      "11.753532409667969\n",
      "12.295748710632324\n",
      "11.785913467407227\n",
      "12.019220352172852\n",
      "12.291468620300293\n",
      "11.929283142089844\n",
      "11.358494758605957\n",
      "11.961694717407227\n",
      "11.881372451782227\n",
      "11.925397872924805\n",
      "11.897730827331543\n",
      "12.045997619628906\n",
      "11.81457233428955\n",
      "11.775935173034668\n",
      "12.074517250061035\n",
      "12.182342529296875\n",
      "11.737936973571777\n",
      "11.802165985107422\n",
      "11.79359245300293\n",
      "12.112456321716309\n",
      "11.979676246643066\n",
      "11.961240768432617\n",
      "11.794879913330078\n",
      "12.17377758026123\n",
      "11.939562797546387\n",
      "11.901142120361328\n",
      "12.10058879852295\n",
      "11.753595352172852\n",
      "11.720718383789062\n",
      "11.668575286865234\n",
      "12.703471183776855\n",
      "12.08909797668457\n",
      "11.51274585723877\n",
      "12.13516902923584\n",
      "11.69843864440918\n",
      "11.497892379760742\n",
      "12.292431831359863\n",
      "12.328782081604004\n",
      "11.941583633422852\n",
      "11.860188484191895\n",
      "12.205939292907715\n",
      "12.154783248901367\n",
      "11.785682678222656\n",
      "12.313822746276855\n",
      "12.137914657592773\n",
      "11.667323112487793\n",
      "12.098389625549316\n",
      "11.945674896240234\n",
      "12.244376182556152\n",
      "11.929683685302734\n",
      "11.792484283447266\n",
      "11.610123634338379\n",
      "11.944639205932617\n",
      "11.588079452514648\n",
      "12.107575416564941\n",
      "11.78166675567627\n",
      "11.649981498718262\n",
      "11.736045837402344\n",
      "11.743814468383789\n",
      "12.107547760009766\n",
      "12.107305526733398\n",
      "12.430593490600586\n",
      "11.888602256774902\n",
      "11.896097183227539\n",
      "11.941387176513672\n",
      "12.095280647277832\n",
      "11.816527366638184\n",
      "12.173299789428711\n",
      "12.172355651855469\n",
      "11.875621795654297\n",
      "12.14951229095459\n",
      "11.701444625854492\n",
      "12.311497688293457\n",
      "11.690539360046387\n",
      "12.022031784057617\n",
      "11.818881034851074\n",
      "11.783447265625\n",
      "11.89624309539795\n",
      "11.482178688049316\n",
      "11.602251052856445\n",
      "11.491337776184082\n",
      "11.936347007751465\n",
      "11.514426231384277\n",
      "11.642807960510254\n",
      "12.279314041137695\n",
      "11.135551452636719\n",
      "12.19326400756836\n",
      "12.227239608764648\n",
      "11.620996475219727\n",
      "11.722432136535645\n",
      "11.877530097961426\n",
      "11.930591583251953\n",
      "12.3685302734375\n",
      "11.900384902954102\n",
      "12.11422348022461\n",
      "11.8772611618042\n",
      "11.581497192382812\n",
      "11.73491096496582\n",
      "11.769804954528809\n",
      "11.462964057922363\n",
      "11.733683586120605\n",
      "11.833789825439453\n",
      "11.538010597229004\n",
      "12.440393447875977\n",
      "11.723669052124023\n",
      "11.70507526397705\n",
      "11.159250259399414\n",
      "11.670164108276367\n",
      "11.743474006652832\n",
      "12.257625579833984\n",
      "11.403989791870117\n",
      "11.755890846252441\n",
      "11.80990219116211\n",
      "11.404329299926758\n",
      "12.211613655090332\n",
      "12.142986297607422\n",
      "11.819489479064941\n",
      "11.981056213378906\n",
      "11.654047012329102\n",
      "12.097935676574707\n",
      "12.203189849853516\n",
      "11.518898963928223\n",
      "11.802000045776367\n",
      "11.685554504394531\n",
      "11.95714282989502\n",
      "11.833924293518066\n",
      "12.270670890808105\n",
      "11.823514938354492\n",
      "11.805391311645508\n",
      "12.031837463378906\n",
      "11.69182014465332\n",
      "12.544463157653809\n",
      "11.98937702178955\n",
      "11.623946189880371\n",
      "11.85017204284668\n",
      "11.929963111877441\n",
      "11.887764930725098\n",
      "11.844236373901367\n",
      "11.87752914428711\n",
      "12.00166130065918\n",
      "12.116976737976074\n",
      "12.127408981323242\n",
      "11.690192222595215\n",
      "12.125626564025879\n",
      "11.552401542663574\n",
      "12.146832466125488\n",
      "11.85772705078125\n",
      "11.965469360351562\n",
      "11.677288055419922\n",
      "11.975878715515137\n",
      "11.943692207336426\n",
      "11.801874160766602\n",
      "11.961189270019531\n",
      "12.150431632995605\n",
      "12.019753456115723\n",
      "11.660772323608398\n",
      "11.983865737915039\n",
      "11.597740173339844\n",
      "11.696311950683594\n",
      "11.696643829345703\n",
      "12.114087104797363\n",
      "12.152667045593262\n",
      "12.294322967529297\n",
      "11.636869430541992\n",
      "12.085079193115234\n",
      "11.597148895263672\n",
      "12.385048866271973\n",
      "11.675545692443848\n",
      "12.131892204284668\n",
      "11.641407012939453\n",
      "11.34789752960205\n",
      "11.653961181640625\n",
      "11.389837265014648\n",
      "11.97311019897461\n",
      "12.013148307800293\n",
      "12.162469863891602\n",
      "12.221924781799316\n",
      "12.249849319458008\n",
      "11.837973594665527\n",
      "12.350130081176758\n",
      "11.705558776855469\n",
      "12.271783828735352\n",
      "11.901250839233398\n",
      "11.780739784240723\n",
      "11.993369102478027\n",
      "12.336623191833496\n",
      "11.91211223602295\n",
      "11.675848007202148\n",
      "12.073248863220215\n",
      "11.539022445678711\n",
      "11.871767044067383\n",
      "11.567360877990723\n",
      "11.997455596923828\n",
      "11.461400985717773\n",
      "11.946117401123047\n",
      "11.909361839294434\n",
      "12.030250549316406\n",
      "11.575623512268066\n",
      "11.869759559631348\n",
      "11.718008995056152\n",
      "11.784337997436523\n",
      "11.306021690368652\n",
      "11.929698944091797\n",
      "11.594376564025879\n",
      "11.87370491027832\n",
      "11.147869110107422\n",
      "11.668954849243164\n",
      "11.923629760742188\n",
      "12.04918384552002\n",
      "12.024466514587402\n",
      "12.370019912719727\n",
      "11.880050659179688\n",
      "11.820109367370605\n",
      "11.509708404541016\n",
      "11.994251251220703\n",
      "11.608515739440918\n",
      "11.286760330200195\n",
      "11.802973747253418\n",
      "11.430990219116211\n",
      "11.991610527038574\n",
      "11.563615798950195\n",
      "11.652627944946289\n",
      "11.28142261505127\n",
      "11.78207015991211\n",
      "11.831457138061523\n",
      "12.249048233032227\n",
      "11.38978385925293\n",
      "11.556851387023926\n",
      "11.540704727172852\n",
      "11.483909606933594\n",
      "11.80252456665039\n",
      "11.326472282409668\n",
      "12.516361236572266\n",
      "11.994847297668457\n",
      "11.777547836303711\n",
      "12.181885719299316\n",
      "12.003292083740234\n",
      "12.11548900604248\n",
      "11.820208549499512\n",
      "11.736950874328613\n",
      "11.906241416931152\n",
      "11.684295654296875\n",
      "11.979612350463867\n",
      "12.184198379516602\n",
      "12.377412796020508\n",
      "11.90817642211914\n",
      "11.9195556640625\n",
      "12.00223445892334\n",
      "11.808752059936523\n",
      "11.772603988647461\n",
      "12.062604904174805\n",
      "11.434844970703125\n",
      "12.114920616149902\n",
      "11.778564453125\n",
      "11.909062385559082\n",
      "12.064445495605469\n",
      "11.939229965209961\n",
      "12.16372299194336\n",
      "11.363256454467773\n",
      "11.809651374816895\n",
      "12.023896217346191\n",
      "11.750969886779785\n",
      "11.700645446777344\n",
      "11.301883697509766\n",
      "12.0236234664917\n",
      "11.87179946899414\n",
      "11.857583045959473\n",
      "12.018442153930664\n",
      "11.932226181030273\n",
      "11.756025314331055\n",
      "12.000968933105469\n",
      "11.697806358337402\n",
      "12.075118064880371\n",
      "11.84037971496582\n",
      "12.252073287963867\n",
      "11.362013816833496\n",
      "11.828099250793457\n",
      "12.05321979522705\n",
      "11.068681716918945\n",
      "11.50987434387207\n",
      "11.857295989990234\n",
      "11.892770767211914\n",
      "11.761434555053711\n",
      "12.063578605651855\n",
      "11.590496063232422\n",
      "11.483956336975098\n",
      "11.813846588134766\n",
      "12.060182571411133\n",
      "11.85794734954834\n",
      "11.677934646606445\n",
      "11.783529281616211\n",
      "11.969876289367676\n",
      "11.710516929626465\n",
      "11.13483715057373\n",
      "11.407785415649414\n",
      "11.923945426940918\n",
      "12.11577033996582\n",
      "11.94198989868164\n",
      "11.6168794631958\n",
      "11.710000991821289\n",
      "11.236618995666504\n",
      "11.588967323303223\n",
      "12.105398178100586\n",
      "11.692956924438477\n",
      "12.545875549316406\n",
      "11.782876014709473\n",
      "11.792724609375\n",
      "12.486568450927734\n",
      "11.902558326721191\n",
      "11.932072639465332\n",
      "11.58216381072998\n",
      "12.181166648864746\n",
      "11.7992525100708\n",
      "11.910327911376953\n",
      "11.901936531066895\n",
      "11.726479530334473\n",
      "11.722789764404297\n",
      "11.915531158447266\n",
      "11.83600902557373\n",
      "12.2599458694458\n",
      "11.504841804504395\n",
      "11.942827224731445\n",
      "11.5933198928833\n",
      "11.61535930633545\n",
      "11.95746898651123\n",
      "12.047440528869629\n",
      "11.867177963256836\n",
      "12.010359764099121\n",
      "11.38110637664795\n",
      "11.400864601135254\n",
      "11.9754638671875\n",
      "11.94330883026123\n",
      "11.552017211914062\n",
      "11.669197082519531\n",
      "12.083258628845215\n",
      "11.579764366149902\n",
      "11.807890892028809\n",
      "12.45445442199707\n",
      "11.610347747802734\n",
      "11.613037109375\n",
      "11.42067813873291\n",
      "11.963269233703613\n",
      "11.923746109008789\n",
      "12.066720008850098\n",
      "12.1743803024292\n",
      "12.225274085998535\n",
      "11.844118118286133\n",
      "12.12059497833252\n",
      "11.827028274536133\n",
      "12.126945495605469\n",
      "11.61686897277832\n",
      "11.050236701965332\n",
      "11.331932067871094\n",
      "11.906881332397461\n",
      "11.640936851501465\n",
      "12.091455459594727\n",
      "11.659428596496582\n",
      "12.060306549072266\n",
      "11.19388484954834\n",
      "11.626928329467773\n",
      "12.097251892089844\n",
      "11.779086112976074\n",
      "11.694911003112793\n",
      "11.9916410446167\n",
      "11.831049919128418\n",
      "12.064910888671875\n",
      "12.410642623901367\n",
      "12.08465576171875\n",
      "12.262266159057617\n",
      "11.703629493713379\n",
      "11.99373722076416\n",
      "12.014639854431152\n",
      "11.962811470031738\n",
      "12.076032638549805\n",
      "11.600908279418945\n",
      "12.185827255249023\n",
      "11.880009651184082\n",
      "11.61331558227539\n",
      "11.94518756866455\n",
      "11.665017127990723\n",
      "11.413346290588379\n",
      "11.913537979125977\n",
      "11.757164001464844\n",
      "11.779681205749512\n",
      "11.962080001831055\n",
      "11.770598411560059\n",
      "11.585700988769531\n",
      "11.543317794799805\n",
      "11.223130226135254\n",
      "12.011092185974121\n",
      "11.870168685913086\n",
      "11.777227401733398\n",
      "11.762161254882812\n",
      "11.655006408691406\n",
      "11.589564323425293\n",
      "11.951669692993164\n",
      "11.615042686462402\n",
      "11.688759803771973\n",
      "11.417261123657227\n",
      "12.084310531616211\n",
      "11.751361846923828\n",
      "11.600200653076172\n",
      "11.965983390808105\n",
      "11.844585418701172\n",
      "11.238340377807617\n",
      "11.875736236572266\n",
      "11.702583312988281\n",
      "11.992574691772461\n",
      "11.798883438110352\n",
      "11.569361686706543\n",
      "11.533534049987793\n",
      "11.513657569885254\n",
      "12.138945579528809\n",
      "11.511998176574707\n",
      "11.876520156860352\n",
      "11.759214401245117\n",
      "11.076615333557129\n",
      "11.449028015136719\n",
      "11.492637634277344\n",
      "11.390096664428711\n",
      "11.563358306884766\n",
      "12.340222358703613\n",
      "11.825188636779785\n",
      "12.20156478881836\n",
      "11.996260643005371\n",
      "11.678364753723145\n",
      "11.592467308044434\n",
      "11.571873664855957\n",
      "11.759652137756348\n",
      "11.941520690917969\n",
      "12.351038932800293\n",
      "12.011543273925781\n",
      "11.83486270904541\n",
      "11.574400901794434\n",
      "11.889012336730957\n",
      "11.596900939941406\n",
      "11.903051376342773\n",
      "11.868430137634277\n",
      "11.78583812713623\n",
      "11.52235221862793\n",
      "11.853013038635254\n",
      "11.845298767089844\n",
      "12.096624374389648\n",
      "11.512328147888184\n",
      "12.077445983886719\n",
      "11.627684593200684\n",
      "11.55902099609375\n",
      "11.487700462341309\n",
      "11.679054260253906\n",
      "11.881025314331055\n",
      "11.840998649597168\n",
      "11.960914611816406\n",
      "11.42593765258789\n",
      "12.412471771240234\n",
      "11.71176528930664\n",
      "11.476960182189941\n",
      "11.809459686279297\n",
      "11.519983291625977\n",
      "11.78264045715332\n",
      "12.048850059509277\n",
      "11.160478591918945\n",
      "11.627164840698242\n",
      "11.626136779785156\n",
      "11.972219467163086\n",
      "11.820096969604492\n",
      "11.178729057312012\n",
      "11.9752779006958\n",
      "11.54237174987793\n",
      "11.339998245239258\n",
      "12.030385971069336\n",
      "11.193537712097168\n",
      "11.99478530883789\n",
      "11.901033401489258\n",
      "11.800050735473633\n",
      "12.221784591674805\n",
      "12.027478218078613\n",
      "11.698285102844238\n",
      "11.629127502441406\n",
      "11.701643943786621\n",
      "11.885705947875977\n",
      "11.546807289123535\n",
      "11.630718231201172\n",
      "11.86906623840332\n",
      "11.661075592041016\n",
      "11.577230453491211\n",
      "11.845499992370605\n",
      "11.589910507202148\n",
      "11.79018783569336\n",
      "11.565064430236816\n",
      "11.651973724365234\n",
      "11.840194702148438\n",
      "11.929810523986816\n",
      "11.83614730834961\n",
      "11.906617164611816\n",
      "11.507758140563965\n",
      "11.820335388183594\n",
      "11.359224319458008\n",
      "11.855674743652344\n",
      "11.538200378417969\n",
      "11.430346488952637\n",
      "11.979575157165527\n",
      "12.229930877685547\n",
      "11.491284370422363\n",
      "12.06722640991211\n",
      "11.803486824035645\n",
      "12.232409477233887\n",
      "11.4824800491333\n",
      "11.810308456420898\n",
      "11.541691780090332\n",
      "11.63433837890625\n",
      "11.494938850402832\n",
      "11.503863334655762\n",
      "11.590678215026855\n",
      "10.80322551727295\n",
      "11.51706600189209\n",
      "11.516706466674805\n",
      "11.674603462219238\n",
      "11.76385498046875\n",
      "12.440261840820312\n",
      "11.625936508178711\n",
      "11.814521789550781\n",
      "11.978126525878906\n",
      "11.709643363952637\n",
      "11.352278709411621\n",
      "11.918745040893555\n",
      "11.84321117401123\n",
      "11.792923927307129\n",
      "11.618559837341309\n",
      "11.423657417297363\n",
      "11.585558891296387\n",
      "11.494536399841309\n",
      "11.819380760192871\n",
      "12.321974754333496\n",
      "11.156435012817383\n",
      "11.702951431274414\n",
      "11.872477531433105\n",
      "11.982208251953125\n",
      "12.242827415466309\n",
      "12.070481300354004\n",
      "11.610945701599121\n",
      "11.430999755859375\n",
      "11.58149242401123\n",
      "11.88979434967041\n",
      "11.371874809265137\n",
      "11.657666206359863\n",
      "11.913347244262695\n",
      "11.64644718170166\n",
      "11.669304847717285\n",
      "11.721216201782227\n",
      "11.884542465209961\n",
      "11.27376651763916\n",
      "11.716135025024414\n",
      "11.774621963500977\n",
      "11.676939964294434\n",
      "12.118889808654785\n",
      "11.854765892028809\n",
      "11.19942569732666\n",
      "11.637473106384277\n",
      "11.67422866821289\n",
      "11.892996788024902\n",
      "12.08669376373291\n",
      "11.619824409484863\n",
      "11.901741027832031\n",
      "11.35249137878418\n",
      "11.616338729858398\n",
      "11.540038108825684\n",
      "11.736381530761719\n",
      "11.838058471679688\n",
      "11.843107223510742\n",
      "11.93059253692627\n",
      "11.844025611877441\n",
      "12.32262897491455\n",
      "11.996026992797852\n",
      "11.490141868591309\n",
      "11.842904090881348\n",
      "11.780543327331543\n",
      "11.797844886779785\n",
      "11.776859283447266\n",
      "12.020830154418945\n",
      "11.759381294250488\n",
      "11.896777153015137\n",
      "11.799825668334961\n",
      "11.600188255310059\n",
      "12.165142059326172\n",
      "11.685078620910645\n",
      "12.389314651489258\n",
      "11.207979202270508\n",
      "11.846415519714355\n",
      "11.956626892089844\n",
      "11.84597396850586\n",
      "11.767560958862305\n",
      "11.555840492248535\n",
      "11.715863227844238\n",
      "11.775731086730957\n",
      "11.59428596496582\n",
      "11.898826599121094\n",
      "11.597238540649414\n",
      "11.210999488830566\n",
      "12.00067138671875\n",
      "11.601346015930176\n",
      "11.922980308532715\n",
      "11.600056648254395\n",
      "11.924346923828125\n",
      "11.646822929382324\n",
      "11.600736618041992\n",
      "11.814743041992188\n",
      "11.789084434509277\n",
      "11.24109935760498\n",
      "11.756799697875977\n",
      "11.69241714477539\n",
      "11.736666679382324\n",
      "11.551201820373535\n",
      "11.955009460449219\n",
      "11.673046112060547\n",
      "11.755118370056152\n",
      "11.776904106140137\n",
      "12.040592193603516\n",
      "11.97529125213623\n",
      "11.724963188171387\n",
      "11.583233833312988\n",
      "12.145987510681152\n",
      "11.9861478805542\n",
      "11.830866813659668\n",
      "12.01707649230957\n",
      "11.603612899780273\n",
      "11.624147415161133\n",
      "11.8588228225708\n",
      "11.76579475402832\n",
      "11.04238224029541\n",
      "11.592535972595215\n",
      "11.194998741149902\n",
      "11.09469985961914\n",
      "11.608001708984375\n",
      "12.024025917053223\n",
      "11.408967971801758\n",
      "11.647045135498047\n",
      "11.527580261230469\n",
      "11.933723449707031\n",
      "11.874479293823242\n",
      "12.015593528747559\n",
      "11.7332181930542\n",
      "11.595406532287598\n",
      "12.16761589050293\n",
      "11.747628211975098\n",
      "11.375650405883789\n",
      "11.497844696044922\n",
      "11.863607406616211\n",
      "11.149188041687012\n",
      "11.883856773376465\n",
      "11.770438194274902\n",
      "11.997514724731445\n",
      "11.634736061096191\n",
      "11.601629257202148\n",
      "11.602015495300293\n",
      "11.820987701416016\n",
      "11.187626838684082\n",
      "11.903075218200684\n",
      "11.491090774536133\n",
      "12.061969757080078\n",
      "11.734902381896973\n",
      "11.721508026123047\n",
      "11.477082252502441\n",
      "11.617229461669922\n",
      "11.4196195602417\n",
      "11.907726287841797\n",
      "11.372390747070312\n",
      "11.613152503967285\n",
      "12.037211418151855\n",
      "11.410449981689453\n",
      "11.595813751220703\n",
      "12.164787292480469\n",
      "11.644964218139648\n",
      "12.171544075012207\n",
      "11.421004295349121\n",
      "11.749344825744629\n",
      "11.712896347045898\n",
      "11.963827133178711\n",
      "11.339226722717285\n",
      "11.615880966186523\n",
      "11.599180221557617\n",
      "11.893244743347168\n",
      "11.193092346191406\n",
      "11.608473777770996\n",
      "11.777579307556152\n",
      "11.213785171508789\n",
      "11.5535888671875\n",
      "11.838583946228027\n",
      "11.349668502807617\n",
      "11.084197998046875\n",
      "11.865647315979004\n",
      "11.900032043457031\n",
      "11.343609809875488\n",
      "11.576380729675293\n",
      "11.41135025024414\n",
      "11.672842025756836\n",
      "11.448057174682617\n",
      "11.654129981994629\n",
      "11.65333080291748\n",
      "11.863173484802246\n",
      "11.631612777709961\n",
      "11.700814247131348\n",
      "11.480203628540039\n",
      "12.125398635864258\n",
      "11.331808090209961\n",
      "11.288724899291992\n",
      "11.842562675476074\n",
      "11.611403465270996\n",
      "11.770153999328613\n",
      "12.332771301269531\n",
      "11.299375534057617\n",
      "11.762578964233398\n",
      "11.585305213928223\n",
      "11.367181777954102\n",
      "11.567229270935059\n",
      "11.32602310180664\n",
      "11.470821380615234\n",
      "11.611858367919922\n",
      "11.275262832641602\n",
      "11.703954696655273\n",
      "11.836224555969238\n",
      "11.469184875488281\n",
      "11.933440208435059\n",
      "11.67650032043457\n",
      "11.717564582824707\n",
      "11.474752426147461\n",
      "12.11041259765625\n",
      "11.752623558044434\n",
      "11.77883529663086\n",
      "11.992011070251465\n",
      "11.647869110107422\n",
      "11.884217262268066\n",
      "11.60261344909668\n",
      "11.02486801147461\n",
      "11.944849967956543\n",
      "11.459970474243164\n",
      "11.798062324523926\n",
      "11.506619453430176\n",
      "12.2088041305542\n",
      "12.050175666809082\n",
      "11.892284393310547\n",
      "11.596413612365723\n",
      "11.288614273071289\n",
      "11.38094711303711\n",
      "11.786354064941406\n",
      "11.893142700195312\n",
      "11.629913330078125\n",
      "11.572181701660156\n",
      "11.369159698486328\n",
      "11.807430267333984\n",
      "11.72480583190918\n",
      "11.37608528137207\n",
      "11.85604190826416\n",
      "11.79240894317627\n",
      "11.618586540222168\n",
      "11.584847450256348\n",
      "11.75868034362793\n",
      "11.610304832458496\n",
      "11.764695167541504\n",
      "11.948927879333496\n",
      "11.072649002075195\n",
      "12.023733139038086\n",
      "11.68087387084961\n",
      "11.311057090759277\n",
      "11.968526840209961\n",
      "12.053092002868652\n",
      "11.823366165161133\n",
      "11.682917594909668\n",
      "11.091466903686523\n",
      "11.45201301574707\n",
      "11.630990028381348\n",
      "11.686753273010254\n",
      "11.229146957397461\n",
      "11.039907455444336\n",
      "11.698793411254883\n",
      "12.279531478881836\n",
      "12.040911674499512\n",
      "11.6515474319458\n",
      "12.080881118774414\n",
      "12.163838386535645\n",
      "11.498981475830078\n",
      "11.789117813110352\n",
      "11.792282104492188\n",
      "11.698897361755371\n",
      "11.964322090148926\n",
      "11.364797592163086\n",
      "12.011954307556152\n",
      "11.78665542602539\n",
      "11.331304550170898\n",
      "11.292925834655762\n",
      "12.125659942626953\n",
      "11.332714080810547\n",
      "11.766637802124023\n",
      "11.820355415344238\n",
      "11.290511131286621\n",
      "11.794502258300781\n",
      "11.953250885009766\n",
      "11.831754684448242\n",
      "12.120467185974121\n",
      "11.507011413574219\n",
      "11.451111793518066\n",
      "11.701946258544922\n",
      "11.435519218444824\n",
      "11.272322654724121\n",
      "11.419538497924805\n",
      "12.010249137878418\n",
      "11.77289867401123\n",
      "11.785309791564941\n",
      "11.621451377868652\n",
      "11.983882904052734\n",
      "11.647897720336914\n",
      "11.917035102844238\n",
      "11.097658157348633\n",
      "11.415319442749023\n",
      "11.994147300720215\n",
      "11.641210556030273\n",
      "11.0789213180542\n",
      "11.595084190368652\n",
      "11.764510154724121\n",
      "11.593032836914062\n",
      "11.743077278137207\n",
      "11.867084503173828\n",
      "11.283761978149414\n",
      "11.349471092224121\n",
      "11.570513725280762\n",
      "11.488283157348633\n",
      "12.161760330200195\n",
      "11.6171875\n",
      "11.568092346191406\n",
      "12.127189636230469\n",
      "11.41625690460205\n",
      "11.358964920043945\n",
      "12.028449058532715\n",
      "11.420491218566895\n",
      "10.662698745727539\n",
      "11.907341003417969\n",
      "11.98047161102295\n",
      "12.032944679260254\n",
      "11.76026439666748\n",
      "11.667312622070312\n",
      "11.893621444702148\n",
      "11.63635540008545\n",
      "11.528900146484375\n",
      "11.536110877990723\n",
      "11.738262176513672\n",
      "11.672861099243164\n",
      "11.58273696899414\n",
      "11.575064659118652\n",
      "12.360063552856445\n",
      "11.48791790008545\n",
      "12.039030075073242\n",
      "11.487079620361328\n",
      "11.50073528289795\n",
      "11.743512153625488\n",
      "11.286114692687988\n",
      "11.753045082092285\n",
      "11.578486442565918\n",
      "11.443851470947266\n",
      "11.266852378845215\n",
      "11.829093933105469\n",
      "11.490605354309082\n",
      "10.994413375854492\n",
      "11.850879669189453\n",
      "11.83713150024414\n",
      "11.581023216247559\n",
      "11.43156623840332\n",
      "10.910707473754883\n",
      "11.523506164550781\n",
      "11.429628372192383\n",
      "11.898907661437988\n",
      "11.824254035949707\n",
      "11.643139839172363\n",
      "11.126219749450684\n",
      "11.710814476013184\n",
      "11.57319164276123\n",
      "10.972567558288574\n",
      "11.619282722473145\n",
      "11.865345001220703\n",
      "11.762933731079102\n",
      "11.74032974243164\n",
      "11.226009368896484\n",
      "11.584046363830566\n",
      "11.619091033935547\n",
      "11.493114471435547\n",
      "11.614152908325195\n",
      "11.143057823181152\n",
      "12.037117958068848\n",
      "12.06037425994873\n",
      "11.71807861328125\n",
      "12.369528770446777\n",
      "11.612360000610352\n",
      "11.587397575378418\n",
      "11.473697662353516\n",
      "11.99561595916748\n",
      "11.061355590820312\n",
      "11.154027938842773\n",
      "11.488653182983398\n",
      "12.03375244140625\n",
      "11.561261177062988\n",
      "10.97261905670166\n",
      "11.724658966064453\n",
      "11.73915958404541\n",
      "11.816300392150879\n",
      "11.654306411743164\n",
      "11.8814115524292\n",
      "11.307158470153809\n",
      "11.335515975952148\n",
      "11.387475967407227\n",
      "11.35840129852295\n",
      "11.583763122558594\n",
      "11.687154769897461\n",
      "11.635869026184082\n",
      "11.366246223449707\n",
      "11.744497299194336\n",
      "11.618595123291016\n",
      "11.570359230041504\n",
      "11.579973220825195\n",
      "11.516729354858398\n",
      "11.778618812561035\n",
      "11.720175743103027\n",
      "11.758960723876953\n",
      "11.635052680969238\n",
      "11.385684967041016\n",
      "11.752961158752441\n",
      "11.803598403930664\n",
      "11.727555274963379\n",
      "11.279461860656738\n",
      "11.390264511108398\n",
      "11.597704887390137\n",
      "11.750722885131836\n",
      "11.655757904052734\n",
      "11.310925483703613\n",
      "11.785593032836914\n",
      "11.20138168334961\n",
      "11.633612632751465\n",
      "11.329561233520508\n",
      "11.659764289855957\n",
      "11.515560150146484\n",
      "11.428257942199707\n",
      "11.366188049316406\n",
      "11.748653411865234\n",
      "11.523701667785645\n",
      "11.515466690063477\n",
      "11.450876235961914\n",
      "11.034744262695312\n",
      "11.690742492675781\n",
      "11.10911750793457\n",
      "11.414379119873047\n",
      "11.416877746582031\n",
      "11.316996574401855\n",
      "11.99985408782959\n",
      "11.540836334228516\n",
      "11.406795501708984\n",
      "11.519759178161621\n",
      "10.952223777770996\n",
      "11.539718627929688\n",
      "11.798827171325684\n",
      "11.782419204711914\n",
      "11.40481185913086\n",
      "11.522972106933594\n",
      "11.594396591186523\n",
      "11.458569526672363\n",
      "11.602225303649902\n",
      "11.718414306640625\n",
      "12.107725143432617\n",
      "11.641393661499023\n",
      "11.80726146697998\n",
      "11.349032402038574\n",
      "11.303027153015137\n",
      "11.862728118896484\n",
      "11.43246078491211\n",
      "11.566234588623047\n",
      "11.397239685058594\n",
      "11.635885238647461\n",
      "11.806069374084473\n",
      "11.428686141967773\n",
      "10.856396675109863\n",
      "11.88729476928711\n",
      "11.216262817382812\n",
      "11.597237586975098\n",
      "11.540102005004883\n",
      "11.89722728729248\n",
      "11.544713020324707\n",
      "11.530240058898926\n",
      "11.692440032958984\n",
      "11.53349494934082\n",
      "11.761749267578125\n",
      "11.304366111755371\n",
      "11.655693054199219\n",
      "11.692778587341309\n",
      "11.215690612792969\n",
      "11.697866439819336\n",
      "11.488751411437988\n",
      "11.078483581542969\n",
      "11.396571159362793\n",
      "11.917675971984863\n",
      "11.733756065368652\n",
      "11.276104927062988\n",
      "11.980106353759766\n",
      "11.536901473999023\n",
      "11.18552017211914\n",
      "11.514753341674805\n",
      "11.5558443069458\n",
      "11.688756942749023\n",
      "11.761125564575195\n",
      "11.836740493774414\n",
      "11.697135925292969\n",
      "11.701105117797852\n",
      "11.352042198181152\n",
      "11.169425964355469\n",
      "11.674134254455566\n",
      "11.447019577026367\n",
      "11.663972854614258\n",
      "11.784156799316406\n",
      "11.424216270446777\n",
      "11.65988540649414\n",
      "11.817178726196289\n",
      "11.792387008666992\n",
      "11.294879913330078\n",
      "11.742358207702637\n",
      "11.460874557495117\n",
      "11.459918022155762\n",
      "11.669866561889648\n",
      "11.722660064697266\n",
      "11.597853660583496\n",
      "11.874024391174316\n",
      "11.2828950881958\n",
      "11.907625198364258\n",
      "11.828253746032715\n",
      "11.686598777770996\n",
      "11.2673921585083\n",
      "11.825628280639648\n",
      "11.439584732055664\n",
      "11.264313697814941\n",
      "11.165369987487793\n",
      "11.48917007446289\n",
      "10.997121810913086\n",
      "11.41695499420166\n",
      "11.563334465026855\n",
      "11.537385940551758\n",
      "11.631048202514648\n",
      "11.288021087646484\n",
      "11.931851387023926\n",
      "11.367242813110352\n",
      "11.794293403625488\n",
      "11.30903148651123\n",
      "11.279498100280762\n",
      "11.710885047912598\n",
      "11.527830123901367\n",
      "11.613934516906738\n",
      "11.38924789428711\n",
      "11.662559509277344\n",
      "11.488530158996582\n",
      "11.924412727355957\n",
      "11.376787185668945\n",
      "11.29343318939209\n",
      "11.717153549194336\n",
      "11.253741264343262\n",
      "11.670052528381348\n",
      "11.367411613464355\n",
      "10.654464721679688\n",
      "11.647555351257324\n",
      "11.58178424835205\n",
      "11.509510040283203\n",
      "11.029727935791016\n",
      "11.433732986450195\n",
      "11.098430633544922\n",
      "11.985023498535156\n",
      "11.337532043457031\n",
      "11.51024055480957\n",
      "11.765031814575195\n",
      "11.524262428283691\n",
      "11.115732192993164\n",
      "11.105074882507324\n",
      "11.139638900756836\n",
      "11.694586753845215\n",
      "11.61671257019043\n",
      "11.50682258605957\n",
      "11.339189529418945\n",
      "11.178731918334961\n",
      "11.494616508483887\n",
      "11.63619327545166\n",
      "11.637231826782227\n",
      "11.324197769165039\n",
      "11.294205665588379\n",
      "11.598942756652832\n",
      "11.12480640411377\n",
      "11.874680519104004\n",
      "11.67586898803711\n",
      "11.499195098876953\n",
      "12.028693199157715\n",
      "11.807046890258789\n",
      "11.68596363067627\n",
      "11.245980262756348\n",
      "11.651124000549316\n",
      "11.302637100219727\n",
      "11.372819900512695\n",
      "11.374658584594727\n",
      "11.647446632385254\n",
      "11.588762283325195\n",
      "11.652679443359375\n",
      "11.590813636779785\n",
      "11.574967384338379\n",
      "11.918196678161621\n",
      "11.273710250854492\n",
      "11.615310668945312\n",
      "11.413514137268066\n",
      "11.848271369934082\n",
      "11.752228736877441\n",
      "11.870340347290039\n",
      "11.382826805114746\n",
      "11.574196815490723\n",
      "11.114473342895508\n",
      "11.877952575683594\n",
      "11.633932113647461\n",
      "11.951837539672852\n",
      "11.270186424255371\n",
      "11.710286140441895\n",
      "11.616235733032227\n",
      "12.008392333984375\n",
      "11.306434631347656\n",
      "11.117219924926758\n",
      "11.459181785583496\n",
      "11.914464950561523\n",
      "11.757841110229492\n",
      "11.1273775100708\n",
      "11.155189514160156\n",
      "11.150802612304688\n",
      "11.164920806884766\n",
      "11.309815406799316\n",
      "11.530258178710938\n",
      "11.43808650970459\n",
      "11.93082332611084\n",
      "11.450213432312012\n",
      "11.332379341125488\n",
      "11.792163848876953\n",
      "10.81412124633789\n",
      "11.393348693847656\n",
      "11.012815475463867\n",
      "11.682065963745117\n",
      "11.811676025390625\n",
      "11.436623573303223\n",
      "12.18010139465332\n",
      "11.551431655883789\n",
      "11.05573844909668\n",
      "11.548882484436035\n",
      "11.335578918457031\n",
      "11.322957992553711\n",
      "11.487420082092285\n",
      "11.20246696472168\n",
      "11.136669158935547\n",
      "11.739843368530273\n",
      "11.792740821838379\n",
      "11.611494064331055\n",
      "11.261152267456055\n",
      "11.564374923706055\n",
      "11.240734100341797\n",
      "11.208118438720703\n",
      "11.285135269165039\n",
      "11.635897636413574\n",
      "11.738092422485352\n",
      "11.620500564575195\n",
      "11.59609317779541\n",
      "12.126060485839844\n",
      "11.094826698303223\n",
      "11.291315078735352\n",
      "11.688668251037598\n",
      "11.243854522705078\n",
      "11.368033409118652\n",
      "11.73410415649414\n",
      "11.480147361755371\n",
      "11.290406227111816\n",
      "11.578292846679688\n",
      "11.680572509765625\n",
      "11.4730806350708\n",
      "11.411412239074707\n",
      "11.815520286560059\n",
      "11.057798385620117\n",
      "11.53395938873291\n",
      "11.462516784667969\n",
      "11.2179594039917\n",
      "11.810880661010742\n",
      "11.485907554626465\n",
      "11.516426086425781\n",
      "11.986039161682129\n",
      "11.059200286865234\n",
      "11.99062442779541\n",
      "11.278276443481445\n",
      "11.532184600830078\n",
      "11.384488105773926\n",
      "11.04464054107666\n",
      "11.629644393920898\n",
      "11.303631782531738\n",
      "12.007614135742188\n",
      "11.839943885803223\n",
      "11.267876625061035\n",
      "11.312165260314941\n",
      "11.2740478515625\n",
      "12.033690452575684\n",
      "11.421802520751953\n",
      "11.305573463439941\n",
      "11.79419231414795\n",
      "11.672393798828125\n",
      "11.87917709350586\n",
      "11.652883529663086\n",
      "11.487286567687988\n",
      "11.532757759094238\n",
      "11.28032398223877\n",
      "11.578561782836914\n",
      "10.807452201843262\n",
      "11.952771186828613\n",
      "11.63257122039795\n",
      "11.408933639526367\n",
      "11.327753067016602\n",
      "11.64694595336914\n",
      "11.35226058959961\n",
      "11.24393081665039\n",
      "12.010602951049805\n",
      "11.38573169708252\n",
      "11.271601676940918\n",
      "11.345600128173828\n",
      "11.337579727172852\n",
      "11.672186851501465\n",
      "11.045580863952637\n",
      "11.187468528747559\n",
      "11.601178169250488\n",
      "11.615912437438965\n",
      "11.862857818603516\n",
      "11.398204803466797\n",
      "11.580330848693848\n",
      "11.49275016784668\n",
      "11.076595306396484\n",
      "11.722131729125977\n",
      "11.553386688232422\n",
      "11.668610572814941\n",
      "11.045744895935059\n",
      "11.568866729736328\n",
      "10.675562858581543\n",
      "10.897411346435547\n",
      "11.656242370605469\n",
      "11.351531982421875\n",
      "11.756528854370117\n",
      "11.235320091247559\n",
      "11.871427536010742\n",
      "12.036629676818848\n",
      "11.169984817504883\n",
      "11.076550483703613\n",
      "11.206917762756348\n",
      "11.468504905700684\n",
      "11.551241874694824\n",
      "11.426620483398438\n",
      "11.35712718963623\n",
      "11.430800437927246\n",
      "11.391493797302246\n",
      "11.081693649291992\n",
      "11.025938034057617\n",
      "11.21767807006836\n",
      "12.258634567260742\n",
      "11.30499267578125\n",
      "10.745935440063477\n",
      "11.427719116210938\n",
      "11.558035850524902\n",
      "11.197641372680664\n",
      "11.76733112335205\n",
      "11.186555862426758\n",
      "11.723225593566895\n",
      "10.9923677444458\n",
      "11.659608840942383\n",
      "11.302824974060059\n",
      "11.04094123840332\n",
      "11.632087707519531\n",
      "11.225431442260742\n",
      "11.469701766967773\n",
      "11.221732139587402\n",
      "11.719185829162598\n",
      "11.19562816619873\n",
      "10.996274948120117\n",
      "11.321431159973145\n",
      "11.54702091217041\n",
      "11.561629295349121\n",
      "11.777666091918945\n",
      "11.836845397949219\n",
      "11.038423538208008\n",
      "11.924670219421387\n",
      "11.55819034576416\n",
      "11.014259338378906\n",
      "11.189684867858887\n",
      "11.615363121032715\n",
      "10.967460632324219\n",
      "11.03490161895752\n",
      "11.297520637512207\n",
      "11.1272611618042\n",
      "11.229082107543945\n",
      "11.194673538208008\n",
      "11.36400032043457\n",
      "11.63674545288086\n",
      "11.440179824829102\n",
      "11.514213562011719\n",
      "11.504470825195312\n",
      "11.553481101989746\n",
      "11.568863868713379\n",
      "11.283944129943848\n",
      "11.023777961730957\n",
      "11.736285209655762\n",
      "11.816482543945312\n",
      "11.644194602966309\n",
      "11.469621658325195\n",
      "11.635773658752441\n",
      "11.790236473083496\n",
      "11.668609619140625\n",
      "11.284873008728027\n",
      "11.574666976928711\n",
      "11.087539672851562\n",
      "11.826053619384766\n",
      "11.407037734985352\n",
      "11.433822631835938\n",
      "11.316696166992188\n",
      "11.11944580078125\n",
      "11.757943153381348\n",
      "11.828869819641113\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb, zb \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# xb: (B, T), yb: (B, T), zb: (Z_idx, Z_p) with shapes (B,T,K)\u001b[39;00m\n\u001b[32m     34\u001b[39m     logits, loss = model(xb, \u001b[38;5;28;01mNone\u001b[39;00m, zb, aux_scale=\u001b[32m1.0\u001b[39m)  \u001b[38;5;66;03m# tweak aux_scale as desired\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     37\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "36c6dd9b-dafc-4c7e-94e9-fde8dd10f186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x354174f50>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe76faa0-3933-416d-b923-993c67cfcf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Relingninely reason alike ring.\n",
      "\n",
      "YORK:\n",
      "Well, let you bear your rogues in your enemies,\n",
      "And ere you shall find thy late head:\n",
      "We presently you are sortful for yourself.\n",
      "\n",
      "ROMEO:\n",
      "Lord Warwick, kill a suring sighs again,\n",
      "Unto my one hands a liars as thou and dooms:\n",
      "A good cause to love him for the death makes\n",
      "He will soon sufferance of all all,\n",
      "Laid his patriap, sadles out to the truth's writ:\n",
      "Together, you never bear any noble seal,\n",
      "And with a nobtutes more proneful lables:\n",
      "Not whom thou dost all resolve Duke of Gloucester's heat?\n",
      "\n",
      "ROMEO:\n",
      "Going give me water.\n",
      "\n",
      "Nurte:\n",
      "Not to Tyrel, end Perdita, heart thy looks;\n",
      "Thy greeting preserve sound against all winess it\n",
      "Thy cloushes, thy joine Richard's way,\n",
      "For here vain, yet not give us all meeting,\n",
      "And tapster's royallure, doth say to thy son:\n",
      "I help she hath patnifice Menry, and for titles,\n",
      "As is the freeded in the beloved of thee,\n",
      "It shall, resigns she his from thy tall grathes:\n",
      "Yet be mounted at Cuesby's note,\n",
      "But I hand not should shape stroke but life,\n",
      "Knom of Redeme envywards and Timel;\n",
      "Yield aleady shame to will be sorry in;\n",
      "For he is from suplented and thy crowned,\n",
      "Can make the damned ear troubte thee,\n",
      "My grey set and Dittalion,\n",
      "Thing Reward is their into eat grace\n",
      "The second destroy of jectly Lancaster;\n",
      "His very enviles spitition of Siquit\n",
      "His trumpetance off his true would woo fait;\n",
      "To toil Liancas to God Valian this eyes\n",
      "Like him and company his ad caste to death,\n",
      "And desperate and for but passion.\n",
      "By hot you, aboutdy, son Richard's brows!\n",
      "No, stown, or Richard, at oath for Richard;\n",
      "And that Clarence by Rusmo affence\n",
      "More tears, down God bless and thee,\n",
      "Tell thou trangeto Romeo, and thou lies,\n",
      "Tregrous art art crowned thy orb;\n",
      "When they shall live them all this friends,\n",
      "The tear that in passess and duke,\n",
      "And fear them for tine out of law:\n",
      "But thou shalt, to set twenty of death,\n",
      "To stab my Margager our deceit death;\n",
      "And with thyself and I know lay\n",
      "Thou seplings for Clarge and is death.\n",
      "Who is thy day, good hast to thou mught!\n",
      "And I betime \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "def decode_sequence_char(\n",
    "    model, stoi, itos, prompt, max_new_tokens=100, block_size=256,\n",
    "    use_fenchel=False, tau=1.0, fenchel_iters=3, temperature=1.0\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    max_ctx = int(block_size)\n",
    "\n",
    "    # ?1 assume space token exists; fallback to 0 if missing\n",
    "    space_id = stoi.get(' ', 0)\n",
    "\n",
    "    # encode prompt\n",
    "    start_ids = torch.tensor([encode_chars(prompt, stoi)], dtype=torch.long, device=device)\n",
    "\n",
    "    # prepend the pad once; from now on the window just slides\n",
    "\n",
    "    idx = start_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = idx[:, -max_ctx:]  # rotating buffer: last pad_len+block_size tokens\n",
    "        logits, _ = model(context, None)\n",
    "        last_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    # drop the initial pad when returning the string\n",
    "    out_tokens = idx[0].tolist()\n",
    "    return decode_chars(out_tokens, itos)\n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "\n",
    "prompt = \"ROMEO:\"\n",
    "generated = decode_sequence_char(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=2024,\n",
    "    block_size=1024,\n",
    "    use_fenchel=False,\n",
    "    tau=1.5,\n",
    "    fenchel_iters=2,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e045fae-f129-499c-a0f6-0376e15fef27",
   "metadata": {},
   "source": [
    "This is Not equivalent to learning a Markov model. the synthetic composition of the aux target breaks that equivalence on multiple fronts, so the transformer can‚Äîand evidently does‚Äîlearn supra-Markov structure even though Z itself is built from n-gram machinery.\n",
    "\n",
    "why it‚Äôs not Markov-equivalent:\n",
    "\n",
    "teacher ‚â† single-order chain. Z is a composite of multiple orders (2‚Äì64) with B-tree intersection/union, top-K capping, and Œµ fills from global frequency. there‚Äôs no single conditional table that reproduces those distributions; they‚Äôre a curated prior, not a proper n-gram MLE.\n",
    "\n",
    "objective ‚â† local count fitting. we optimize a target with a deep network. nothing forces it to compute conditionals the way an n-gram would. the network can use any cues in its receptive field (including >64 tokens) to match Z statistically across the corpus.\n",
    "\n",
    "multi-depth CE (same Z, different skew). supervising every block with depth-wise sharpening introduces constraints on internal representations (curriculum/distillation effect) that Markov models don‚Äôt have. it shapes a hierarchy, not just the final conditional.\n",
    "\n",
    "support prior, not ground truth. by constraining support to plausible continuations, Z acts like an energy manifold (valid vs. invalid regions). the model is trained to align with that manifold; it is not estimating n-gram probabilities per se.\n",
    "\n",
    "global coupling through parameter sharing. transformers share parameters across positions and contexts; learning to match Z in one regime generalizes to others in ways that exceed local-count estimators.\n",
    "\n",
    "observed behavior contradicts Markov collapse. the low-temperature generations degrade into structured attractors (speaker scaffolds, grammatical frames), not trivial loops; that‚Äôs a hallmark of a smooth, global field learned over sequences, not a brittle n-gram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89689e67-ff91-4fc0-a0e4-79e4330d4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"fuzzy.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72509ed-bff1-43e2-a08a-4aad880fb293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
