{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7741bdf-98ab-4400-a0d1-4b831244ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# ----------------------------\n",
    "# Layers\n",
    "# ----------------------------\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm with optional learnable negative bias via -softplus(bias) \"\"\"\n",
    "    def __init__(self, ndim: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.use_bias = bias\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(ndim))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b = -F.softplus(self.bias) if self.use_bias else None\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, b, 1e-5)\n",
    "\n",
    "class LinearNegativeBias(nn.Module):\n",
    "    \"\"\" Linear with non-positive bias via -softplus() \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        nn.init.kaiming_uniform_(self.weight, a=5 ** 0.5)\n",
    "        self._bias_raw = nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return F.linear(x, self.weight, -F.softplus(self._bias_raw))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = LinearNegativeBias(config.n_embd, 4 * config.n_embd)\n",
    "        self.scale = math.pi / math.sqrt(3.0)\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.c_fc(x)\n",
    "        x =   x * torch.sigmoid(self.scale * x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "def variance_scaled_softmax(scores, dim: int = -1, eps: float = 1e-6):\n",
    "    # scores may contain -inf from masking\n",
    "    finite = torch.isfinite(scores)\n",
    "    m = finite.to(scores.dtype)                     # 1 where valid, 0 where masked\n",
    "    n = m.sum(dim=dim, keepdim=True).clamp_min(1)  # count of valid entries per row\n",
    "\n",
    "    # mean/var over valid entries only (population var)\n",
    "    safe_scores = torch.where(finite, scores, torch.zeros_like(scores))\n",
    "    mean = (safe_scores * m).sum(dim=dim, keepdim=True) / n\n",
    "    var  = ((safe_scores - mean)**2 * m).sum(dim=dim, keepdim=True) / n\n",
    "    std  = var.clamp_min(eps).sqrt()\n",
    "\n",
    "    scaled = (safe_scores - mean) / std\n",
    "    scaled = torch.where(finite, scaled, float('-inf'))  # restore mask\n",
    "    out = torch.softmax(scaled, dim=dim)\n",
    "    out = torch.where(n == 0, torch.zeros_like(out), out)  # fully-masked rows -> zeros\n",
    "    return out\n",
    "\n",
    "# ----------------------------\n",
    "# 2D EM-like convective‚Äìdiffusive updater (time-causal; mixes across heads)\n",
    "# ----------------------------\n",
    "class EMDiffuse2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Evolve X on a (time T √ó heads H) grid using geometry G (same shape) to\n",
    "    produce per-position, per-channel coefficients:\n",
    "        a_t : advection speed along time (forward only, upwind in T)\n",
    "        d_t : diffusion along time (backward Laplacian in T)\n",
    "        d_h : diffusion across heads (nearest-neighbor Laplacian at t-1)\n",
    "\n",
    "    Update (per pass), for t>=0, h in [0..H-1]:\n",
    "        X^{new}_{t,h} = X_{t,h}\n",
    "                        + dt * [ -a_t * (X_{t,h} - X_{t-1,h})\n",
    "                                 + d_t * (X_{t,h} - 2 X_{t-1,h} + X_{t-2,h})\n",
    "                                 + d_h * (X_{t-1,h+1} + X_{t-1,h-1} - 2 X_{t-1,h}) ]\n",
    "\n",
    "    Notes:\n",
    "      ‚Ä¢ Causal in time: only uses t, t-1, t-2. No look-ahead.\n",
    "      ‚Ä¢ Cross-head mixing uses data from t-1, so effects manifest only downstream in time.\n",
    "      ‚Ä¢ Coeffs come from G via a small linear; clamped for stability.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, n_passes: int = 1,\n",
    "                 a_max: float = 1.0, d_t_max: float = 0.35, d_h_max: float = 0.15):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_passes = max(1, int(n_passes))\n",
    "        self.a_max = float(a_max)\n",
    "        self.d_t_max = float(d_t_max)\n",
    "        self.d_h_max = float(d_h_max)\n",
    "\n",
    "        # geometry -> [a_t, d_t, d_h] per channel\n",
    "        self.coeff = nn.Linear(dim, 3*dim, bias=True)\n",
    "        nn.init.xavier_uniform_(self.coeff.weight)\n",
    "        nn.init.zeros_(self.coeff.bias)\n",
    "\n",
    "        # learnable global step in (0,1)\n",
    "        self._dt = nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "    def _pad_time(self, X, k=1):\n",
    "        # prepend k zeros along time\n",
    "        B, T, H, D = X.shape\n",
    "        z = torch.zeros(B, k, H, D, device=X.device, dtype=X.dtype)\n",
    "        return torch.cat([z, X[:, :T-k]], dim=1) if k <= T else torch.cat([z, z[:, :0]], dim=1)\n",
    "\n",
    "    def _lap_head_prev(self, X_tm1):\n",
    "        # head Laplacian at t-1 with zero boundary (minimal bleed at edges)\n",
    "        B, T, H, D = X_tm1.shape\n",
    "        left  = torch.cat([torch.zeros(B, T, 1, D, device=X_tm1.device, dtype=X_tm1.dtype),\n",
    "                           X_tm1[:, :, :-1]], dim=2)\n",
    "        right = torch.cat([X_tm1[:, :, 1:],\n",
    "                           torch.zeros(B, T, 1, D, device=X_tm1.device, dtype=X_tm1.dtype)], dim=2)\n",
    "        return left + right - 2.0 * X_tm1\n",
    "\n",
    "    def forward(self, X: torch.Tensor, G: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        X, G: (B*, T, H, D)\n",
    "        \"\"\"\n",
    "        assert X.shape == G.shape and X.dim() == 4, \"X,G must be (B*,T,H,D)\"\n",
    "        Bstar, T, H, D = X.shape\n",
    "        assert D == self.dim\n",
    "\n",
    "        # coefficients from geometry; clamp for stability\n",
    "        cg = self.coeff(G)                      # (B*, T, H, 3D)\n",
    "        a_raw, dt_raw, dh_raw = torch.chunk(cg, 3, dim=-1)\n",
    "        a_t  = torch.sigmoid(a_raw) * self.a_max           # [0, a_max]\n",
    "        d_t  = (F.softplus(dt_raw) / (1.0 + F.softplus(dt_raw))) * self.d_t_max  # [0, d_t_max]\n",
    "        d_h  = (F.softplus(dh_raw) / (1.0 + F.softplus(dh_raw))) * self.d_h_max  # [0, d_h_max]\n",
    "\n",
    "        # causal time neighbors\n",
    "        X_tm1 = self._pad_time(X, k=1)\n",
    "        X_tm2 = self._pad_time(X, k=2)\n",
    "\n",
    "        # head Laplacian taken at t-1 (so cross-head influence appears downstream only)\n",
    "        lap_h_prev = self._lap_head_prev(X_tm1)\n",
    "\n",
    "        # global causal step\n",
    "        dt = torch.sigmoid(self._dt)  # (0,1)\n",
    "\n",
    "        Xn = X\n",
    "        for _ in range(self.n_passes):\n",
    "            # recompute causal neighbors from current state\n",
    "            Xm1 = self._pad_time(Xn, k=1)\n",
    "            Xm2 = self._pad_time(Xn, k=2)\n",
    "            lap_h_prev = self._lap_head_prev(Xm1)\n",
    "\n",
    "            # upwind first difference in time, backward Laplacian in time\n",
    "            dx_t  = Xn - Xm1\n",
    "            lap_t = Xn - 2.0*Xm1 + Xm2\n",
    "\n",
    "            update = -a_t * dx_t + d_t * lap_t + d_h * lap_h_prev\n",
    "            Xn = Xn + dt * update\n",
    "\n",
    "        return Xn\n",
    "\n",
    "# ----------------------------\n",
    "# Wire into attention: evolve q with 2D EM-like updater using v as geometry\n",
    "# ----------------------------\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        assert self.n_embd % self.n_head == 0\n",
    "\n",
    "        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)\n",
    "        self.c_k = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)\n",
    "        self.c_v = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "\n",
    "        # 2D time-causal updater: mixes across heads via (t-1) head-Laplacian\n",
    "        self.em2d = EMDiffuse2D(self.head_dim, n_passes=1,\n",
    "                                a_max=1.0, d_t_max=0.35, d_h_max=0.15)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size)\n",
    "        )\n",
    "        # rope-like signifier: 2D phase over (time, head) mapped into head_dim\n",
    "        self.omega_t = nn.Parameter(torch.tensor(0.01))   # time twist\n",
    "        self.omega_h = nn.Parameter(torch.tensor(0.10))   # head twist\n",
    "\n",
    "        self.rope_proj = nn.Linear(4, self.head_dim, bias=False)\n",
    "        nn.init.normal_(self.rope_proj.weight, mean=0.0, std=1e-2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        H, Dh = self.n_head, self.head_dim\n",
    "\n",
    "        # project\n",
    "        q = self.c_q(x).view(B, T, H, Dh)\n",
    "        k = self.c_k(x).view(B, T, H, Dh)\n",
    "        v = self.c_v(x).view(B, T, H, Dh)\n",
    "\n",
    "        # build rope feature R(t,h) : proper broadcast to (1, T, H, 4)\n",
    "        t_idx = torch.arange(T, device=x.device, dtype=x.dtype).view(1, T, 1, 1)\n",
    "        h_idx = torch.arange(H, device=x.device, dtype=x.dtype).view(1, 1, H, 1)\n",
    "\n",
    "        # expand to full grid so concat works\n",
    "        phase_t = t_idx.expand(1, T, H, 1) * self.omega_t\n",
    "        phase_h = h_idx.expand(1, T, H, 1) * self.omega_h\n",
    "\n",
    "        R = torch.cat([\n",
    "            torch.sin(phase_t),   # (1,T,H,1)\n",
    "            torch.cos(phase_t),   # (1,T,H,1)\n",
    "            torch.sin(phase_h),   # (1,T,H,1)\n",
    "            torch.cos(phase_h),   # (1,T,H,1)\n",
    "        ], dim=-1)  # final shape (1, T, H, 4)\n",
    "\n",
    "        # map rope feature into head_dim and broadcast over batch\n",
    "        rope_emb = self.rope_proj(R)              # (1, T, H, Dh)\n",
    "        rope_emb = rope_emb.expand(B, -1, -1, -1) # (B, T, H, Dh)\n",
    "\n",
    "        # two geometry copies derived from V : differential signal\n",
    "        G_q = v + rope_emb\n",
    "        G_k = v - rope_emb\n",
    "\n",
    "        # evolve q and k on (T x H) with their respective geometry\n",
    "        q = self.em2d(q, G_q)\n",
    "        k = self.em2d(k, G_k)\n",
    "        # attention expects (B, H, T, Dh)\n",
    "        q = q.permute(0, 2, 1, 3).contiguous()\n",
    "        k = k.permute(0, 2, 1, 3).contiguous()\n",
    "        v = v.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # scaled dot-product attention with causal mask\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = variance_scaled_softmax(att, dim=-1)\n",
    "        y = att @ v\n",
    "\n",
    "        # merge heads\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Transformer Block\n",
    "# ----------------------------\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=None)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "         x = x + self.attn(self.ln_1(x))\n",
    "         x = x + self.mlp(self.ln_2(x))\n",
    "         return x\n",
    "\n",
    "def soft_ce(logits: torch.Tensor, target_probs: torch.Tensor) -> torch.Tensor:\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    loss = -(target_probs * logp).sum(dim=-1)\n",
    "    return loss.mean()\n",
    "\n",
    "def sharpen_distribution(idx: torch.Tensor, p: torch.Tensor, V: int, alpha: float) -> torch.Tensor:\n",
    "    B, T, K = idx.shape\n",
    "    out = torch.full((B, T, V), 0.0, dtype=p.dtype, device=p.device)\n",
    "    q = torch.clamp(p, min=1e-12) ** alpha\n",
    "    q = q / q.sum(dim=-1, keepdim=True)\n",
    "    out.scatter_add_(dim=-1, index=idx, src=q)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 66\n",
    "    n_layer: int = 4\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 128\n",
    "    n_kv_head: int = 8\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            \"wte\": nn.Embedding(\n",
    "                config.vocab_size, config.n_embd),\n",
    "            \"h\": nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            \"ln_f\": LayerNorm(config.n_embd, bias=config.bias),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.apply(self._init_weights)\n",
    "        # zero out classifier weights\n",
    "        torch.nn.init.zeros_(self.lm_head.weight)\n",
    "        # zero out c_proj weights in all blocks\n",
    "        for block in self.transformer.h:\n",
    "            torch.nn.init.zeros_(block.mlp.c_proj.weight)\n",
    "            torch.nn.init.zeros_(block.attn.c_proj.weight)\n",
    "        # init the rotary embeddings\n",
    "        head_dim = self.config.n_embd // self.config.n_head\n",
    "        # Cast the embeddings from fp32 to bf16: optim can tolerate it and it saves memory: both in the model and the activations\n",
    "        if self.transformer.wte.weight.device.type == \"cuda\":\n",
    "            self.transformer.wte.to(dtype=torch.bfloat16)\n",
    "\n",
    "    # ------------------------\n",
    "    # Forward\n",
    "    # ------------------------\n",
    "\n",
    "    def forward(self,\n",
    "                idx: torch.Tensor,\n",
    "                targets: Optional[torch.Tensor] = None,\n",
    "                zb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
    "        \"\"\"\n",
    "        idx: (B, T) Long\n",
    "        targets: (B, T) Long or None\n",
    "        zb: tuple (Z_idx, Z_p) for distributional aux \n",
    "       \n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        B, T = idx.size()\n",
    "\n",
    "        T0 = 0 \n",
    "\n",
    "        tok_emb = self.transformer[\"wte\"](idx)                 # (B, T, D)\n",
    "        x = tok_emb\n",
    "\n",
    "        # defaults\n",
    "        L = len(self.transformer[\"h\"])\n",
    "        depth_alphas = [0.8 + 1.2 * (i/(L-1)) for i in range(L)] if L > 1 else [1.0]\n",
    "        x = norm(x)\n",
    "        # aux distribution pack\n",
    "        Z_idx, Z_p = zb if zb is not None else (None, None)\n",
    "        aux_loss = None\n",
    "\n",
    "        # blocks\n",
    "        for bidx, block in enumerate(self.transformer[\"h\"]):\n",
    "            x = block(x)                                       # (B, T, D)\n",
    "\n",
    "            # in-pass early head for self-teaching\n",
    "            #  Z-based distributional aux loss \n",
    "            if (Z_idx is not None) and (Z_p is not None):\n",
    "                V = self.lm_head.out_features\n",
    "                logits_b = self.lm_head(self.transformer[\"ln_f\"](x))\n",
    "                Z_dense = sharpen_distribution(Z_idx, Z_p, V, alpha=float(depth_alphas[bidx]))\n",
    "                aux_b = soft_ce(logits_b, Z_dense)\n",
    "                aux_loss = aux_b if aux_loss is None else aux_loss + aux_b\n",
    "\n",
    "        x = self.transformer[\"ln_f\"](x)\n",
    "        logits = self.lm_head(x)                               # (B, T, V)\n",
    "\n",
    "        # standard CE\n",
    "        ce_loss = None\n",
    "        if targets is not None:\n",
    "            ce_loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "\n",
    "        # total\n",
    "        total = None\n",
    "        parts = []\n",
    "        if ce_loss is not None:\n",
    "            total = ce_loss\n",
    "        if aux_loss is not None:\n",
    "            total = aux_loss if total is None else total + aux_loss\n",
    "   \n",
    "        # generation convenience\n",
    "        if targets is None:\n",
    "            logits = logits[:, [-1], :]\n",
    "\n",
    "        return logits, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d76ace-c3c1-4a68-adce-851c842dbbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading aochildes.txt...\n",
      "üì• Downloading cbt.txt...\n",
      "üì• Downloading children_stories.txt...\n",
      "üì• Downloading gutenberg.txt...\n",
      "üì• Downloading qed.txt...\n",
      "üì• Downloading simple_wikipedia.txt...\n",
      "üì• Downloading switchboard.txt...\n",
      "üì• Downloading wikipedia.txt...\n",
      "üì• Downloading shakespeare.txt...\n",
      "‚úÖ Done. Files saved to ./babylm_10m_cleaned\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"üì• Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"‚úÖ Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a695ab-753c-4b35-8834-a1d4f59859bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Char tokenizer finalized.\n",
      "üßæ Train tokens: 1016242 | Val tokens: 99152\n",
      "üî§ Vocab size: 66\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Char tokenizer finalized.\")\n",
    "print(f\"üßæ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"üî§ Vocab size: {len(stoi)}\")\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "data_dir = \"./babylm_char_tokenized\"\n",
    "train_path = os.path.join(data_dir, \"train.bin\")\n",
    "val_path   = os.path.join(data_dir, \"val.bin\")\n",
    "meta_path  = os.path.join(data_dir, \"meta.pkl\")\n",
    "train_ids = np.fromfile(train_path, dtype=np.uint16)\n",
    "val_ids   = np.fromfile(val_path,   dtype=np.uint16)\n",
    "\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "vocab_size = meta[\"vocab_size\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e48b14-a88f-48b6-b3e6-5507e3990262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Char tokenizer finalized.\n",
      "üßæ Train tokens: 1016242 | Val tokens: 99152\n",
      "üî§ Vocab size: 66\n",
      "Loaded 1016242 train tokens and 99152 val tokens | vocab=66\n",
      "Building order-2 Markov...\n",
      "Building order-4 Markov...\n",
      "Building order-8 Markov...\n",
      "Building order-16 Markov...\n",
      "Building order-32 Markov...\n",
      "Building order-64 Markov...\n",
      "Building bigram db...\n",
      "‚úÖ Markov and Bigram models saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Char tokenizer finalized.\")\n",
    "print(f\"üßæ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"üî§ Vocab size: {len(stoi)}\")\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "data_dir = \"./babylm_char_tokenized\"\n",
    "train_path = os.path.join(data_dir, \"train.bin\")\n",
    "val_path   = os.path.join(data_dir, \"val.bin\")\n",
    "meta_path  = os.path.join(data_dir, \"meta.pkl\")\n",
    "train_ids = np.fromfile(train_path, dtype=np.uint16)\n",
    "val_ids   = np.fromfile(val_path,   dtype=np.uint16)\n",
    "\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "vocab_size = meta[\"vocab_size\"]\n",
    "\n",
    "def global_freqs(ids: np.ndarray, V: int) -> np.ndarray:\n",
    "    cnt = np.bincount(ids.astype(np.int64), minlength=V)\n",
    "    # normalize to probability (avoid zero)\n",
    "    p = cnt.astype(np.float64)\n",
    "    p = p / max(1.0, p.sum())\n",
    "    return p\n",
    "print(f\"Loaded {len(train_ids)} train tokens and {len(val_ids)} val tokens | vocab={vocab_size}\")\n",
    "p_global = global_freqs(train_ids, vocab_size)  # used for disciplined fill only\n",
    "\n",
    "def build_markov_chain(data: np.ndarray, window: int) -> Dict[Tuple[int, ...], Counter]:\n",
    "    chain = defaultdict(Counter)\n",
    "    for i in range(len(data) - window):\n",
    "        ctx = tuple(map(int, data[i:i+window]))\n",
    "        nxt = int(data[i+window])\n",
    "        chain[ctx][nxt] += 1\n",
    "    return chain\n",
    "\n",
    "ngram_orders = [2,4,8,16,32,64]\n",
    "markov_models: Dict[int, Dict[Tuple[int,...], Counter]] = {}\n",
    "for w in ngram_orders:\n",
    "    print(f\"Building order-{w} Markov...\")\n",
    "    markov_models[w] = build_markov_chain(train_ids, w)\n",
    "\n",
    "def build_bigram_db(data: np.ndarray, V: int, top_k=16, epsilon=1e-6, seed=1337):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    counts = np.zeros((V, V), dtype=np.int64)\n",
    "    a = data[:-1].astype(np.int64)\n",
    "    b = data[1:].astype(np.int64)\n",
    "    np.add.at(counts, (a, b), 1)\n",
    "    out = {}\n",
    "    all_ids = np.arange(V, dtype=np.int64)\n",
    "    for t in range(V):\n",
    "        row = counts[t]\n",
    "        tot = row.sum()\n",
    "        if tot == 0:\n",
    "            idx = rng.choice(V, size=top_k, replace=False)\n",
    "            p = np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "        else:\n",
    "            pr = row.astype(np.float64) / float(tot)\n",
    "            obs = np.flatnonzero(row)\n",
    "            if len(obs) >= top_k:\n",
    "                sel = np.argpartition(pr[obs], -top_k)[-top_k:]\n",
    "                idx = obs[sel]\n",
    "                p = pr[idx].astype(np.float32)\n",
    "                s = p.sum()\n",
    "                p = p/s if s > 0 else np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "            else:\n",
    "                need = top_k - len(obs)\n",
    "                mask = np.ones(V, dtype=bool); mask[obs] = False\n",
    "                extra = np.random.default_rng(seed+t).choice(np.nonzero(mask)[0], size=need, replace=False)\n",
    "                idx = np.concatenate([obs, extra])\n",
    "                p   = pr[idx].astype(np.float32)\n",
    "                # give epsilon to never-seen extras\n",
    "                unseen = (row[idx] == 0)\n",
    "                if unseen.any():\n",
    "                    p = p + unseen.astype(np.float32) * epsilon\n",
    "                p = p / p.sum()\n",
    "        order = np.argsort(-p)\n",
    "        out[t] = (idx[order].astype(np.int64), p[order])\n",
    "    return out\n",
    "\n",
    "print(\"Building bigram db...\")\n",
    "bigram_db = build_bigram_db(train_ids, vocab_size, top_k=64)  # collect a bit wider; we'll cap later\n",
    "\n",
    "# === Save ===\n",
    "model_dir = \"./markov_bigram_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "with open(os.path.join(model_dir, \"markov_models.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(markov_models, f)\n",
    "\n",
    "with open(os.path.join(model_dir, \"bigram_db.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(bigram_db, f)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Markov and Bigram models saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd4dd64a-8f8e-471e-97f2-7ffd1df4c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "# === Replacement dataloader that uses SAVED bigram + markov models and yields (X, Y, Z) ===\n",
    "import os, pickle, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# expects `vocab_size` and `device` already defined in the outer scope\n",
    "# expects saved models at ./markov_bigram_models/{bigram_db.pkl, markov_models.pkl}\n",
    "\n",
    "class DisciplinedZ:\n",
    "    def __init__(self, markov_models: Dict[int, Dict[Tuple[int,...], Counter]],\n",
    "                 bigram_db: Dict[int, Tuple[np.ndarray, np.ndarray]],\n",
    "                 p_global: np.ndarray,\n",
    "                 vocab_size: int,\n",
    "                 top_k: int = 32,\n",
    "                 epsilon: float = 1e-6):\n",
    "        self.models = markov_models\n",
    "        self.bigram_db = bigram_db\n",
    "        self.p_global = p_global.astype(np.float64)\n",
    "        self.V = vocab_size\n",
    "        self.K = top_k\n",
    "        self.eps = float(epsilon)\n",
    "        # global sort for fill\n",
    "        self.global_order = np.argsort(-self.p_global)\n",
    "\n",
    "    def _cands_from_counter(self, ctr: Optional[Counter]) -> Optional[np.ndarray]:\n",
    "        if not ctr:\n",
    "            return None\n",
    "        return np.fromiter((int(t) for t,_ in ctr.items()), dtype=np.int64)\n",
    "\n",
    "    def _probs_from_counter(self, ctr: Optional[Counter]) -> Optional[Dict[int, float]]:\n",
    "        if not ctr:\n",
    "            return None\n",
    "        tot = sum(ctr.values())\n",
    "        if tot == 0:\n",
    "            return None\n",
    "        return {int(t): c/tot for t, c in ctr.items()}\n",
    "\n",
    "    def _bigram_top(self, tok: int, limit: int) -> np.ndarray:\n",
    "        idx, prob = self.bigram_db.get(int(tok), (None, None))\n",
    "        if idx is None:\n",
    "            return np.array([], dtype=np.int64)\n",
    "        return idx[:limit]\n",
    "\n",
    "    def _btree_candidates(self, contexts: Dict[int, Tuple[Tuple[int,...], Optional[Counter]]], backoff_tok: int) -> np.ndarray:\n",
    "        # collect candidate sets from each available context\n",
    "        sets = []\n",
    "        for n, (_, ctr) in contexts.items():\n",
    "            c = self._cands_from_counter(ctr)\n",
    "            if c is not None and c.size > 0:\n",
    "                sets.append(set(c.tolist()))\n",
    "        if len(sets) == 0:\n",
    "            # no ctx ‚Üí use bigram set as starting point\n",
    "            return self._bigram_top(backoff_tok, self.K)\n",
    "\n",
    "        # try full intersection; if empty, progressively intersect strongest contexts first\n",
    "        inter = set.intersection(*sets) if len(sets) > 1 else sets[0]\n",
    "        if len(inter) == 0:\n",
    "            # heuristic: sort by context order (longest first), intersect greedily\n",
    "            sets_sorted = sorted(sets, key=lambda s: -len(s))\n",
    "            inter = sets_sorted[0].copy()\n",
    "            for s in sets_sorted[1:]:\n",
    "                new_inter = inter.intersection(s)\n",
    "                if len(new_inter) > 0:\n",
    "                    inter = new_inter\n",
    "        if len(inter) == 0:\n",
    "            # last resort: union (still disciplined; no random injection)\n",
    "            union = set()\n",
    "            for s in sets:\n",
    "                union |= s\n",
    "            inter = union\n",
    "\n",
    "        arr = np.fromiter(inter, dtype=np.int64)\n",
    "        if arr.size == 0:\n",
    "            return self._bigram_top(backoff_tok, self.K)\n",
    "        return arr\n",
    "\n",
    "    def _score_candidates(self, cands: np.ndarray, contexts: Dict[int, Tuple[Tuple[int,...], Optional[Counter]]]) -> np.ndarray:\n",
    "        # score = sum over contexts of presence * local prob\n",
    "        # local prob from per-context normalized counts\n",
    "        scores = np.zeros(cands.size, dtype=np.float64)\n",
    "        idxmap = {int(t): i for i, t in enumerate(cands)}\n",
    "        for n, (_, ctr) in contexts.items():\n",
    "            probs = self._probs_from_counter(ctr)\n",
    "            if probs is None:\n",
    "                continue\n",
    "            for t, p in probs.items():\n",
    "                if t in idxmap:\n",
    "                    scores[idxmap[t]] += float(p)\n",
    "        # tiny floor to avoid zeros\n",
    "        scores = scores + (scores == 0) * self.eps\n",
    "        return scores\n",
    "\n",
    "    def build_Z_for_sequence(self, seq: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        seq: array of length L = block_size (+optional pad)\n",
    "        returns:\n",
    "          topk_idx: (L, K) int64\n",
    "          topk_p:   (L, K) float32  (row-normalized)\n",
    "        \"\"\"\n",
    "        L = len(seq)\n",
    "        topk_idx = np.zeros((L, self.K), dtype=np.int64)\n",
    "        topk_p   = np.zeros((L, self.K), dtype=np.float32)\n",
    "        for j in range(L):\n",
    "            back_tok = int(seq[j])\n",
    "            # collect contexts\n",
    "            contexts = {}\n",
    "            for n in ngram_orders:\n",
    "                if j - (n-1) < 0:\n",
    "                    continue\n",
    "                ctx = tuple(int(x) for x in seq[j-(n-1):j+1])\n",
    "                ctr = self.models[n].get(ctx, None)\n",
    "                contexts[n] = (ctx, ctr)\n",
    "\n",
    "            # disciplined candidate set\n",
    "            cands = self._btree_candidates(contexts, back_tok)\n",
    "\n",
    "            # cap K by candidate count\n",
    "            if cands.size >= self.K:\n",
    "                # score & take best K\n",
    "                scores = self._score_candidates(cands, contexts)\n",
    "                order = np.argsort(-scores)[:self.K]\n",
    "                idx = cands[order]\n",
    "                sc  = scores[order]\n",
    "            else:\n",
    "                # we must fill with globally-most-common tokens (no randoms), excluding existing\n",
    "                scores = self._score_candidates(cands, contexts) if cands.size > 0 else np.array([], dtype=np.float64)\n",
    "                missing = self.K - cands.size\n",
    "                mask = np.ones(vocab_size, dtype=bool)\n",
    "                mask[cands] = False\n",
    "                fill = []\n",
    "                for t in self.global_order:\n",
    "                    if mask[t]:\n",
    "                        fill.append(int(t))\n",
    "                        if len(fill) == missing:\n",
    "                            break\n",
    "                if cands.size == 0:\n",
    "                    idx = np.array(fill, dtype=np.int64)\n",
    "                    sc  = np.full(len(fill), self.eps, dtype=np.float64)\n",
    "                else:\n",
    "                    idx = np.concatenate([cands, np.array(fill, dtype=np.int64)])\n",
    "                    sc  = np.concatenate([scores, np.full(missing, self.eps, dtype=np.float64)])\n",
    "\n",
    "            # normalize to prob\n",
    "            p = sc.astype(np.float64)\n",
    "            p = p / p.sum() if p.sum() > 0 else np.full_like(p, 1.0/len(p))\n",
    "            topk_idx[j, :] = idx\n",
    "            topk_p[j, :]   = p.astype(np.float32)\n",
    "        return topk_idx, topk_p\n",
    "\n",
    "discZ = DisciplinedZ(markov_models, bigram_db, p_global, vocab_size, top_k=32, epsilon=1e-6)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class GPUDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size: int, batch_size: int, builder: DisciplinedZ, pad_len:int=0, jitter:int=63, p_aligned:float=0.5, seed:int=1337):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = int(block_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.pad_len    = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = max(1, self.total // self.sample_len)\n",
    "        self.jitter = int(jitter)\n",
    "        self.p_aligned = float(p_aligned)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.builder = builder\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def _sample_block(self):\n",
    "        base_block = self.rng.integers(0, self.n_blocks)\n",
    "        start = base_block * self.sample_len\n",
    "        if self.rng.random() > self.p_aligned:\n",
    "            j = self.rng.integers(0, self.jitter + 1)\n",
    "            start = min(start + j, self.total)\n",
    "        return start\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        B, T = self.batch_size, self.block_size\n",
    "        X = np.empty((B, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((B, T), dtype=np.int64)\n",
    "        Z_idx = np.empty((B, T, self.builder.K), dtype=np.int64)\n",
    "        Z_p   = np.empty((B, T, self.builder.K), dtype=np.float32)\n",
    "        for i in range(B):\n",
    "            start = self._sample_block()\n",
    "            xseq = self.data[start : start + self.sample_len].astype(np.int64)\n",
    "            yseq = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + T].astype(np.int64)\n",
    "            X[i] = xseq\n",
    "            Y[i] = yseq\n",
    "            idxs, probs = self.builder.build_Z_for_sequence(xseq[:T])\n",
    "            Z_idx[i] = idxs\n",
    "            Z_p[i]   = probs\n",
    "        # torch tensors\n",
    "        X = torch.from_numpy(X[:, :T]).to(device)\n",
    "        Y = torch.from_numpy(Y).to(device)\n",
    "        Z_idx = torch.from_numpy(Z_idx).to(device)\n",
    "        Z_p   = torch.from_numpy(Z_p).to(device)\n",
    "        return X, Y, (Z_idx, Z_p)\n",
    "\n",
    "def collate_identity(batch):\n",
    "    Xs, Ys, Zs = zip(*batch)\n",
    "    X = torch.cat(Xs, dim=0)\n",
    "    Y = torch.cat(Ys, dim=0)\n",
    "    Zi = torch.cat([z[0] for z in Zs], dim=0)\n",
    "    Zp = torch.cat([z[1] for z in Zs], dim=0)\n",
    "    return X, Y, (Zi, Zp)\n",
    "\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "GPU_DATASET = GPUDataset(\n",
    "    np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r'),\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    builder=discZ,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    GPU_DATASET,\n",
    "    batch_size=1,            # keep outer loader at 1; inner dataset batches on GPU\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_identity\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "efacb560-8e61-4e4e-9399-3c1611b476ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "config =  GPTConfig(\n",
    "    block_size,\n",
    "    vocab_size,\n",
    "    n_layer=4,      \n",
    "    n_head = 8,\n",
    "    n_embd =128)\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "model = torch.compile(model)\n",
    "model = model.to(device)\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, yb, zb in train_loader:\n",
    "        # xb: (B, T), yb: (B, T), zb: (Z_idx, Z_p) with shapes (B,T,K)\n",
    "        logits, loss = model(xb, None,zb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total += loss.item()\n",
    "        print(loss.item())\n",
    "        losses.append(loss.item())\n",
    "    return total / len(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c43d3e5c-ba7e-4a1e-9b59-1b593316e2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811212\n"
     ]
    }
   ],
   "source": [
    "print(sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742f12d-294f-4bcf-8b15-e447dffa45d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2ae6451f-1367-40a2-9fb9-85a5085ef5bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.453693389892578\n",
      "14.763554573059082\n",
      "13.77098274230957\n",
      "13.203584671020508\n",
      "12.92220401763916\n",
      "12.60098648071289\n",
      "12.201725006103516\n",
      "11.656890869140625\n",
      "11.595643997192383\n",
      "11.18924617767334\n",
      "11.315074920654297\n",
      "10.944597244262695\n",
      "10.96977424621582\n",
      "10.589164733886719\n",
      "10.597025871276855\n",
      "10.476603507995605\n",
      "10.412544250488281\n",
      "10.294838905334473\n",
      "10.144824981689453\n",
      "10.15491008758545\n",
      "10.300714492797852\n",
      "10.179580688476562\n",
      "10.104328155517578\n",
      "9.838715553283691\n",
      "9.881558418273926\n",
      "9.949862480163574\n",
      "9.791242599487305\n",
      "9.8859281539917\n",
      "9.837141990661621\n",
      "9.62732982635498\n",
      "9.68205738067627\n",
      "9.627324104309082\n",
      "9.770841598510742\n",
      "9.484748840332031\n",
      "9.573033332824707\n",
      "9.784578323364258\n",
      "9.475741386413574\n",
      "9.624314308166504\n",
      "9.392894744873047\n",
      "9.43175983428955\n",
      "9.343853950500488\n",
      "9.3670654296875\n",
      "9.510416030883789\n",
      "9.458150863647461\n",
      "9.254674911499023\n",
      "9.19708251953125\n",
      "9.36358642578125\n",
      "9.330330848693848\n",
      "9.290142059326172\n",
      "9.331991195678711\n",
      "9.17996597290039\n",
      "9.269172668457031\n",
      "9.143237113952637\n",
      "9.158016204833984\n",
      "9.076835632324219\n",
      "9.180686950683594\n",
      "9.251260757446289\n",
      "9.163299560546875\n",
      "9.19961929321289\n",
      "9.310606956481934\n",
      "9.048510551452637\n",
      "8.974359512329102\n",
      "8.956969261169434\n",
      "9.01588249206543\n",
      "9.097176551818848\n",
      "8.861818313598633\n",
      "8.956611633300781\n",
      "9.039873123168945\n",
      "8.987808227539062\n",
      "8.925003051757812\n",
      "9.03483772277832\n",
      "8.934791564941406\n",
      "8.922483444213867\n",
      "8.918878555297852\n",
      "8.859025955200195\n",
      "8.885324478149414\n",
      "8.923599243164062\n",
      "8.837814331054688\n",
      "8.894857406616211\n",
      "8.78365421295166\n",
      "9.039726257324219\n",
      "8.841024398803711\n",
      "8.790964126586914\n",
      "8.678022384643555\n",
      "8.9149808883667\n",
      "8.965536117553711\n",
      "8.813728332519531\n",
      "8.603033065795898\n",
      "8.783632278442383\n",
      "8.681928634643555\n",
      "8.990181922912598\n",
      "8.855076789855957\n",
      "8.930919647216797\n",
      "8.719586372375488\n",
      "8.629206657409668\n",
      "8.809745788574219\n",
      "8.903592109680176\n",
      "8.82516860961914\n",
      "8.682275772094727\n",
      "8.832425117492676\n",
      "8.58699893951416\n",
      "8.668341636657715\n",
      "8.772809982299805\n",
      "8.897125244140625\n",
      "8.670321464538574\n",
      "8.51933479309082\n",
      "8.694580078125\n",
      "8.677374839782715\n",
      "8.712393760681152\n",
      "8.655221939086914\n",
      "8.644754409790039\n",
      "8.496018409729004\n",
      "8.630411148071289\n",
      "8.5390043258667\n",
      "8.458803176879883\n",
      "8.636635780334473\n",
      "8.60969352722168\n",
      "8.66748332977295\n",
      "8.578924179077148\n",
      "8.514976501464844\n",
      "8.379886627197266\n",
      "8.688627243041992\n",
      "8.507969856262207\n",
      "8.4100341796875\n",
      "8.585911750793457\n",
      "8.453272819519043\n",
      "8.309337615966797\n",
      "8.385530471801758\n",
      "8.56403636932373\n",
      "8.510604858398438\n",
      "8.491653442382812\n",
      "8.560672760009766\n",
      "8.600251197814941\n",
      "8.520819664001465\n",
      "8.532241821289062\n",
      "8.560327529907227\n",
      "8.370532035827637\n",
      "8.476539611816406\n",
      "8.626779556274414\n",
      "8.419931411743164\n",
      "8.336103439331055\n",
      "8.279735565185547\n",
      "8.383417129516602\n",
      "8.503515243530273\n",
      "8.319252014160156\n",
      "8.498611450195312\n",
      "8.242345809936523\n",
      "8.407540321350098\n",
      "8.505718231201172\n",
      "8.499692916870117\n",
      "8.328621864318848\n",
      "8.498802185058594\n",
      "8.355607032775879\n",
      "8.236825942993164\n",
      "8.314480781555176\n",
      "8.391144752502441\n",
      "8.205236434936523\n",
      "8.421501159667969\n",
      "8.509846687316895\n",
      "8.221813201904297\n",
      "8.165312767028809\n",
      "8.181842803955078\n",
      "8.284584045410156\n",
      "8.344011306762695\n",
      "8.258050918579102\n",
      "8.412649154663086\n",
      "8.296730995178223\n",
      "8.210338592529297\n",
      "8.232264518737793\n",
      "8.464350700378418\n",
      "8.275121688842773\n",
      "8.327479362487793\n",
      "8.225296974182129\n",
      "8.22673225402832\n",
      "8.160416603088379\n",
      "8.475066184997559\n",
      "8.120410919189453\n",
      "8.10134220123291\n",
      "8.161173820495605\n",
      "8.321114540100098\n",
      "8.314085006713867\n",
      "8.761490821838379\n",
      "8.805959701538086\n",
      "8.661008834838867\n",
      "8.654985427856445\n",
      "8.331533432006836\n",
      "8.395316123962402\n",
      "8.384544372558594\n",
      "8.440528869628906\n",
      "8.624261856079102\n",
      "8.344193458557129\n",
      "8.218494415283203\n",
      "8.358838081359863\n",
      "8.358192443847656\n",
      "8.542961120605469\n",
      "8.430482864379883\n",
      "8.452579498291016\n",
      "8.311668395996094\n",
      "8.074039459228516\n",
      "8.15988540649414\n",
      "8.153585433959961\n",
      "8.322639465332031\n",
      "8.300817489624023\n",
      "8.117450714111328\n",
      "8.182290077209473\n",
      "8.257026672363281\n",
      "8.088199615478516\n",
      "8.027875900268555\n",
      "8.447615623474121\n",
      "8.200969696044922\n",
      "8.108205795288086\n",
      "8.521146774291992\n",
      "8.046464920043945\n",
      "8.103344917297363\n",
      "7.931563377380371\n",
      "7.999087333679199\n",
      "8.213022232055664\n",
      "8.057985305786133\n",
      "8.127403259277344\n",
      "8.19608211517334\n",
      "8.079103469848633\n",
      "8.086603164672852\n",
      "7.808699607849121\n",
      "7.9471869468688965\n",
      "8.098894119262695\n",
      "8.162181854248047\n",
      "8.032203674316406\n",
      "7.9728875160217285\n",
      "8.077536582946777\n",
      "7.991440296173096\n",
      "8.083114624023438\n",
      "8.127058029174805\n",
      "8.068367958068848\n",
      "7.943122386932373\n",
      "7.950766086578369\n",
      "7.9822235107421875\n",
      "7.939133644104004\n",
      "7.895041465759277\n",
      "8.10499382019043\n",
      "7.9802350997924805\n",
      "7.7065277099609375\n",
      "7.955526351928711\n",
      "8.120595932006836\n",
      "8.135448455810547\n",
      "8.040498733520508\n",
      "8.065153121948242\n",
      "7.784455299377441\n",
      "7.9323344230651855\n",
      "8.035308837890625\n",
      "7.843937397003174\n",
      "8.077510833740234\n",
      "8.065347671508789\n",
      "7.986643314361572\n",
      "7.911450386047363\n",
      "7.8095011711120605\n",
      "7.7386322021484375\n",
      "8.013202667236328\n",
      "7.916734218597412\n",
      "7.93971586227417\n",
      "8.158143997192383\n",
      "7.723991394042969\n",
      "7.888853073120117\n",
      "7.7350754737854\n",
      "7.688788890838623\n",
      "7.94061279296875\n",
      "7.682441234588623\n",
      "7.888473987579346\n",
      "7.707486629486084\n",
      "7.696844100952148\n",
      "7.83169412612915\n",
      "7.825843811035156\n",
      "7.903719425201416\n",
      "7.891082763671875\n",
      "7.77729606628418\n",
      "7.869441032409668\n",
      "7.7718400955200195\n",
      "7.6823015213012695\n",
      "7.751831531524658\n",
      "7.664514541625977\n",
      "7.635101795196533\n",
      "7.681708812713623\n",
      "7.86478328704834\n",
      "7.847514629364014\n",
      "7.703951835632324\n",
      "7.8273396492004395\n",
      "7.687442779541016\n",
      "7.915474891662598\n",
      "7.786219120025635\n",
      "7.814371585845947\n",
      "7.921000957489014\n",
      "7.810400485992432\n",
      "7.891843318939209\n",
      "7.94781494140625\n",
      "7.850805759429932\n",
      "8.09262466430664\n",
      "8.298670768737793\n",
      "8.036819458007812\n",
      "7.832446098327637\n",
      "7.6717329025268555\n",
      "8.101217269897461\n",
      "7.903694152832031\n",
      "7.741638660430908\n",
      "7.716106414794922\n",
      "7.644763946533203\n",
      "7.75853157043457\n",
      "7.712288856506348\n",
      "7.642059803009033\n",
      "7.69063663482666\n",
      "7.661099433898926\n",
      "7.593478202819824\n",
      "7.638381481170654\n",
      "7.74086856842041\n",
      "7.512334823608398\n",
      "7.750802993774414\n",
      "7.873205661773682\n",
      "7.780202388763428\n",
      "7.8084564208984375\n",
      "7.738788604736328\n",
      "7.605316162109375\n",
      "7.550339698791504\n",
      "7.594803333282471\n",
      "7.638886451721191\n",
      "7.664181709289551\n",
      "7.560089111328125\n",
      "7.628279209136963\n",
      "7.632925033569336\n",
      "7.7281880378723145\n",
      "7.761412143707275\n",
      "7.645895004272461\n",
      "7.809996604919434\n",
      "7.787502288818359\n",
      "7.5207719802856445\n",
      "7.7121686935424805\n",
      "7.8365302085876465\n",
      "7.944552898406982\n",
      "7.890305042266846\n",
      "7.907710552215576\n",
      "7.818394660949707\n",
      "7.903165817260742\n",
      "7.647905349731445\n",
      "7.700716495513916\n",
      "7.581531524658203\n",
      "7.7448625564575195\n",
      "7.810986518859863\n",
      "7.58329963684082\n",
      "7.491852760314941\n",
      "7.4933061599731445\n",
      "7.5317888259887695\n",
      "7.557826042175293\n",
      "7.6571831703186035\n",
      "7.625753402709961\n",
      "7.8011579513549805\n",
      "7.798911094665527\n",
      "7.690872669219971\n",
      "7.741656303405762\n",
      "7.72339391708374\n",
      "7.498733997344971\n",
      "7.682197570800781\n",
      "7.640038013458252\n",
      "7.6140947341918945\n",
      "7.824474334716797\n",
      "7.612174034118652\n",
      "7.589792251586914\n",
      "7.42661190032959\n",
      "7.504525661468506\n",
      "7.550233364105225\n",
      "7.855172157287598\n",
      "7.595983505249023\n",
      "7.567775249481201\n",
      "7.650547981262207\n",
      "7.656900882720947\n",
      "7.250316619873047\n",
      "7.541417598724365\n",
      "7.50229549407959\n",
      "7.681838035583496\n",
      "7.377436637878418\n",
      "7.751497745513916\n",
      "7.591524124145508\n",
      "7.360694408416748\n",
      "7.71115255355835\n",
      "7.283993244171143\n",
      "7.590714454650879\n",
      "7.688083648681641\n",
      "7.590121746063232\n",
      "7.584336280822754\n",
      "7.668247222900391\n",
      "7.778483867645264\n",
      "7.458022117614746\n",
      "7.43717622756958\n",
      "7.537255764007568\n",
      "7.610987186431885\n",
      "7.115954399108887\n",
      "7.3965864181518555\n",
      "7.444690227508545\n",
      "7.839461803436279\n",
      "7.573120594024658\n",
      "7.4888715744018555\n",
      "7.632420063018799\n",
      "7.560555934906006\n",
      "7.580541610717773\n",
      "7.713675022125244\n",
      "7.6588897705078125\n",
      "7.568613052368164\n",
      "7.345710754394531\n",
      "7.48940896987915\n",
      "7.4790940284729\n",
      "7.37324333190918\n",
      "7.20839786529541\n",
      "7.4590864181518555\n",
      "7.429079055786133\n",
      "7.2545390129089355\n",
      "7.4662041664123535\n",
      "7.422441482543945\n",
      "7.445001125335693\n",
      "7.511645317077637\n",
      "7.377019882202148\n",
      "7.3888163566589355\n",
      "7.352628231048584\n",
      "7.41534423828125\n",
      "7.572185516357422\n",
      "7.776307106018066\n",
      "7.8561859130859375\n",
      "7.682727336883545\n",
      "7.323394775390625\n",
      "7.614320755004883\n",
      "7.43746280670166\n",
      "7.437103271484375\n",
      "7.497130393981934\n",
      "7.192495346069336\n",
      "7.458018779754639\n",
      "7.2148847579956055\n",
      "7.54312801361084\n",
      "7.132866859436035\n",
      "7.516358852386475\n",
      "7.451643466949463\n",
      "7.125273704528809\n",
      "7.715719223022461\n",
      "7.389415740966797\n",
      "7.3560051918029785\n",
      "7.296966552734375\n",
      "7.386094093322754\n",
      "7.2287917137146\n",
      "7.231801986694336\n",
      "7.4253830909729\n",
      "7.47398567199707\n",
      "7.327676296234131\n",
      "7.35459041595459\n",
      "7.527297019958496\n",
      "7.317238807678223\n",
      "7.409570693969727\n",
      "7.300177097320557\n",
      "7.247591018676758\n",
      "7.633266448974609\n",
      "7.278785705566406\n",
      "7.398750305175781\n",
      "7.39564847946167\n",
      "7.188965797424316\n",
      "7.22505521774292\n",
      "7.356081962585449\n",
      "7.671811103820801\n",
      "7.348290920257568\n",
      "7.141214847564697\n",
      "7.435404300689697\n",
      "7.425961971282959\n",
      "7.037721157073975\n",
      "7.3756256103515625\n",
      "7.385980606079102\n",
      "7.650823593139648\n",
      "7.645535945892334\n",
      "7.629580497741699\n",
      "7.51237678527832\n",
      "7.251579761505127\n",
      "7.443361282348633\n",
      "7.5259809494018555\n",
      "7.476984977722168\n",
      "7.359113693237305\n",
      "7.328042030334473\n",
      "7.259452819824219\n",
      "7.196840286254883\n",
      "7.2056050300598145\n",
      "7.024812698364258\n",
      "7.049177169799805\n",
      "7.336791038513184\n",
      "7.197118759155273\n",
      "7.308804035186768\n",
      "7.258347511291504\n",
      "7.1265764236450195\n",
      "7.166696548461914\n",
      "6.994287490844727\n",
      "7.095244884490967\n",
      "7.354945182800293\n",
      "7.260258197784424\n",
      "7.313412666320801\n",
      "7.085491180419922\n",
      "7.409283638000488\n",
      "7.357498645782471\n",
      "7.2791643142700195\n",
      "7.091763973236084\n",
      "7.184190273284912\n",
      "7.573019504547119\n",
      "7.5354719161987305\n",
      "7.495749473571777\n",
      "7.367036819458008\n",
      "7.469289302825928\n",
      "7.329051971435547\n",
      "7.377912998199463\n",
      "7.220369338989258\n",
      "7.329676151275635\n",
      "7.239020347595215\n",
      "7.0569329261779785\n",
      "7.165791988372803\n",
      "7.326889514923096\n",
      "7.648662567138672\n",
      "7.116065502166748\n",
      "7.185741901397705\n",
      "7.202605247497559\n",
      "7.342247009277344\n",
      "7.370707035064697\n",
      "7.403724670410156\n",
      "7.285511493682861\n",
      "7.158267974853516\n",
      "7.29657506942749\n",
      "7.067019939422607\n",
      "7.364472389221191\n",
      "7.330523490905762\n",
      "7.378424644470215\n",
      "6.9460296630859375\n",
      "7.336399078369141\n",
      "7.287934303283691\n",
      "7.2857561111450195\n",
      "7.730407238006592\n",
      "7.995429515838623\n",
      "7.940250873565674\n",
      "7.555957794189453\n",
      "7.788782596588135\n",
      "7.624560356140137\n",
      "7.46317195892334\n",
      "7.520000457763672\n",
      "7.540949821472168\n",
      "7.337166786193848\n",
      "7.333339691162109\n",
      "7.4898858070373535\n",
      "7.230656623840332\n",
      "7.397080421447754\n",
      "7.141511917114258\n",
      "7.308746337890625\n",
      "7.363204002380371\n",
      "7.328971862792969\n",
      "7.26685905456543\n",
      "7.060163497924805\n",
      "7.453411102294922\n",
      "7.393228530883789\n",
      "7.252805233001709\n",
      "7.1762166023254395\n",
      "7.0578460693359375\n",
      "7.410638809204102\n",
      "7.020732879638672\n",
      "7.279226779937744\n",
      "7.239058494567871\n",
      "7.283294677734375\n",
      "7.007299900054932\n",
      "6.89898681640625\n",
      "7.166067123413086\n",
      "7.1297197341918945\n",
      "7.234034538269043\n",
      "7.3300957679748535\n",
      "7.177169322967529\n",
      "7.195055961608887\n",
      "7.468655109405518\n",
      "7.401269435882568\n",
      "7.435393333435059\n",
      "7.492772102355957\n",
      "7.421882629394531\n",
      "7.319760799407959\n",
      "7.012047290802002\n",
      "7.30087947845459\n",
      "7.242270469665527\n",
      "7.155512809753418\n",
      "7.001183986663818\n",
      "7.050623893737793\n",
      "7.332329750061035\n",
      "6.90571403503418\n",
      "7.420248985290527\n",
      "7.187331199645996\n",
      "7.296906471252441\n",
      "7.187843322753906\n",
      "7.293862342834473\n",
      "7.347243785858154\n",
      "7.146738529205322\n",
      "7.190033912658691\n",
      "7.130544185638428\n",
      "6.931811332702637\n",
      "7.2796196937561035\n",
      "7.062226295471191\n",
      "7.1876749992370605\n",
      "7.305649757385254\n",
      "6.9644694328308105\n",
      "6.958169460296631\n",
      "7.049403190612793\n",
      "7.259875297546387\n",
      "7.649622440338135\n",
      "7.449763298034668\n",
      "7.421749114990234\n",
      "7.4908857345581055\n",
      "7.4649457931518555\n",
      "7.423562526702881\n",
      "7.028695583343506\n",
      "7.4557318687438965\n",
      "7.187178611755371\n",
      "7.132823944091797\n",
      "7.344326972961426\n",
      "7.190187454223633\n",
      "6.998332977294922\n",
      "7.06541109085083\n",
      "7.0833539962768555\n",
      "7.263886451721191\n",
      "7.295123100280762\n",
      "7.127148151397705\n",
      "7.016598701477051\n",
      "7.170804977416992\n",
      "7.572793960571289\n",
      "7.397378444671631\n",
      "7.296487808227539\n",
      "7.183962821960449\n",
      "7.028317451477051\n",
      "7.0977277755737305\n",
      "7.097813129425049\n",
      "7.2359619140625\n",
      "7.23714017868042\n",
      "7.072049140930176\n",
      "7.222084045410156\n",
      "7.1942009925842285\n",
      "6.862898349761963\n",
      "7.016568183898926\n",
      "6.983104705810547\n",
      "7.10647439956665\n",
      "6.911501407623291\n",
      "6.999359130859375\n",
      "7.08461856842041\n",
      "7.049132347106934\n",
      "7.16475248336792\n",
      "7.049154281616211\n",
      "6.914661407470703\n",
      "7.086472988128662\n",
      "7.28208589553833\n",
      "7.420510292053223\n",
      "7.137048721313477\n",
      "7.175922870635986\n",
      "7.025650978088379\n",
      "7.143017768859863\n",
      "7.223834991455078\n",
      "7.323639392852783\n",
      "7.118596076965332\n",
      "6.939946174621582\n",
      "7.2876434326171875\n",
      "7.037110805511475\n",
      "7.219722747802734\n",
      "7.151808261871338\n",
      "7.0864691734313965\n",
      "7.320422172546387\n",
      "7.086552619934082\n",
      "7.031704425811768\n",
      "7.199472427368164\n",
      "7.064535140991211\n",
      "7.157301902770996\n",
      "7.085786819458008\n",
      "7.094115257263184\n",
      "7.047598838806152\n",
      "6.883213043212891\n",
      "7.1059770584106445\n",
      "6.948523044586182\n",
      "6.965456962585449\n",
      "7.25510835647583\n",
      "7.221593856811523\n",
      "7.0783491134643555\n",
      "7.3393940925598145\n",
      "6.95465612411499\n",
      "7.187033653259277\n",
      "7.067882061004639\n",
      "7.086352825164795\n",
      "7.174369812011719\n",
      "7.091681957244873\n",
      "7.308671951293945\n",
      "7.264041423797607\n",
      "7.103354454040527\n",
      "7.068539619445801\n",
      "7.124700546264648\n",
      "7.152543544769287\n",
      "7.107366561889648\n",
      "7.226007461547852\n",
      "7.047033309936523\n",
      "7.112736701965332\n",
      "6.977824687957764\n",
      "6.994222640991211\n",
      "7.033660888671875\n",
      "6.961478233337402\n",
      "6.912271022796631\n",
      "6.856767654418945\n",
      "6.927173614501953\n",
      "6.8073530197143555\n",
      "6.805446147918701\n",
      "6.929836273193359\n",
      "7.002902984619141\n",
      "6.853170871734619\n",
      "7.150808334350586\n",
      "7.040369510650635\n",
      "7.0583343505859375\n",
      "7.010093688964844\n",
      "6.991109848022461\n",
      "6.816913604736328\n",
      "7.069329738616943\n",
      "6.921020984649658\n",
      "7.087656021118164\n",
      "6.93121862411499\n",
      "6.990184307098389\n",
      "7.015542507171631\n",
      "7.0146989822387695\n",
      "7.179913520812988\n",
      "6.829119682312012\n",
      "6.932973384857178\n",
      "7.100125789642334\n",
      "6.964308261871338\n",
      "6.9241180419921875\n",
      "7.029232978820801\n",
      "6.814033031463623\n",
      "6.842307090759277\n",
      "7.143606185913086\n",
      "7.463146209716797\n",
      "7.6664204597473145\n",
      "7.811417579650879\n",
      "7.371086120605469\n",
      "7.3482346534729\n",
      "7.294260025024414\n",
      "7.3754777908325195\n",
      "7.160839080810547\n",
      "6.922542572021484\n",
      "7.158517837524414\n",
      "7.064814567565918\n",
      "7.0894389152526855\n",
      "7.0702056884765625\n",
      "7.1468095779418945\n",
      "6.865668773651123\n",
      "7.0147929191589355\n",
      "7.251383304595947\n",
      "7.149494171142578\n",
      "6.998462200164795\n",
      "6.846889495849609\n",
      "7.114254951477051\n",
      "7.135128498077393\n",
      "7.044966697692871\n",
      "6.984813690185547\n",
      "7.153012275695801\n",
      "7.160926818847656\n",
      "7.101161956787109\n",
      "7.219525337219238\n",
      "7.07187032699585\n",
      "7.2702107429504395\n",
      "7.155735015869141\n",
      "7.193210601806641\n",
      "7.191769599914551\n",
      "6.970879077911377\n",
      "7.3527445793151855\n",
      "7.175943374633789\n",
      "7.010007858276367\n",
      "6.951409816741943\n",
      "7.337479591369629\n",
      "7.256285190582275\n",
      "7.001264572143555\n",
      "6.979208946228027\n",
      "6.8229660987854\n",
      "6.855779647827148\n",
      "6.909367561340332\n",
      "7.073141574859619\n",
      "7.0380024909973145\n",
      "6.9456586837768555\n",
      "6.84073543548584\n",
      "7.0199971199035645\n",
      "7.048086166381836\n",
      "6.82796573638916\n",
      "6.9399733543396\n",
      "7.001533031463623\n",
      "7.025211334228516\n",
      "6.883087158203125\n",
      "6.916156768798828\n",
      "6.853969573974609\n",
      "7.136333465576172\n",
      "7.082283973693848\n",
      "6.93002986907959\n",
      "6.932433128356934\n",
      "6.911411762237549\n",
      "7.012411117553711\n",
      "6.851583957672119\n",
      "7.0163798332214355\n",
      "7.011246204376221\n",
      "7.1149396896362305\n",
      "6.912867546081543\n",
      "6.870896816253662\n",
      "6.810091972351074\n",
      "6.916047096252441\n",
      "7.076401710510254\n",
      "6.773933410644531\n",
      "6.817269325256348\n",
      "7.036711692810059\n",
      "7.069643020629883\n",
      "7.302422046661377\n",
      "7.26617956161499\n",
      "7.1915154457092285\n",
      "7.108553886413574\n",
      "7.253406524658203\n",
      "6.873315811157227\n",
      "7.228485107421875\n",
      "6.948520660400391\n",
      "6.830934524536133\n",
      "6.896960735321045\n",
      "6.983920097351074\n",
      "6.918160915374756\n",
      "7.037167549133301\n",
      "6.9557976722717285\n",
      "6.930817127227783\n",
      "7.211132049560547\n",
      "7.040146827697754\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_70026/1038016598.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Run Training ===\u001b[39;00m\n\u001b[32m      2\u001b[39m num_epochs = \u001b[32m10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;28;01min\u001b[39;00m range(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     train_loss = train_epoch()\n\u001b[32m      5\u001b[39m     print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")\n",
      "\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/ipykernel_70026/4279361606.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     31\u001b[39m     total = \u001b[32m0.0\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m xb, yb, zb \u001b[38;5;28;01min\u001b[39;00m train_loader:\n\u001b[32m     33\u001b[39m         \u001b[38;5;66;03m# xb: (B, T), yb: (B, T), zb: (Z_idx, Z_p) with shapes (B,T,K)\u001b[39;00m\n\u001b[32m     34\u001b[39m         logits, loss = model(xb, \u001b[38;5;28;01mNone\u001b[39;00m,zb)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         loss.backward()\n\u001b[32m     36\u001b[39m         torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     37\u001b[39m         optimizer.step()\n\u001b[32m     38\u001b[39m         optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    643\u001b[39m                 retain_graph=retain_graph,\n\u001b[32m    644\u001b[39m                 create_graph=create_graph,\n\u001b[32m    645\u001b[39m                 inputs=inputs,\n\u001b[32m    646\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m         torch.autograd.backward(\n\u001b[32m    648\u001b[39m             self, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    649\u001b[39m         )\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    350\u001b[39m \n\u001b[32m    351\u001b[39m     \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m     \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m     \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     _engine_run_backward(\n\u001b[32m    355\u001b[39m         tensors,\n\u001b[32m    356\u001b[39m         grad_tensors_,\n\u001b[32m    357\u001b[39m         retain_graph,\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    829\u001b[39m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[32m    830\u001b[39m             t_outputs, *args, **kwargs\n\u001b[32m    831\u001b[39m         )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n\u001b[32m    834\u001b[39m             unregister_hooks()  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/function.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    307\u001b[39m                 \u001b[33m\"Function is not allowed. You should only implement one \"\u001b[39m\n\u001b[32m    308\u001b[39m                 \u001b[33m\"of them.\"\u001b[39m\n\u001b[32m    309\u001b[39m             )\n\u001b[32m    310\u001b[39m         user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m user_fn(self, *args)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(ctx, *flat_args)\u001b[39m\n\u001b[32m   2255\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m needs_grad:\n\u001b[32m   2256\u001b[39m                     \u001b[38;5;66;03m# double backward\u001b[39;00m\n\u001b[32m   2257\u001b[39m                     \u001b[38;5;28;01mreturn\u001b[39;00m CompiledFunction._double_backward(ctx, impl_fn, all_args)\n\u001b[32m   2258\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m                     \u001b[38;5;28;01mreturn\u001b[39;00m impl_fn()\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(double_ctx)\u001b[39m\n\u001b[32m   2244\u001b[39m                 \u001b[38;5;28;01mdef\u001b[39;00m impl_fn(double_ctx=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m                     out = CompiledFunction._backward_impl(ctx, all_args)\n\u001b[32m   2246\u001b[39m                     return _backward_epilogue_functional(\n\u001b[32m   2247\u001b[39m                         CompiledFunction.metadata,\n\u001b[32m   2248\u001b[39m                         CompiledFunction.maybe_subclass_metadata,\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(ctx, all_args)\u001b[39m\n\u001b[32m   2372\u001b[39m                             \u001b[33m\"donated buffer.\"\u001b[39m\n\u001b[32m   2373\u001b[39m                         ),\n\u001b[32m   2374\u001b[39m                     )\n\u001b[32m   2375\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m2376\u001b[39m                 out = call_func_at_runtime_with_args(\n\u001b[32m   2377\u001b[39m                     CompiledFunction.compiled_bw,\n\u001b[32m   2378\u001b[39m                     all_args,\n\u001b[32m   2379\u001b[39m                     steal_args=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(f, args, steal_args, disable_amp)\u001b[39m\n\u001b[32m    122\u001b[39m \n\u001b[32m    123\u001b[39m     context = torch._C._DisableAutocast \u001b[38;5;28;01mif\u001b[39;00m disable_amp \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m    125\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m getattr(f, \u001b[33m\"_boxed_call\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m             out = normalize_as_list(f(args))\n\u001b[32m    127\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    128\u001b[39m             \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[32m    129\u001b[39m             \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    929\u001b[39m                     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    930\u001b[39m                 \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    931\u001b[39m                     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    932\u001b[39m             \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m                 _maybe_set_eval_frame(prior)\n",
      "\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/_inductor/output_code.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    584\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.current_callable(inputs)\n\u001b[32m    585\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    586\u001b[39m             get_runtime_metrics_context().finish()\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m             AutotuneCacheBundler.end_compile()\n",
      "\u001b[32m/var/folders/7_/98nk8q255lxf2kbxm92j37rh0000gn/T/torchinductor_joshuahkuttenkuler/hz/chzzsh5imaahwmsjapesqlnwakbesvyqhmv2gluopimd5vilzpo7.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m   9616\u001b[39m     extern_kernels.bmm(permute_213, reinterpret_tensor(buf310, (\u001b[32m64\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m16\u001b[39m), (\u001b[32m16384\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m0\u001b[39m), out=buf311)\n\u001b[32m   9617\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m permute_213\n\u001b[32m   9618\u001b[39m     buf312 = buf219; \u001b[38;5;28;01mdel\u001b[39;00m buf219  \u001b[38;5;66;03m# reuse\u001b[39;00m\n\u001b[32m   9619\u001b[39m     \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [], Original ATen: [aten.bmm]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m9620\u001b[39m     extern_kernels.bmm(reinterpret_tensor(buf310, (\u001b[32m64\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m16\u001b[39m), (\u001b[32m16384\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m1\u001b[39m), \u001b[32m0\u001b[39m), permute_214, out=buf312)\n\u001b[32m   9621\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m permute_214\n\u001b[32m   9622\u001b[39m     buf313 = empty_strided_cpu((\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m1024\u001b[39m), (\u001b[32m8388608\u001b[39m, \u001b[32m1048576\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m1\u001b[39m), torch.float32)\n\u001b[32m   9623\u001b[39m     buf315 = empty_strided_cpu((\u001b[32m8\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m8192\u001b[39m, \u001b[32m1024\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m65536\u001b[39m), torch.float32)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "36c6dd9b-dafc-4c7e-94e9-fde8dd10f186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x121e17680>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fe76faa0-3933-416d-b923-993c67cfcf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Y byshich yorld Set,\n",
      "Berett fanctizens;\n",
      "My pree to me pray! be should the would liel's tree\n",
      "No mane ful that the in take\n",
      "This do deatens, gently noted theres?\n",
      "\n",
      "GRUMERMIO Come myse had a pure this good I saint of in to to but 'ime and hearcher seek goatetify name,\n",
      "My Bolight\n",
      "Than otherly the poor the to prise they any not\n",
      "To their'd say\n",
      "such: thunk please,\n",
      "For mers you one do read That Caprain,\n",
      "To sina I knecks whathosolion's bed:ign it brothing,\n",
      "As teard slaberhip, introling\n",
      "Teak ny brings\n",
      "Morse are your M\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "def decode_sequence_char(\n",
    "    model, stoi, itos, prompt, max_new_tokens=100, block_size=256,\n",
    "    use_fenchel=False, tau=1.0, fenchel_iters=3, temperature=1.0\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    max_ctx = int(block_size)\n",
    "\n",
    "    # ?1 assume space token exists; fallback to 0 if missing\n",
    "    space_id = stoi.get(' ', 0)\n",
    "\n",
    "    # encode prompt\n",
    "    start_ids = torch.tensor([encode_chars(prompt, stoi)], dtype=torch.long, device=device)\n",
    "\n",
    "    # prepend the pad once; from now on the window just slides\n",
    "\n",
    "    idx = start_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = idx[:, -max_ctx:]  # rotating buffer: last pad_len+block_size tokens\n",
    "        logits, _ = model(context, None)\n",
    "        last_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    # drop the initial pad when returning the string\n",
    "    out_tokens = idx[0].tolist()\n",
    "    return decode_chars(out_tokens, itos)\n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "\n",
    "prompt = \"ROMEO:\"\n",
    "generated = decode_sequence_char(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=512,\n",
    "    block_size=1024,\n",
    "    use_fenchel=False,\n",
    "    tau=1.5,\n",
    "    fenchel_iters=2,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9b2490-4fda-4d68-9385-6df58b9d06d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5766b-6947-4fbe-8704-8ea7c19c5afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37109166-b414-4e08-964d-be55048c9d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef652df0-c07e-4e14-8180-0cc75de9863e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60132cb-cf65-49c7-a481-c7a29c0fdf60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc7743d-0569-49ca-a2eb-a0d971455f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa366c-2133-4865-a7a4-ebce14ac2533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4eaa6-9fea-464f-92dc-0036a4ece7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89689e67-ff91-4fc0-a0e4-79e4330d4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"fuzzy.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72509ed-bff1-43e2-a08a-4aad880fb293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
