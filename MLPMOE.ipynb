{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7741bdf-98ab-4400-a0d1-4b831244ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "# --- GPT with auxiliary reverse-embedding loss from zb ---\n",
    "import math, inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# assumes Block and LayerNorm are defined elsewhere (as in your current setup)\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig,\n",
    "                 aux_scale: float = 1.0,\n",
    "                 noise_constituent: float = 1e-4,\n",
    "                 noise_final: float = 1e-4):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None and config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        # per-block orthonormal maps for blocks 0..3\n",
    "        need_blocks = 4\n",
    "        if config.n_layer < need_blocks:\n",
    "            raise ValueError(f\"need at least {need_blocks} transformer blocks for aux; got {config.n_layer}\")\n",
    "        self.aux_blocks = list(range(need_blocks))  # [0,1,2,3] fixed\n",
    "        self.aux_maps = nn.ModuleList()\n",
    "        for _ in self.aux_blocks:\n",
    "            lin = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            nn.init.orthogonal_(lin.weight)  # square => orthonormal rows & columns\n",
    "            self.aux_maps.append(lin)\n",
    "\n",
    "        # noise/scales\n",
    "        self.aux_scale_default = float(aux_scale)\n",
    "        self.noise_constituent = float(noise_constituent)\n",
    "        self.noise_final = float(noise_final)\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # reverse-embedding for one lane (list length T of [idxs, probs])\n",
    "    def _rev_embed_lane(self, lane_seq, device):\n",
    "        T = len(lane_seq)\n",
    "        if T == 0:\n",
    "            return torch.empty(0, self.config.n_embd, device=device)\n",
    "        idxs = torch.tensor([pair[0] for pair in lane_seq], device=device, dtype=torch.long)      # (T, K)\n",
    "        probs = torch.tensor([pair[1] for pair in lane_seq], device=device, dtype=torch.float32)  # (T, K)\n",
    "        probs = probs / (probs.sum(dim=-1, keepdim=True) + 1e-12)\n",
    "\n",
    "        E = self.transformer.wte.weight  # (V, D)\n",
    "        emb = E.index_select(0, idxs.reshape(-1)).reshape(*idxs.shape, E.size(1))  # (T, K, D)\n",
    "        if self.noise_constituent > 0:\n",
    "            emb = emb + torch.randn_like(emb) * self.noise_constituent\n",
    "        rev = torch.einsum('tkd,tk->td', emb, probs)  # (T, D)\n",
    "        if self.noise_final > 0:\n",
    "            rev = rev + torch.randn_like(rev) * self.noise_final\n",
    "        return rev\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 66\n",
    "    n_layer: int = 4          # explicitly 4 per your instruction\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "\n",
    "    # -------- reverse-embedding helpers (batchified, same dtype/device as x) --------\n",
    "def soft_ce(logits: torch.Tensor, target_probs: torch.Tensor, ignore_mask =None):\n",
    "        \"\"\"\n",
    "        logits: (B, T, V)\n",
    "        target_probs: (B, T, V) row-normalized\n",
    "        ignore_mask: (B, T) bool, True where we KEEP, False to ignore\n",
    "        \"\"\"\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        loss = -(target_probs * logp).sum(dim=-1)  # (B, T)\n",
    "        if ignore_mask is not None:\n",
    "            loss = loss * ignore_mask\n",
    "            denom = torch.clamp(ignore_mask.sum(), min=1)\n",
    "            return loss.sum() / denom\n",
    "        else:\n",
    "            return loss.mean()\n",
    "\n",
    "def sharpen_distribution(idx: torch.Tensor, p: torch.Tensor, V: int, alpha: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: (B, T, K) long\n",
    "        p:   (B, T, K) float\n",
    "        return dense (B, T, V) probs with sharpening exponent alpha (alpha>1 => more peaked)\n",
    "        \"\"\"\n",
    "        B, T, K = idx.shape\n",
    "        out = torch.full((B, T, V), 0.0, dtype=p.dtype, device=p.device)\n",
    "        # apply exponent (temperature-like). alpha==1 means unchanged\n",
    "        q = torch.clamp(p, min=1e-12) ** alpha\n",
    "        q = q / q.sum(dim=-1, keepdim=True)\n",
    "        out.scatter_add_(dim=-1, index=idx, src=q)\n",
    "        return out\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class StochasticEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    A swap-in replacement for nn.Embedding that sometimes \"smudges\" inputs by\n",
    "    replacing a hard index lookup with a small weighted mixture over up to K tokens.\n",
    "\n",
    "    Key behavior:\n",
    "    - With probability `mix_prob` (per position), use a mixture; otherwise do a standard idx->embedding.\n",
    "    - The mixture *always* includes the original idx and keeps it as the most likely token.\n",
    "    - The remaining mixture members (up to `max_blend`) are:\n",
    "        * If Z is provided: taken from the previous position's Z-index candidates, with randomized sharpening.\n",
    "        * Else: random distinct tokens (excluding idx).\n",
    "    - Weights are *uneven* and decay gradually (geometric), then normalized.\n",
    "    - Position t=0 is always pure (hard idx).\n",
    "    - During eval mode, mixing is disabled by default (can be enabled with `enable_in_eval=True`).\n",
    "\n",
    "    Expected Z format (optional):\n",
    "        z_prev = (Z_idx, Z_p)\n",
    "        shapes: Z_idx: (B, T, Kz), Z_p: (B, T, Kz)\n",
    "        For input at time t, we *key* into Z at t-1 (previous position).\n",
    "        If Z is provided for the whole sequence, you can pass it directly and this layer\n",
    "        will handle the t-1 alignment internally.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings: vocab size\n",
    "        embedding_dim:  embedding size\n",
    "        mix_prob:       probability to use a mixture at a given (b, t) position\n",
    "        max_blend:      up to this many *additional* indices beyond `idx` (<=16 as requested)\n",
    "        alpha_range:    (low, high) range for random sharpening exponent applied to Z probs\n",
    "        geom_decay_range: range for random geometric ratio r in (low, high) to create uneven, decaying weights\n",
    "        ensure_idx_top_margin: multiplicative margin to ensure idx remains strictly the top prob\n",
    "        enable_in_eval: if True, still perform mixing during model.eval(); else mixing only in training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        mix_prob: float = 0.3,\n",
    "        max_blend: int = 16,\n",
    "        alpha_range: Tuple[float, float] = (0.8, 2.0),\n",
    "        geom_decay_range: Tuple[float, float] = (0.55, 0.9),\n",
    "        ensure_idx_top_margin: float = 1.05,\n",
    "        enable_in_eval: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(num_embeddings, embedding_dim, **kwargs)\n",
    "        if max_blend < 0 or max_blend > 16:\n",
    "            raise ValueError(\"max_blend must be in [0, 16].\")\n",
    "        self.mix_prob = float(mix_prob)\n",
    "        self.max_blend = int(max_blend)\n",
    "        self.alpha_range = alpha_range\n",
    "        self.geom_decay_range = geom_decay_range\n",
    "        self.ensure_idx_top_margin = float(ensure_idx_top_margin)\n",
    "        self.enable_in_eval = bool(enable_in_eval)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _rand_geom_weights(self, k: int, device: torch.device, dtype: torch.dtype) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create a strictly decreasing, uneven weight vector of length k via a random geometric ratio.\n",
    "        Returns a normalized vector (sums to 1).\n",
    "        \"\"\"\n",
    "        r = torch.empty(1, device=device, dtype=dtype).uniform_(*self.geom_decay_range).item()  # scalar ratio\n",
    "        # weights ~ [1, r, r^2, ...]\n",
    "        exps = torch.arange(k, device=device, dtype=dtype)\n",
    "        w = torch.pow(torch.tensor(r, device=device, dtype=dtype), exps)\n",
    "        w = w / (w.sum() + 1e-12)\n",
    "        return w\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _apply_sharpening(self, probs: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sharpen/widen probabilities: q_i ‚àù p_i^alpha. alpha>1 => more peaked, alpha<1 => flatter.\n",
    "        \"\"\"\n",
    "        q = torch.clamp(probs, min=1e-12) ** alpha\n",
    "        q = q / (q.sum() + 1e-12)\n",
    "        return q\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        idx: torch.Tensor,\n",
    "        z_prev: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # (Z_idx, Z_p), each (B,T,Kz)\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: (B, T) Long\n",
    "        z_prev: optional tuple (Z_idx, Z_p), both shaped (B, T, Kz).\n",
    "                For current position t, we look at z_prev[:, t-1] (previous position).\n",
    "        Returns:\n",
    "            embeddings: (B, T, D)\n",
    "        \"\"\"\n",
    "        if idx.dtype != torch.long:\n",
    "            raise TypeError(\"idx must be torch.long\")\n",
    "\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "        D = self.embedding_dim\n",
    "        E = self.weight  # (V, D)\n",
    "\n",
    "        # Early exit if mixing disabled (e.g., eval mode and not enabled)\n",
    "        mixing_active = self.training or self.enable_in_eval\n",
    "        if not mixing_active or self.mix_prob <= 0.0 or self.max_blend == 0:\n",
    "            # Just standard embedding, except we always keep t=0 pure anyway (same outcome here).\n",
    "            return super().forward(idx)\n",
    "\n",
    "        # We will construct output one position group at a time for clarity.\n",
    "        out = torch.empty(B, T, D, device=device, dtype=E.dtype)\n",
    "\n",
    "        # Sample where to mix (Bernoulli per token position), but force t==0 to False.\n",
    "        mix_mask = torch.rand(B, T, device=device) < self.mix_prob\n",
    "        if T > 0:\n",
    "            mix_mask[:, 0] = False  # first element always x (pure)\n",
    "        if model.training == False:\n",
    "            mix_mask[:,:]= False#always pure when eval\n",
    "\n",
    "        # Helper to do a pure lookup fast:\n",
    "        if (~mix_mask).any():\n",
    "            out[~mix_mask] = E[idx[~mix_mask]]\n",
    "\n",
    "        # If we have no positions to mix, we are done.\n",
    "        if not mix_mask.any():\n",
    "            return out\n",
    "\n",
    "        # If Z is provided, we will use previous position (t-1) candidates to key the mixture.\n",
    "        have_z = z_prev is not None\n",
    "        if have_z:\n",
    "            Z_idx, Z_p = z_prev\n",
    "            assert Z_idx.shape[:2] == (B, T) and Z_p.shape[:2] == (B, T), \"Z shapes must be (B,T, Kz)\"\n",
    "            assert Z_idx.shape == Z_p.shape, \"Z_idx and Z_p must have identical shapes\"\n",
    "            # We'll access Z at t-1 for each position t; t=0 is never mixed per mask anyway.\n",
    "\n",
    "        # Process mixing positions, possibly in a light loop (readability > micro-optimizations)\n",
    "        mix_positions = mix_mask.nonzero(as_tuple=False)  # list of (b, t) rows\n",
    "        V = E.size(0)\n",
    "\n",
    "        for b, t in mix_positions.tolist():\n",
    "            cur_idx = idx[b, t].item()\n",
    "\n",
    "            # Build candidate set: always include cur_idx first\n",
    "            cand = [cur_idx]\n",
    "            cand_weights = None  # will be set later (unnormalized)\n",
    "\n",
    "            if have_z and t > 0:\n",
    "                # Use Z keyed from previous position\n",
    "                z_i = Z_idx[b, t - 1]  # (Kz,)\n",
    "                z_p = Z_p[b, t - 1]    # (Kz,)\n",
    "\n",
    "                # Randomly sharpen/widen Z\n",
    "                alpha = float(torch.empty(1, device=device).uniform_(*self.alpha_range).item())\n",
    "                z_q = self._apply_sharpening(z_p.to(torch.float32), alpha)  # (Kz,)\n",
    "\n",
    "                # Take top candidates from z (excluding cur_idx), up to max_blend\n",
    "                # We'll sort by z_q descending to preserve a \"gradual reduction\" feel.\n",
    "                top_vals, top_idx = torch.topk(z_q, k=min(self.max_blend + 4, z_q.numel()))  # oversample a bit\n",
    "                z_cands = []\n",
    "                for j in top_idx.tolist():\n",
    "                    tok = int(z_i[j].item())\n",
    "                    if tok != cur_idx and tok not in z_cands:\n",
    "                        z_cands.append(tok)\n",
    "                        if len(z_cands) >= self.max_blend:\n",
    "                            break\n",
    "\n",
    "                # If Z didn't yield enough distincts (rare), fill randomly\n",
    "                need = self.max_blend - len(z_cands)\n",
    "                if need > 0:\n",
    "                    # sample without replacement, excluding cur_idx and z_cands\n",
    "                    exclude = set([cur_idx] + z_cands)\n",
    "                    # Draw a bit more than needed and filter\n",
    "                    rand_pool = torch.randperm(V, device=device)[: max(need * 4, need + 8)].tolist()\n",
    "                    for tok in rand_pool:\n",
    "                        if tok not in exclude:\n",
    "                            z_cands.append(tok)\n",
    "                            exclude.add(tok)\n",
    "                            if len(z_cands) >= self.max_blend:\n",
    "                                break\n",
    "\n",
    "                cand.extend(z_cands)\n",
    "\n",
    "                # Create uneven, decaying weights aligned with sorted z_q order\n",
    "                # Start from geometric decay, then bias by z_q ranks a bit\n",
    "                k_total = len(cand)\n",
    "                geom_w = self._rand_geom_weights(k_total, device=device, dtype=torch.float32)\n",
    "\n",
    "                # Modulate the tail (excluding idx at position 0) by the (sorted) z_q values we used\n",
    "                # to keep a Z-keyed shape. Map z_cands in the order we picked them (already roughly top->down).\n",
    "                tail = geom_w[1:].clone()\n",
    "                if len(z_cands) > 0:\n",
    "                    # z shape weights based on normalized top_vals (already top->down)\n",
    "                    zshape = top_vals[: len(z_cands)].to(torch.float32)\n",
    "                    zshape = zshape / (zshape.sum() + 1e-12)\n",
    "                    # mix geometric decay with zshape to keep unevenness and Z-key\n",
    "                    tail = 0.5 * tail + 0.5 * (zshape * tail.sum())  # keep total tail mass\n",
    "\n",
    "                # Reassemble, ensure idx (slot 0) is strongest\n",
    "                cand_weights = torch.cat([geom_w[0:1], tail], dim=0)\n",
    "\n",
    "            else:\n",
    "                # No Z available (or t==0, but t==0 never mixes). Fall back to random candidates with decaying weights.\n",
    "                # Sample distinct random tokens excluding cur_idx\n",
    "                need = self.max_blend\n",
    "                exclude = {cur_idx}\n",
    "                rand_cands = []\n",
    "                # Draw a bit more than needed for uniqueness\n",
    "                for tok in torch.randperm(V, device=device)[: max(need * 4, need + 8)].tolist():\n",
    "                    if tok not in exclude:\n",
    "                        rand_cands.append(tok)\n",
    "                        exclude.add(tok)\n",
    "                        if len(rand_cands) >= need:\n",
    "                            break\n",
    "                cand.extend(rand_cands)\n",
    "                # Geometric decay weights (uneven)\n",
    "                cand_weights = self._rand_geom_weights(len(cand), device=device, dtype=torch.float32)\n",
    "\n",
    "            # Make sure the first element (the true idx) has the HIGHEST probability\n",
    "            # by granting it a small multiplicative margin, then renormalize.\n",
    "            cand_weights = cand_weights.clone()\n",
    "            cand_weights[0] = cand_weights[0] * self.ensure_idx_top_margin\n",
    "            cand_weights = cand_weights / (cand_weights.sum() + 1e-12)\n",
    "\n",
    "            # Turn the mixture into an embedding: sum_j p_j * E[cand_j]\n",
    "            cand_t = torch.tensor(cand, device=device, dtype=torch.long)\n",
    "            vecs = E.index_select(0, cand_t)           # (k, D)\n",
    "            probs = cand_weights.view(-1, 1).to(vecs)  # (k, 1)\n",
    "            mixed = (vecs * probs).sum(dim=0)          # (D,)\n",
    "            out[b, t] = mixed\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig,\n",
    "                 aux_scale: float = 1.0,           # (12) fixed\n",
    "                 noise_constituent: float = 1e-6,    # (7) fixed\n",
    "                 noise_final: float = 1e-4):         # (7) fixed\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None and config.block_size is not None\n",
    "\n",
    "        self.config = config\n",
    "        self.aux_scale = float(aux_scale)\n",
    "        self.noise_constituent = float(noise_constituent)\n",
    "        self.noise_final = float(noise_final)\n",
    "\n",
    "        # core transformer\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = StochasticEmbedding(\n",
    "            config.vocab_size, config.n_embd,\n",
    "            mix_prob=0.3,          # tune\n",
    "            max_blend=16,          # per your cap\n",
    "            alpha_range=(0.8, 2.0) # gentle->sharp randomization\n",
    "        ),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        #self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        # per-block orthonormal linears (square D√óD, columns orthonormal)\n",
    "        self.aux_maps = nn.ModuleList()\n",
    "        for _ in range(4):\n",
    "            lin = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            nn.init.orthogonal_(lin.weight)\n",
    "            self.aux_maps.append(lin)\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # --- Patch your GPT class: replace forward with the following ---\n",
    "    def forward(self, idx, targets=None, zb=None,\n",
    "                               aux_scale: float = 1.0,\n",
    "                               depth_alphas  = None,\n",
    "                               warmup_ignores  = None):\n",
    "        \"\"\"\n",
    "        idx: (B, T) Long\n",
    "        targets: (B, T) Long or None\n",
    "        zb: tuple (Z_idx, Z_p) each (B, T, K); single Z shared across blocks\n",
    "        depth_alphas: per-block sharpening exponents (len == n_layer). e.g. [0.8, 1.0, 1.5, 2.0, ...]\n",
    "                      alpha<1 widens early, >1 sharpens later ‚Äî approximates your 'gaussian‚Üípeaked student-t'\n",
    "        warmup_ignores: per-block number of initial positions to ignore for aux\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size\n",
    "    \n",
    "        # embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "        tok_emb = self.transformer.wte(idx, z_prev=zb)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "    \n",
    "        # defaults\n",
    "        L = len(self.transformer.h)\n",
    "        if depth_alphas is None:\n",
    "            # gentle ‚Üí sharp\n",
    "            depth_alphas = [0.8 + 1.2 * (i/(L-1)) for i in range(L)] if L > 1 else [1.0]\n",
    "        if warmup_ignores is None:\n",
    "            # ignore more in shallow blocks\n",
    "            warmup_ignores = [min(2**i - 1, T-1) for i in range(L)]  # 0,1,3,7,... capped\n",
    "    \n",
    "        # unpack Z\n",
    "        Z_idx, Z_p = zb if zb is not None else (None, None)\n",
    "    \n",
    "        aux_loss = None\n",
    "        for bidx, block in enumerate(self.transformer.h):\n",
    "            x = block(x)  # (B, T, D)\n",
    "            if Z_idx is not None and Z_p is not None:\n",
    "                V = self.lm_head.out_features\n",
    "                logits_b = self.lm_head(self.transformer.ln_f(x))  # (B, T, V)\n",
    "    \n",
    "                # per-depth skew of the SAME base Z\n",
    "                Z_dense = sharpen_distribution(Z_idx, Z_p, V, alpha=float(depth_alphas[bidx]))\n",
    "    \n",
    "                # warmup ignore mask (keep positions >= ignore)\n",
    "                keep = torch.arange(T, device=device).expand(B, T) >= int(warmup_ignores[bidx])\n",
    "                keep = keep.to(logits_b.dtype)\n",
    "                aux_b = soft_ce(logits_b, Z_dense, ignore_mask=(keep>0))\n",
    "                aux_loss = aux_b if aux_loss is None else aux_loss + aux_b\n",
    "    \n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "    \n",
    "        ce_loss = None\n",
    "        if targets is not None:\n",
    "            ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "                                      targets.view(-1), ignore_index=-100)\n",
    "    \n",
    "        total = None\n",
    "        if ce_loss is None and aux_loss is None:\n",
    "            total = None\n",
    "        elif aux_loss is None:\n",
    "            total = ce_loss\n",
    "        elif ce_loss is None:\n",
    "            total = aux_scale * aux_loss\n",
    "        else:\n",
    "            total = ce_loss + aux_scale * aux_loss\n",
    "    \n",
    "        if targets is None:\n",
    "            logits = logits[:, [-1], :]\n",
    "    \n",
    "        return logits, total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e1f1c-1f54-43b7-975c-4229c4ae1866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d76ace-c3c1-4a68-adce-851c842dbbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading aochildes.txt...\n",
      "üì• Downloading cbt.txt...\n",
      "üì• Downloading children_stories.txt...\n",
      "üì• Downloading gutenberg.txt...\n",
      "üì• Downloading qed.txt...\n",
      "üì• Downloading simple_wikipedia.txt...\n",
      "üì• Downloading switchboard.txt...\n",
      "üì• Downloading wikipedia.txt...\n",
      "üì• Downloading shakespeare.txt...\n",
      "‚úÖ Done. Files saved to ./babylm_10m_cleaned\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"üì• Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"‚úÖ Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95a695ab-753c-4b35-8834-a1d4f59859bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Char tokenizer finalized.\n",
      "üßæ Train tokens: 1016242 | Val tokens: 99152\n",
      "üî§ Vocab size: 66\n",
      "Loaded 1016242 train tokens and 99152 val tokens | vocab=66\n",
      "Building order-2 Markov...\n",
      "Building order-4 Markov...\n",
      "Building order-8 Markov...\n",
      "Building order-16 Markov...\n",
      "Building order-32 Markov...\n",
      "Building order-64 Markov...\n",
      "Building bigram db...\n",
      "‚úÖ Markov and Bigram models saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Char tokenizer finalized.\")\n",
    "print(f\"üßæ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"üî§ Vocab size: {len(stoi)}\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "\n",
    "data_dir = \"./babylm_char_tokenized\"\n",
    "train_path = os.path.join(data_dir, \"train.bin\")\n",
    "val_path   = os.path.join(data_dir, \"val.bin\")\n",
    "meta_path  = os.path.join(data_dir, \"meta.pkl\")\n",
    "train_ids = np.fromfile(train_path, dtype=np.uint16)\n",
    "val_ids   = np.fromfile(val_path,   dtype=np.uint16)\n",
    "\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "vocab_size = meta[\"vocab_size\"]\n",
    "\n",
    "def global_freqs(ids: np.ndarray, V: int) -> np.ndarray:\n",
    "    cnt = np.bincount(ids.astype(np.int64), minlength=V)\n",
    "    # normalize to probability (avoid zero)\n",
    "    p = cnt.astype(np.float64)\n",
    "    p = p / max(1.0, p.sum())\n",
    "    return p\n",
    "print(f\"Loaded {len(train_ids)} train tokens and {len(val_ids)} val tokens | vocab={vocab_size}\")\n",
    "p_global = global_freqs(train_ids, vocab_size)  # used for disciplined fill only\n",
    "\n",
    "def build_markov_chain(data: np.ndarray, window: int) -> Dict[Tuple[int, ...], Counter]:\n",
    "    chain = defaultdict(Counter)\n",
    "    for i in range(len(data) - window):\n",
    "        ctx = tuple(map(int, data[i:i+window]))\n",
    "        nxt = int(data[i+window])\n",
    "        chain[ctx][nxt] += 1\n",
    "    return chain\n",
    "\n",
    "ngram_orders = [2,4,8,16,32,64]\n",
    "markov_models: Dict[int, Dict[Tuple[int,...], Counter]] = {}\n",
    "for w in ngram_orders:\n",
    "    print(f\"Building order-{w} Markov...\")\n",
    "    markov_models[w] = build_markov_chain(train_ids, w)\n",
    "\n",
    "def build_bigram_db(data: np.ndarray, V: int, top_k=16, epsilon=1e-6, seed=1337):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    counts = np.zeros((V, V), dtype=np.int64)\n",
    "    a = data[:-1].astype(np.int64)\n",
    "    b = data[1:].astype(np.int64)\n",
    "    np.add.at(counts, (a, b), 1)\n",
    "    out = {}\n",
    "    all_ids = np.arange(V, dtype=np.int64)\n",
    "    for t in range(V):\n",
    "        row = counts[t]\n",
    "        tot = row.sum()\n",
    "        if tot == 0:\n",
    "            idx = rng.choice(V, size=top_k, replace=False)\n",
    "            p = np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "        else:\n",
    "            pr = row.astype(np.float64) / float(tot)\n",
    "            obs = np.flatnonzero(row)\n",
    "            if len(obs) >= top_k:\n",
    "                sel = np.argpartition(pr[obs], -top_k)[-top_k:]\n",
    "                idx = obs[sel]\n",
    "                p = pr[idx].astype(np.float32)\n",
    "                s = p.sum()\n",
    "                p = p/s if s > 0 else np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "            else:\n",
    "                need = top_k - len(obs)\n",
    "                mask = np.ones(V, dtype=bool); mask[obs] = False\n",
    "                extra = np.random.default_rng(seed+t).choice(np.nonzero(mask)[0], size=need, replace=False)\n",
    "                idx = np.concatenate([obs, extra])\n",
    "                p   = pr[idx].astype(np.float32)\n",
    "                # give epsilon to never-seen extras\n",
    "                unseen = (row[idx] == 0)\n",
    "                if unseen.any():\n",
    "                    p = p + unseen.astype(np.float32) * epsilon\n",
    "                p = p / p.sum()\n",
    "        order = np.argsort(-p)\n",
    "        out[t] = (idx[order].astype(np.int64), p[order])\n",
    "    return out\n",
    "\n",
    "print(\"Building bigram db...\")\n",
    "bigram_db = build_bigram_db(train_ids, vocab_size, top_k=64)  # collect a bit wider; we'll cap later\n",
    "\n",
    "# === Save ===\n",
    "model_dir = \"./markov_bigram_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(model_dir, \"markov_models.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(markov_models, f)\n",
    "\n",
    "with open(os.path.join(model_dir, \"bigram_db.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(bigram_db, f)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Markov and Bigram models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcae90-1cdc-4ef5-8907-75961e9a5ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57eff45-c482-4e59-ace0-5c35e65c879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd4dd64a-8f8e-471e-97f2-7ffd1df4c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "# === Replacement dataloader that uses SAVED bigram + markov models and yields (X, Y, Z) ===\n",
    "import os, pickle, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# expects `vocab_size` and `device` already defined in the outer scope\n",
    "# expects saved models at ./markov_bigram_models/{bigram_db.pkl, markov_models.pkl}\n",
    "\n",
    "class DisciplinedZ:\n",
    "    def __init__(self, markov_models: Dict[int, Dict[Tuple[int,...], Counter]],\n",
    "                 bigram_db: Dict[int, Tuple[np.ndarray, np.ndarray]],\n",
    "                 p_global: np.ndarray,\n",
    "                 vocab_size: int,\n",
    "                 top_k: int = 32,\n",
    "                 epsilon: float = 1e-6):\n",
    "        self.models = markov_models\n",
    "        self.bigram_db = bigram_db\n",
    "        self.p_global = p_global.astype(np.float64)\n",
    "        self.V = vocab_size\n",
    "        self.K = top_k\n",
    "        self.eps = float(epsilon)\n",
    "        # global sort for fill\n",
    "        self.global_order = np.argsort(-self.p_global)\n",
    "\n",
    "    def _cands_from_counter(self, ctr: Optional[Counter]) -> Optional[np.ndarray]:\n",
    "        if not ctr:\n",
    "            return None\n",
    "        return np.fromiter((int(t) for t,_ in ctr.items()), dtype=np.int64)\n",
    "\n",
    "    def _probs_from_counter(self, ctr: Optional[Counter]) -> Optional[Dict[int, float]]:\n",
    "        if not ctr:\n",
    "            return None\n",
    "        tot = sum(ctr.values())\n",
    "        if tot == 0:\n",
    "            return None\n",
    "        return {int(t): c/tot for t, c in ctr.items()}\n",
    "\n",
    "    def _bigram_top(self, tok: int, limit: int) -> np.ndarray:\n",
    "        idx, prob = self.bigram_db.get(int(tok), (None, None))\n",
    "        if idx is None:\n",
    "            return np.array([], dtype=np.int64)\n",
    "        return idx[:limit]\n",
    "\n",
    "    def _btree_candidates(self, contexts: Dict[int, Tuple[Tuple[int,...], Optional[Counter]]], backoff_tok: int) -> np.ndarray:\n",
    "        # collect candidate sets from each available context\n",
    "        sets = []\n",
    "        for n, (_, ctr) in contexts.items():\n",
    "            c = self._cands_from_counter(ctr)\n",
    "            if c is not None and c.size > 0:\n",
    "                sets.append(set(c.tolist()))\n",
    "        if len(sets) == 0:\n",
    "            # no ctx ‚Üí use bigram set as starting point\n",
    "            return self._bigram_top(backoff_tok, self.K)\n",
    "\n",
    "        # try full intersection; if empty, progressively intersect strongest contexts first\n",
    "        inter = set.intersection(*sets) if len(sets) > 1 else sets[0]\n",
    "        if len(inter) == 0:\n",
    "            # heuristic: sort by context order (longest first), intersect greedily\n",
    "            sets_sorted = sorted(sets, key=lambda s: -len(s))\n",
    "            inter = sets_sorted[0].copy()\n",
    "            for s in sets_sorted[1:]:\n",
    "                new_inter = inter.intersection(s)\n",
    "                if len(new_inter) > 0:\n",
    "                    inter = new_inter\n",
    "        if len(inter) == 0:\n",
    "            # last resort: union (still disciplined; no random injection)\n",
    "            union = set()\n",
    "            for s in sets:\n",
    "                union |= s\n",
    "            inter = union\n",
    "\n",
    "        arr = np.fromiter(inter, dtype=np.int64)\n",
    "        if arr.size == 0:\n",
    "            return self._bigram_top(backoff_tok, self.K)\n",
    "        return arr\n",
    "\n",
    "    def _score_candidates(self, cands: np.ndarray, contexts: Dict[int, Tuple[Tuple[int,...], Optional[Counter]]]) -> np.ndarray:\n",
    "        # score = sum over contexts of presence * local prob\n",
    "        # local prob from per-context normalized counts\n",
    "        scores = np.zeros(cands.size, dtype=np.float64)\n",
    "        idxmap = {int(t): i for i, t in enumerate(cands)}\n",
    "        for n, (_, ctr) in contexts.items():\n",
    "            probs = self._probs_from_counter(ctr)\n",
    "            if probs is None:\n",
    "                continue\n",
    "            for t, p in probs.items():\n",
    "                if t in idxmap:\n",
    "                    scores[idxmap[t]] += float(p)\n",
    "        # tiny floor to avoid zeros\n",
    "        scores = scores + (scores == 0) * self.eps\n",
    "        return scores\n",
    "\n",
    "    def build_Z_for_sequence(self, seq: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        seq: array of length L = block_size (+optional pad)\n",
    "        returns:\n",
    "          topk_idx: (L, K) int64\n",
    "          topk_p:   (L, K) float32  (row-normalized)\n",
    "        \"\"\"\n",
    "        L = len(seq)\n",
    "        topk_idx = np.zeros((L, self.K), dtype=np.int64)\n",
    "        topk_p   = np.zeros((L, self.K), dtype=np.float32)\n",
    "        for j in range(L):\n",
    "            back_tok = int(seq[j])\n",
    "            # collect contexts\n",
    "            contexts = {}\n",
    "            for n in ngram_orders:\n",
    "                if j - (n-1) < 0:\n",
    "                    continue\n",
    "                ctx = tuple(int(x) for x in seq[j-(n-1):j+1])\n",
    "                ctr = self.models[n].get(ctx, None)\n",
    "                contexts[n] = (ctx, ctr)\n",
    "\n",
    "            # disciplined candidate set\n",
    "            cands = self._btree_candidates(contexts, back_tok)\n",
    "\n",
    "            # cap K by candidate count\n",
    "            if cands.size >= self.K:\n",
    "                # score & take best K\n",
    "                scores = self._score_candidates(cands, contexts)\n",
    "                order = np.argsort(-scores)[:self.K]\n",
    "                idx = cands[order]\n",
    "                sc  = scores[order]\n",
    "            else:\n",
    "                # we must fill with globally-most-common tokens (no randoms), excluding existing\n",
    "                scores = self._score_candidates(cands, contexts) if cands.size > 0 else np.array([], dtype=np.float64)\n",
    "                missing = self.K - cands.size\n",
    "                mask = np.ones(vocab_size, dtype=bool)\n",
    "                mask[cands] = False\n",
    "                fill = []\n",
    "                for t in self.global_order:\n",
    "                    if mask[t]:\n",
    "                        fill.append(int(t))\n",
    "                        if len(fill) == missing:\n",
    "                            break\n",
    "                if cands.size == 0:\n",
    "                    idx = np.array(fill, dtype=np.int64)\n",
    "                    sc  = np.full(len(fill), self.eps, dtype=np.float64)\n",
    "                else:\n",
    "                    idx = np.concatenate([cands, np.array(fill, dtype=np.int64)])\n",
    "                    sc  = np.concatenate([scores, np.full(missing, self.eps, dtype=np.float64)])\n",
    "\n",
    "            # normalize to prob\n",
    "            p = sc.astype(np.float64)\n",
    "            p = p / p.sum() if p.sum() > 0 else np.full_like(p, 1.0/len(p))\n",
    "            topk_idx[j, :] = idx\n",
    "            topk_p[j, :]   = p.astype(np.float32)\n",
    "        return topk_idx, topk_p\n",
    "\n",
    "discZ = DisciplinedZ(markov_models, bigram_db, p_global, vocab_size, top_k=32, epsilon=1e-6)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class GPUDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size: int, batch_size: int, builder: DisciplinedZ, pad_len:int=0, jitter:int=63, p_aligned:float=0.5, seed:int=1337):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = int(block_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.pad_len    = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = max(1, self.total // self.sample_len)\n",
    "        self.jitter = int(jitter)\n",
    "        self.p_aligned = float(p_aligned)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.builder = builder\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def _sample_block(self):\n",
    "        base_block = self.rng.integers(0, self.n_blocks)\n",
    "        start = base_block * self.sample_len\n",
    "        if self.rng.random() > self.p_aligned:\n",
    "            j = self.rng.integers(0, self.jitter + 1)\n",
    "            start = min(start + j, self.total)\n",
    "        return start\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        B, T = self.batch_size, self.block_size\n",
    "        X = np.empty((B, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((B, T), dtype=np.int64)\n",
    "        Z_idx = np.empty((B, T, self.builder.K), dtype=np.int64)\n",
    "        Z_p   = np.empty((B, T, self.builder.K), dtype=np.float32)\n",
    "        for i in range(B):\n",
    "            start = self._sample_block()\n",
    "            xseq = self.data[start : start + self.sample_len].astype(np.int64)\n",
    "            yseq = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + T].astype(np.int64)\n",
    "            X[i] = xseq\n",
    "            Y[i] = yseq\n",
    "            idxs, probs = self.builder.build_Z_for_sequence(xseq[:T])\n",
    "            Z_idx[i] = idxs\n",
    "            Z_p[i]   = probs\n",
    "        # torch tensors\n",
    "        X = torch.from_numpy(X[:, :T]).to(device)\n",
    "        Y = torch.from_numpy(Y).to(device)\n",
    "        Z_idx = torch.from_numpy(Z_idx).to(device)\n",
    "        Z_p   = torch.from_numpy(Z_p).to(device)\n",
    "        return X, Y, (Z_idx, Z_p)\n",
    "\n",
    "def collate_identity(batch):\n",
    "    Xs, Ys, Zs = zip(*batch)\n",
    "    X = torch.cat(Xs, dim=0)\n",
    "    Y = torch.cat(Ys, dim=0)\n",
    "    Zi = torch.cat([z[0] for z in Zs], dim=0)\n",
    "    Zp = torch.cat([z[1] for z in Zs], dim=0)\n",
    "    return X, Y, (Zi, Zp)\n",
    "\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "GPU_DATASET = GPUDataset(\n",
    "    np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r'),\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    builder=discZ,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    GPU_DATASET,\n",
    "    batch_size=1,            # keep outer loader at 1; inner dataset batches on GPU\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_identity\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efacb560-8e61-4e4e-9399-3c1611b476ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1.67M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "config =  GPTConfig(\n",
    "    block_size,\n",
    "    vocab_size,\n",
    "    n_layer=8,      \n",
    "    n_head = 8,\n",
    "    n_embd =128)\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, yb, zb in train_loader:\n",
    "        # xb: (B, T), yb: (B, T), zb: (Z_idx, Z_p) with shapes (B,T,K)\n",
    "        logits, loss = model(xb, None, zb, aux_scale=1.0)  # tweak aux_scale as desired\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total += loss.item()\n",
    "        print(loss.item())\n",
    "        losses.append(loss.item())\n",
    "    return total / len(train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c43d3e5c-ba7e-4a1e-9b59-1b593316e2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1799936\n"
     ]
    }
   ],
   "source": [
    "print(sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae6451f-1367-40a2-9fb9-85a5085ef5bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.698856353759766\n",
      "30.6633243560791\n",
      "29.268829345703125\n",
      "28.078699111938477\n",
      "29.367504119873047\n",
      "26.953575134277344\n",
      "26.533857345581055\n",
      "26.627788543701172\n",
      "25.156251907348633\n",
      "27.62116813659668\n",
      "25.379484176635742\n",
      "24.63443374633789\n",
      "24.707481384277344\n",
      "24.847110748291016\n",
      "24.485004425048828\n",
      "23.806835174560547\n",
      "24.23027992248535\n",
      "23.782827377319336\n",
      "23.531429290771484\n",
      "23.22844696044922\n",
      "23.27797508239746\n",
      "23.263351440429688\n",
      "23.124021530151367\n",
      "22.832313537597656\n",
      "22.520254135131836\n",
      "22.748977661132812\n",
      "22.25548553466797\n",
      "22.110244750976562\n",
      "22.228118896484375\n",
      "22.002477645874023\n",
      "22.31650733947754\n",
      "21.83387565612793\n",
      "21.63393211364746\n",
      "22.08163833618164\n",
      "21.863000869750977\n",
      "21.64391326904297\n",
      "21.60404396057129\n",
      "21.57720947265625\n",
      "21.316740036010742\n",
      "21.66377067565918\n",
      "21.468870162963867\n",
      "21.442127227783203\n",
      "21.461366653442383\n",
      "21.332015991210938\n",
      "21.502639770507812\n",
      "21.411680221557617\n",
      "21.30423927307129\n",
      "21.378374099731445\n",
      "21.551395416259766\n",
      "21.29563331604004\n",
      "21.196739196777344\n",
      "21.33504295349121\n",
      "21.10474967956543\n",
      "20.866558074951172\n",
      "21.205211639404297\n",
      "20.971385955810547\n",
      "20.830951690673828\n",
      "20.98389434814453\n",
      "20.940441131591797\n",
      "20.868253707885742\n",
      "20.977745056152344\n",
      "20.759017944335938\n",
      "21.03179931640625\n",
      "20.872400283813477\n",
      "20.88095474243164\n",
      "20.811309814453125\n",
      "20.7536563873291\n",
      "20.774852752685547\n",
      "20.37717628479004\n",
      "20.82282829284668\n",
      "20.939998626708984\n",
      "21.16897964477539\n",
      "20.43571662902832\n",
      "20.561065673828125\n",
      "20.773761749267578\n",
      "20.758562088012695\n",
      "20.636377334594727\n",
      "20.439125061035156\n",
      "20.93212127685547\n",
      "20.682687759399414\n",
      "20.834524154663086\n",
      "20.56919288635254\n",
      "20.427589416503906\n",
      "20.360950469970703\n",
      "20.58454704284668\n",
      "20.307777404785156\n",
      "20.21434783935547\n",
      "20.553810119628906\n",
      "20.231605529785156\n",
      "20.452632904052734\n",
      "20.610702514648438\n",
      "20.354568481445312\n",
      "20.413394927978516\n",
      "20.47783660888672\n",
      "20.389564514160156\n",
      "20.50082778930664\n",
      "20.358007431030273\n",
      "20.27850914001465\n",
      "20.23579216003418\n",
      "20.198213577270508\n",
      "20.339754104614258\n",
      "20.35287857055664\n",
      "20.584985733032227\n",
      "20.35878562927246\n",
      "20.466907501220703\n",
      "20.139835357666016\n",
      "20.26032257080078\n",
      "20.190710067749023\n",
      "20.285907745361328\n",
      "20.439006805419922\n",
      "19.971721649169922\n",
      "20.424663543701172\n",
      "20.731603622436523\n",
      "20.42551040649414\n",
      "20.299076080322266\n",
      "20.274505615234375\n",
      "20.311912536621094\n",
      "20.461824417114258\n",
      "20.37703514099121\n",
      "20.410770416259766\n",
      "20.301443099975586\n",
      "20.17630386352539\n",
      "20.59070587158203\n",
      "20.561634063720703\n",
      "20.03978729248047\n",
      "20.183652877807617\n",
      "20.08342933654785\n",
      "19.939889907836914\n",
      "20.47159194946289\n",
      "20.30159568786621\n",
      "20.31887435913086\n",
      "20.307754516601562\n",
      "20.087482452392578\n",
      "20.345081329345703\n",
      "20.296836853027344\n",
      "20.199676513671875\n",
      "19.99850082397461\n",
      "20.264720916748047\n",
      "20.253828048706055\n",
      "20.209226608276367\n",
      "20.364099502563477\n",
      "20.679153442382812\n",
      "20.242572784423828\n",
      "20.14105796813965\n",
      "20.386125564575195\n",
      "20.20411491394043\n",
      "20.13776969909668\n",
      "20.334001541137695\n",
      "20.052501678466797\n",
      "20.27494239807129\n",
      "20.225383758544922\n",
      "20.173625946044922\n",
      "20.109176635742188\n",
      "19.89777946472168\n",
      "20.15034294128418\n",
      "19.962800979614258\n",
      "20.375473022460938\n",
      "20.19888687133789\n",
      "20.101329803466797\n",
      "20.158855438232422\n",
      "20.044652938842773\n",
      "19.985090255737305\n",
      "20.013477325439453\n",
      "20.17261505126953\n",
      "20.364662170410156\n",
      "19.94911003112793\n",
      "20.03215217590332\n",
      "20.092708587646484\n",
      "20.064720153808594\n",
      "20.072193145751953\n",
      "20.1416015625\n",
      "20.072439193725586\n",
      "19.789091110229492\n",
      "20.08173370361328\n",
      "20.06256103515625\n",
      "19.9641170501709\n",
      "20.102203369140625\n",
      "20.296239852905273\n",
      "20.00617218017578\n",
      "20.040403366088867\n",
      "19.748899459838867\n",
      "20.161121368408203\n",
      "20.004392623901367\n",
      "20.014217376708984\n",
      "20.085481643676758\n",
      "19.909820556640625\n",
      "19.94283676147461\n",
      "19.977209091186523\n",
      "20.13175392150879\n",
      "20.070966720581055\n",
      "20.05254554748535\n",
      "19.971437454223633\n",
      "20.20027732849121\n",
      "19.920799255371094\n",
      "20.146881103515625\n",
      "19.892894744873047\n",
      "20.042259216308594\n",
      "19.88441276550293\n",
      "19.9990177154541\n",
      "20.02764320373535\n",
      "20.03593635559082\n",
      "19.99882698059082\n",
      "20.19139289855957\n",
      "19.78632164001465\n",
      "20.04802703857422\n",
      "19.907243728637695\n",
      "19.990453720092773\n",
      "20.211280822753906\n",
      "19.927709579467773\n",
      "20.164703369140625\n",
      "19.999910354614258\n",
      "20.402379989624023\n",
      "19.843860626220703\n",
      "19.775943756103516\n",
      "20.049991607666016\n",
      "20.092819213867188\n",
      "19.963960647583008\n",
      "20.083349227905273\n",
      "19.92186737060547\n",
      "19.77798843383789\n",
      "19.964048385620117\n",
      "20.091032028198242\n",
      "19.830795288085938\n",
      "19.79949188232422\n",
      "19.695199966430664\n",
      "20.076242446899414\n",
      "19.763748168945312\n",
      "20.05306625366211\n",
      "20.325511932373047\n",
      "19.835119247436523\n",
      "20.02007484436035\n",
      "20.069704055786133\n",
      "19.75527572631836\n",
      "20.228307723999023\n",
      "20.162580490112305\n",
      "19.940645217895508\n",
      "20.08908462524414\n",
      "20.014148712158203\n",
      "19.81856346130371\n",
      "19.995555877685547\n",
      "19.959857940673828\n",
      "20.011402130126953\n",
      "19.990659713745117\n",
      "20.0228328704834\n",
      "20.182119369506836\n",
      "20.118741989135742\n",
      "19.939952850341797\n",
      "19.782413482666016\n",
      "20.055255889892578\n",
      "19.817441940307617\n",
      "19.96150016784668\n",
      "19.933616638183594\n",
      "19.890100479125977\n",
      "19.957210540771484\n",
      "19.849971771240234\n",
      "19.850231170654297\n",
      "20.129840850830078\n",
      "19.795804977416992\n",
      "19.93561553955078\n",
      "19.873538970947266\n",
      "20.050430297851562\n",
      "19.979795455932617\n",
      "19.92063331604004\n",
      "19.762786865234375\n",
      "19.832321166992188\n",
      "20.139694213867188\n",
      "20.074499130249023\n",
      "20.201515197753906\n",
      "20.079683303833008\n",
      "19.902835845947266\n",
      "19.84571075439453\n",
      "19.92034912109375\n",
      "19.74015998840332\n",
      "20.235301971435547\n",
      "20.124958038330078\n",
      "20.239683151245117\n",
      "19.841087341308594\n",
      "20.121658325195312\n",
      "20.000507354736328\n",
      "19.990896224975586\n",
      "19.84435272216797\n",
      "19.606290817260742\n",
      "19.892059326171875\n",
      "19.86043930053711\n",
      "20.105361938476562\n",
      "19.874755859375\n",
      "20.12470054626465\n",
      "19.889097213745117\n",
      "19.90699005126953\n",
      "19.883878707885742\n",
      "19.99242401123047\n",
      "20.07425308227539\n",
      "20.170942306518555\n",
      "19.822908401489258\n",
      "19.86256980895996\n",
      "19.976842880249023\n",
      "19.871902465820312\n",
      "19.951108932495117\n",
      "19.737802505493164\n",
      "19.80028533935547\n",
      "19.77670669555664\n",
      "19.80860137939453\n",
      "19.675567626953125\n",
      "19.937421798706055\n",
      "19.849695205688477\n",
      "19.776535034179688\n",
      "19.99795150756836\n",
      "19.838329315185547\n",
      "19.928699493408203\n",
      "20.116281509399414\n",
      "19.810747146606445\n",
      "19.810840606689453\n",
      "19.74956512451172\n",
      "19.579679489135742\n",
      "19.856426239013672\n",
      "19.870502471923828\n",
      "19.79572296142578\n",
      "19.864459991455078\n",
      "19.905763626098633\n",
      "19.888988494873047\n",
      "19.970020294189453\n",
      "19.689655303955078\n",
      "19.833913803100586\n",
      "19.786792755126953\n",
      "19.973180770874023\n",
      "19.966453552246094\n",
      "19.864667892456055\n",
      "20.037147521972656\n",
      "19.897308349609375\n",
      "19.80337142944336\n",
      "19.95948028564453\n",
      "19.721797943115234\n",
      "19.663455963134766\n",
      "19.669965744018555\n",
      "19.650636672973633\n",
      "19.77589225769043\n",
      "20.08486557006836\n",
      "19.94593620300293\n",
      "19.688087463378906\n",
      "19.798171997070312\n",
      "19.894847869873047\n",
      "19.9373779296875\n",
      "20.062971115112305\n",
      "19.980159759521484\n",
      "19.953004837036133\n",
      "20.147661209106445\n",
      "19.964324951171875\n",
      "20.015888214111328\n",
      "19.735380172729492\n",
      "19.74882698059082\n",
      "19.894084930419922\n",
      "20.013046264648438\n",
      "20.082473754882812\n",
      "19.88321876525879\n",
      "19.780776977539062\n",
      "19.721946716308594\n",
      "19.859275817871094\n",
      "19.771190643310547\n",
      "20.003307342529297\n",
      "19.717552185058594\n",
      "19.904926300048828\n",
      "19.758928298950195\n",
      "19.79552459716797\n",
      "19.928743362426758\n",
      "19.65829849243164\n",
      "19.916696548461914\n",
      "19.813228607177734\n",
      "19.847511291503906\n",
      "19.987316131591797\n",
      "19.67414093017578\n",
      "20.20989990234375\n",
      "19.90540885925293\n",
      "20.046979904174805\n",
      "19.91000747680664\n",
      "19.761016845703125\n",
      "19.634004592895508\n",
      "19.857263565063477\n",
      "20.069673538208008\n",
      "19.870580673217773\n",
      "19.611482620239258\n",
      "19.977270126342773\n",
      "20.186492919921875\n",
      "19.873998641967773\n",
      "19.855602264404297\n",
      "19.97930145263672\n",
      "20.03812026977539\n",
      "19.839611053466797\n",
      "19.735456466674805\n",
      "20.20406150817871\n",
      "19.777557373046875\n",
      "19.867347717285156\n",
      "19.82071876525879\n",
      "19.802860260009766\n",
      "19.95948600769043\n",
      "19.87383270263672\n",
      "19.7987003326416\n",
      "19.799318313598633\n",
      "19.865427017211914\n",
      "19.560836791992188\n",
      "20.275074005126953\n",
      "19.907596588134766\n",
      "19.931486129760742\n",
      "19.934947967529297\n",
      "19.95949363708496\n",
      "19.73535919189453\n",
      "19.957740783691406\n",
      "20.205867767333984\n",
      "19.752504348754883\n",
      "19.982688903808594\n",
      "19.926864624023438\n",
      "19.992080688476562\n",
      "19.7066650390625\n",
      "19.768604278564453\n",
      "19.78722381591797\n",
      "19.555233001708984\n",
      "19.573352813720703\n",
      "19.839736938476562\n",
      "19.89067840576172\n",
      "19.61888885498047\n",
      "19.81121253967285\n",
      "19.883529663085938\n",
      "19.76116371154785\n",
      "19.837072372436523\n",
      "19.85051155090332\n",
      "19.812807083129883\n",
      "19.777610778808594\n",
      "19.853816986083984\n",
      "19.663265228271484\n",
      "19.69964599609375\n",
      "19.77785873413086\n",
      "19.618776321411133\n",
      "19.672517776489258\n",
      "19.92536163330078\n",
      "19.736515045166016\n",
      "19.86827850341797\n",
      "19.950687408447266\n",
      "19.58036231994629\n",
      "19.823518753051758\n",
      "19.646835327148438\n",
      "19.584781646728516\n",
      "19.924213409423828\n",
      "19.520586013793945\n",
      "19.62128448486328\n",
      "19.666259765625\n",
      "19.766986846923828\n",
      "19.628992080688477\n",
      "19.695735931396484\n",
      "19.760421752929688\n",
      "19.671823501586914\n",
      "19.977224349975586\n",
      "19.614219665527344\n",
      "19.46617889404297\n",
      "19.392433166503906\n",
      "19.784406661987305\n",
      "19.68593978881836\n",
      "19.809581756591797\n",
      "19.881816864013672\n",
      "19.957992553710938\n",
      "19.894790649414062\n",
      "19.8325252532959\n",
      "19.77338218688965\n",
      "19.518476486206055\n",
      "19.789783477783203\n",
      "19.662681579589844\n",
      "19.813148498535156\n",
      "19.512208938598633\n",
      "19.587915420532227\n",
      "20.081613540649414\n",
      "19.790523529052734\n",
      "19.67278480529785\n",
      "19.599044799804688\n",
      "19.821025848388672\n",
      "20.002992630004883\n",
      "19.592130661010742\n",
      "19.582212448120117\n",
      "19.84672737121582\n",
      "19.73374366760254\n",
      "19.84817123413086\n",
      "20.063945770263672\n",
      "19.945697784423828\n",
      "19.855566024780273\n",
      "19.632671356201172\n",
      "19.806631088256836\n",
      "19.690645217895508\n",
      "19.80891227722168\n",
      "19.732872009277344\n",
      "19.988441467285156\n",
      "19.929107666015625\n",
      "19.58133316040039\n",
      "19.67137908935547\n",
      "19.388214111328125\n",
      "19.602317810058594\n",
      "19.606138229370117\n",
      "19.815622329711914\n",
      "19.677865982055664\n",
      "19.58724594116211\n",
      "19.479019165039062\n",
      "20.015968322753906\n",
      "19.6460018157959\n",
      "19.68951416015625\n",
      "19.80293083190918\n",
      "19.96909523010254\n",
      "19.528718948364258\n",
      "19.70774269104004\n",
      "19.51572036743164\n",
      "19.717426300048828\n",
      "19.81361961364746\n",
      "19.691152572631836\n",
      "19.722652435302734\n",
      "19.549551010131836\n",
      "19.619665145874023\n",
      "19.692283630371094\n",
      "19.90753173828125\n",
      "19.668540954589844\n",
      "19.811321258544922\n",
      "19.70543670654297\n",
      "19.622234344482422\n",
      "19.65104866027832\n",
      "19.53413963317871\n",
      "19.667387008666992\n",
      "19.831369400024414\n",
      "19.814746856689453\n",
      "19.632686614990234\n",
      "19.809505462646484\n",
      "19.499744415283203\n",
      "19.79492950439453\n",
      "19.864965438842773\n",
      "19.650774002075195\n",
      "19.8424072265625\n",
      "19.838150024414062\n",
      "19.965747833251953\n",
      "19.69544219970703\n",
      "19.83184814453125\n",
      "19.765409469604492\n",
      "19.843753814697266\n",
      "19.812957763671875\n",
      "19.55594825744629\n",
      "19.385162353515625\n",
      "19.909801483154297\n",
      "19.65911102294922\n",
      "19.816789627075195\n",
      "19.92786979675293\n",
      "19.632688522338867\n",
      "19.63045310974121\n",
      "19.609050750732422\n",
      "19.5559024810791\n",
      "19.51888656616211\n",
      "19.915964126586914\n",
      "19.556257247924805\n",
      "19.796783447265625\n",
      "20.026538848876953\n",
      "19.820276260375977\n",
      "19.800626754760742\n",
      "19.75933074951172\n",
      "19.563989639282227\n",
      "19.61203956604004\n",
      "19.54010581970215\n",
      "19.729745864868164\n",
      "19.67457389831543\n",
      "19.481670379638672\n",
      "19.779369354248047\n",
      "19.523296356201172\n",
      "19.72632598876953\n",
      "19.560373306274414\n",
      "19.807809829711914\n",
      "19.589950561523438\n",
      "19.41375160217285\n",
      "19.684764862060547\n",
      "19.562400817871094\n",
      "20.09079360961914\n",
      "19.717952728271484\n",
      "20.038827896118164\n",
      "19.6091365814209\n",
      "19.819326400756836\n",
      "19.78102684020996\n",
      "19.98924446105957\n",
      "19.726551055908203\n",
      "19.65458106994629\n",
      "19.85022735595703\n",
      "19.89068031311035\n",
      "19.849994659423828\n",
      "19.955562591552734\n",
      "19.62021255493164\n",
      "19.794239044189453\n",
      "19.8227596282959\n",
      "19.508190155029297\n",
      "19.641176223754883\n",
      "19.64750862121582\n",
      "19.54617691040039\n",
      "19.836076736450195\n",
      "19.72608184814453\n",
      "19.98459243774414\n",
      "19.48933219909668\n",
      "19.60042953491211\n",
      "19.867136001586914\n",
      "19.584083557128906\n",
      "19.559886932373047\n",
      "19.982892990112305\n",
      "19.5647029876709\n",
      "19.75709342956543\n",
      "19.542869567871094\n",
      "19.71794891357422\n",
      "19.814796447753906\n",
      "19.68074607849121\n",
      "19.962188720703125\n",
      "19.918842315673828\n",
      "19.687992095947266\n",
      "19.82870101928711\n",
      "19.875680923461914\n",
      "19.582502365112305\n",
      "19.571382522583008\n",
      "19.687854766845703\n",
      "19.649066925048828\n",
      "19.63447380065918\n",
      "19.64299201965332\n",
      "19.77442169189453\n",
      "19.50849151611328\n",
      "19.46529197692871\n",
      "19.81541633605957\n",
      "19.56125831604004\n",
      "19.478851318359375\n",
      "19.803569793701172\n",
      "19.653091430664062\n",
      "19.916515350341797\n",
      "19.553895950317383\n",
      "19.632965087890625\n",
      "19.61517333984375\n",
      "19.66620635986328\n",
      "19.832122802734375\n",
      "19.55675506591797\n",
      "19.502002716064453\n",
      "19.485702514648438\n",
      "19.83513069152832\n",
      "19.530704498291016\n",
      "19.831146240234375\n",
      "19.795631408691406\n",
      "19.619361877441406\n",
      "19.45531463623047\n",
      "19.56363296508789\n",
      "19.633888244628906\n",
      "19.73857307434082\n",
      "19.439481735229492\n",
      "19.754915237426758\n",
      "19.483732223510742\n",
      "19.72986602783203\n",
      "19.948898315429688\n",
      "19.64182472229004\n",
      "19.971637725830078\n",
      "19.684457778930664\n",
      "19.5744686126709\n",
      "19.62849235534668\n",
      "19.35916519165039\n",
      "19.933914184570312\n",
      "19.699501037597656\n",
      "19.624006271362305\n",
      "19.452760696411133\n",
      "19.75345802307129\n",
      "19.42230224609375\n",
      "19.66691017150879\n",
      "19.822467803955078\n",
      "19.597379684448242\n",
      "19.656625747680664\n",
      "19.926074981689453\n",
      "19.474090576171875\n",
      "19.565876007080078\n",
      "19.670452117919922\n",
      "19.391679763793945\n",
      "19.4212646484375\n",
      "19.72002410888672\n",
      "19.45243263244629\n",
      "19.778457641601562\n",
      "19.69717025756836\n",
      "19.740659713745117\n",
      "19.45200538635254\n",
      "19.799726486206055\n",
      "19.580293655395508\n",
      "19.589107513427734\n",
      "19.939453125\n",
      "19.607681274414062\n",
      "19.782588958740234\n",
      "19.774459838867188\n",
      "19.712129592895508\n",
      "19.56772232055664\n",
      "19.670902252197266\n",
      "19.77849006652832\n",
      "19.797590255737305\n",
      "19.631881713867188\n",
      "19.949129104614258\n",
      "19.50592041015625\n",
      "19.804609298706055\n",
      "19.55651092529297\n",
      "19.824188232421875\n",
      "19.62366485595703\n",
      "19.83629035949707\n",
      "19.260061264038086\n",
      "19.45162010192871\n",
      "19.356098175048828\n",
      "19.597259521484375\n",
      "19.592926025390625\n",
      "19.591951370239258\n",
      "19.696216583251953\n",
      "19.58164405822754\n",
      "19.735870361328125\n",
      "19.981693267822266\n",
      "19.58449363708496\n",
      "19.774141311645508\n",
      "19.411319732666016\n",
      "19.913158416748047\n",
      "19.31931495666504\n",
      "19.785778045654297\n",
      "19.895036697387695\n",
      "19.72730255126953\n",
      "19.766956329345703\n",
      "19.51056671142578\n",
      "19.649137496948242\n",
      "19.503536224365234\n",
      "19.879737854003906\n",
      "19.47536849975586\n",
      "19.74083709716797\n",
      "19.81073760986328\n",
      "19.77680206298828\n",
      "19.9343204498291\n",
      "19.670358657836914\n",
      "19.447349548339844\n",
      "19.677017211914062\n",
      "19.476886749267578\n",
      "19.737699508666992\n",
      "19.477760314941406\n",
      "19.539091110229492\n",
      "19.545516967773438\n",
      "19.5911865234375\n",
      "19.319841384887695\n",
      "19.78999900817871\n",
      "19.644283294677734\n",
      "19.775115966796875\n",
      "19.887786865234375\n",
      "19.791765213012695\n",
      "19.616676330566406\n",
      "19.314170837402344\n",
      "19.59068489074707\n",
      "19.775012969970703\n",
      "19.186681747436523\n",
      "19.48716926574707\n",
      "19.733726501464844\n",
      "19.953603744506836\n",
      "19.665822982788086\n",
      "19.57834243774414\n",
      "19.431875228881836\n",
      "19.546279907226562\n",
      "19.6637020111084\n",
      "19.723783493041992\n",
      "19.510787963867188\n",
      "19.407161712646484\n",
      "19.365947723388672\n",
      "19.48566246032715\n",
      "19.607219696044922\n",
      "19.436845779418945\n",
      "19.49399185180664\n",
      "19.7067928314209\n",
      "19.606658935546875\n",
      "19.65229034423828\n",
      "19.63823890686035\n",
      "19.56014060974121\n",
      "19.52775764465332\n",
      "19.647655487060547\n",
      "19.577226638793945\n",
      "19.608013153076172\n",
      "19.646060943603516\n",
      "19.518299102783203\n",
      "19.63158416748047\n",
      "19.559436798095703\n",
      "19.768585205078125\n",
      "19.428224563598633\n",
      "19.769081115722656\n",
      "19.6805419921875\n",
      "19.481504440307617\n",
      "19.270448684692383\n",
      "19.412813186645508\n",
      "19.403060913085938\n",
      "19.451366424560547\n",
      "19.358802795410156\n",
      "19.594810485839844\n",
      "19.768285751342773\n",
      "19.32212257385254\n",
      "19.41138458251953\n",
      "19.765512466430664\n",
      "19.433855056762695\n",
      "19.46062660217285\n",
      "19.519498825073242\n",
      "19.4517822265625\n",
      "19.544296264648438\n",
      "19.67754364013672\n",
      "19.468719482421875\n",
      "19.569059371948242\n",
      "19.721757888793945\n",
      "19.524580001831055\n",
      "19.64056396484375\n",
      "19.45033836364746\n",
      "19.59185218811035\n",
      "19.61075210571289\n",
      "19.547279357910156\n",
      "19.519290924072266\n",
      "19.481517791748047\n",
      "19.540220260620117\n",
      "19.411605834960938\n",
      "19.627277374267578\n",
      "19.361072540283203\n",
      "19.444917678833008\n",
      "19.57118034362793\n",
      "19.733909606933594\n",
      "19.663578033447266\n",
      "19.380260467529297\n",
      "19.5527400970459\n"
     ]
    }
   ],
   "source": [
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36c6dd9b-dafc-4c7e-94e9-fde8dd10f186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3bb2a0530>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe76faa0-3933-416d-b923-993c67cfcf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tale to the world the worlf the wall the wall.\n",
      "\n",
      "LADY CAPULET:\n",
      "I will not the death the way the trumpetted thee.\n",
      "\n",
      "Nurse:\n",
      "Nurse, I will not the way the way, then I was they say,\n",
      "then they shall the way they are they was a woffer.\n",
      "\n",
      "Nurse:\n",
      "I do not thee war, then they warrant they way.\n",
      "\n",
      "LADY CAPULET:\n",
      "I will say thee to the way thee was they was a wone.\n",
      "\n",
      "Nurse:\n",
      "I do not thee to the way then thee way thee way.\n",
      "\n",
      "Nurse:\n",
      "I do you are to the way the way they was a word.\n",
      "\n",
      "Nurse:\n",
      "I do not thee to-morrow that I was they was a word.\n",
      "\n",
      "LADY CAPULET:\n",
      "I do not thee to the way to the way to thee.\n",
      "\n",
      "Nurse:\n",
      "I do not thee to the way to the world of the world.\n",
      "\n",
      "Nurse:\n",
      "I do not the way thee way the way the way thee.\n",
      "\n",
      "LADY CAPULET:\n",
      "I do not thee to the world the world the world.\n",
      "\n",
      "Nurse:\n",
      "I do not thee to the way the way the worther.\n",
      "\n",
      "LADY CAPULET:\n",
      "I do thee to the worthy the world of the cheeks.\n",
      "\n",
      "Nurse:\n",
      "I think thee the heart the world of the world,\n",
      "And then they have the to the heart they had they had they\n",
      "And the truly of the world of the world of the state.\n",
      "\n",
      "\n",
      "Nurse:\n",
      "I do thee to the worth of the worth of the words.\n",
      "\n",
      "\n",
      "LANTIS:\n",
      "I would thee to the worth of the words the world,\n",
      "And the wall the worth of the state of the state.\n",
      "\n",
      "\n",
      "LARTCA:\n",
      "I the day thee to the worth of the words the world.\n",
      "\n",
      "\n",
      "LARTCA:\n",
      "The trust the state of the state of the state.\n",
      "\n",
      "\n",
      "Nurse:\n",
      "O the stab of the state of the state,\n",
      "And the state of the state of the state.\n",
      "\n",
      "\n",
      "Nurse:\n",
      "O the stabb'd the state of the state.\n",
      "\n",
      "\n",
      "LARTIS:\n",
      "I would the doon of the state, and the words of the stand of the stander.\n",
      "\n",
      "\n",
      "LARTCA:\n",
      "I the day the worthy the state of the state.\n",
      "\n",
      "\n",
      "Nurse:\n",
      "The day of the state of the state of the state.\n",
      "\n",
      "\n",
      "LARTCA:\n",
      "I do the lost of the worth of the state.\n",
      "\n",
      "\n",
      "LARTIS:\n",
      "I would the worthy the state of the words of the words.\n",
      "\n",
      "\n",
      "LARTIS:\n",
      "The day the state of the state of the state.\n",
      "\n",
      "\n",
      "PAULINA:\n",
      "I do see the state the worth of the state.\n",
      "\n",
      "\n",
      "PAULINA:\n",
      "I have say to the state of the words of the worse.\n",
      "\n",
      "\n",
      "PAULINA:\n",
      "I have she shall be the state of the state.\n",
      "\n",
      "\n",
      "PAULINA:\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "def decode_sequence_char(\n",
    "    model, stoi, itos, prompt, max_new_tokens=100, block_size=256,\n",
    "    use_fenchel=False, tau=1.0, fenchel_iters=3, temperature=1.0\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    max_ctx = int(block_size)\n",
    "\n",
    "    # ?1 assume space token exists; fallback to 0 if missing\n",
    "    space_id = stoi.get(' ', 0)\n",
    "\n",
    "    # encode prompt\n",
    "    start_ids = torch.tensor([encode_chars(prompt, stoi)], dtype=torch.long, device=device)\n",
    "\n",
    "    # prepend the pad once; from now on the window just slides\n",
    "\n",
    "    idx = start_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = idx[:, -max_ctx:]  # rotating buffer: last pad_len+block_size tokens\n",
    "        logits, _ = model(context, None)\n",
    "        last_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    # drop the initial pad when returning the string\n",
    "    out_tokens = idx[0].tolist()\n",
    "    return decode_chars(out_tokens, itos)\n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "\n",
    "prompt = \" \"\n",
    "generated = decode_sequence_char(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=2024,\n",
    "    block_size=1024,\n",
    "    use_fenchel=False,\n",
    "    tau=1.5,\n",
    "    fenchel_iters=2,\n",
    "    temperature=0.0001\n",
    ")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e045fae-f129-499c-a0f6-0376e15fef27",
   "metadata": {},
   "source": [
    "This is Not equivalent to learning a Markov model. the synthetic composition of the aux target breaks that equivalence on multiple fronts, so the transformer can‚Äîand evidently does‚Äîlearn supra-Markov structure even though Z itself is built from n-gram machinery.\n",
    "\n",
    "why it‚Äôs not Markov-equivalent:\n",
    "\n",
    "teacher ‚â† single-order chain. Z is a composite of multiple orders (2‚Äì64) with B-tree intersection/union, top-K capping, and Œµ fills from global frequency. there‚Äôs no single conditional table that reproduces those distributions; they‚Äôre a curated prior, not a proper n-gram MLE.\n",
    "\n",
    "objective ‚â† local count fitting. we optimize a target with a deep network. nothing forces it to compute conditionals the way an n-gram would. the network can use any cues in its receptive field (including >64 tokens) to match Z statistically across the corpus.\n",
    "\n",
    "multi-depth CE (same Z, different skew). supervising every block with depth-wise sharpening introduces constraints on internal representations (curriculum/distillation effect) that Markov models don‚Äôt have. it shapes a hierarchy, not just the final conditional.\n",
    "\n",
    "support prior, not ground truth. by constraining support to plausible continuations, Z acts like an energy manifold (valid vs. invalid regions). the model is trained to align with that manifold; it is not estimating n-gram probabilities per se.\n",
    "\n",
    "global coupling through parameter sharing. transformers share parameters across positions and contexts; learning to match Z in one regime generalizes to others in ways that exceed local-count estimators.\n",
    "\n",
    "observed behavior contradicts Markov collapse. the low-temperature generations degrade into structured attractors (speaker scaffolds, grammatical frames), not trivial loops; that‚Äôs a hallmark of a smooth, global field learned over sequences, not a brittle n-gram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72509ed-bff1-43e2-a08a-4aad880fb293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
