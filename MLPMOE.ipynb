{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7741bdf-98ab-4400-a0d1-4b831244ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "# --- GPT with auxiliary reverse-embedding loss from zb ---\n",
    "import math, inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# assumes Block and LayerNorm are defined elsewhere (as in your current setup)\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config: GPTConfig,\n",
    "                 aux_scale: float = 1.0,\n",
    "                 noise_constituent: float = 1e-4,\n",
    "                 noise_final: float = 1e-4):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None and config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        # per-block orthonormal maps for blocks 0..3\n",
    "        need_blocks = 4\n",
    "        if config.n_layer < need_blocks:\n",
    "            raise ValueError(f\"need at least {need_blocks} transformer blocks for aux; got {config.n_layer}\")\n",
    "        self.aux_blocks = list(range(need_blocks))  # [0,1,2,3] fixed\n",
    "        self.aux_maps = nn.ModuleList()\n",
    "        for _ in self.aux_blocks:\n",
    "            lin = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            nn.init.orthogonal_(lin.weight)  # square => orthonormal rows & columns\n",
    "            self.aux_maps.append(lin)\n",
    "\n",
    "        # noise/scales\n",
    "        self.aux_scale_default = float(aux_scale)\n",
    "        self.noise_constituent = float(noise_constituent)\n",
    "        self.noise_final = float(noise_final)\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # reverse-embedding for one lane (list length T of [idxs, probs])\n",
    "    def _rev_embed_lane(self, lane_seq, device):\n",
    "        T = len(lane_seq)\n",
    "        if T == 0:\n",
    "            return torch.empty(0, self.config.n_embd, device=device)\n",
    "        idxs = torch.tensor([pair[0] for pair in lane_seq], device=device, dtype=torch.long)      # (T, K)\n",
    "        probs = torch.tensor([pair[1] for pair in lane_seq], device=device, dtype=torch.float32)  # (T, K)\n",
    "        probs = probs / (probs.sum(dim=-1, keepdim=True) + 1e-12)\n",
    "\n",
    "        E = self.transformer.wte.weight  # (V, D)\n",
    "        emb = E.index_select(0, idxs.reshape(-1)).reshape(*idxs.shape, E.size(1))  # (T, K, D)\n",
    "        if self.noise_constituent > 0:\n",
    "            emb = emb + torch.randn_like(emb) * self.noise_constituent\n",
    "        rev = torch.einsum('tkd,tk->td', emb, probs)  # (T, D)\n",
    "        if self.noise_final > 0:\n",
    "            rev = rev + torch.randn_like(rev) * self.noise_final\n",
    "        return rev\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 66\n",
    "    n_layer: int = 4          # explicitly 4 per your instruction\n",
    "    n_head: int = 8\n",
    "    n_embd: int = 128\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig,\n",
    "                 aux_scale: float = 1e-3,           # (12) fixed\n",
    "                 noise_constituent: float = 1e-6,    # (7) fixed\n",
    "                 noise_final: float = 1e-4):         # (7) fixed\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None and config.block_size is not None\n",
    "        if config.n_layer != 4:\n",
    "            raise ValueError(\"Set n_layer=4 (aux aligns: bigram, 4, 8, 16).\")\n",
    "\n",
    "        self.config = config\n",
    "        self.aux_scale = float(aux_scale)\n",
    "        self.noise_constituent = float(noise_constituent)\n",
    "        self.noise_final = float(noise_final)\n",
    "\n",
    "        # core transformer\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        #self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        # per-block orthonormal linears (square D√óD, columns orthonormal)\n",
    "        self.aux_maps = nn.ModuleList()\n",
    "        for _ in range(4):\n",
    "            lin = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            nn.init.orthogonal_(lin.weight)\n",
    "            self.aux_maps.append(lin)\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # -------- reverse-embedding helpers (batchified, same dtype/device as x) --------\n",
    "    def _extract_idx_prob(self, zb, pair_offset):\n",
    "        \"\"\"\n",
    "        zb: Python list of length B; zb[b] is list length T;\n",
    "            zb[b][t] is length-8 list per spec:\n",
    "              [idxs_bi, probs_bi, idxs_4, probs_4, idxs_8, probs_8, idxs_16, probs_16]\n",
    "        pair_offset: 0 for bigram, 2 for m4, 4 for m8, 6 for m16.\n",
    "        returns: (idxs, probs, K) with shapes (B, T, K)\n",
    "        \"\"\"\n",
    "        B = len(zb)\n",
    "        T = len(zb[0])\n",
    "        # infer K from the first timestep\n",
    "        K = len(zb[0][0][pair_offset])\n",
    "        idxs = torch.empty((B, T, K), dtype=torch.long)\n",
    "        probs = torch.empty((B, T, K), dtype=torch.float32)\n",
    "        for b in range(B):\n",
    "            seq = zb[b]\n",
    "            for t in range(T):\n",
    "                idxs[b, t] = torch.tensor(seq[t][pair_offset], dtype=torch.long)\n",
    "                probs[b, t] = torch.tensor(seq[t][pair_offset + 1], dtype=torch.float32)\n",
    "        return idxs, probs, K\n",
    "\n",
    "    def _rev_embed_batch(self, idxs, probs, dtype, device):\n",
    "        \"\"\"\n",
    "        idxs:  (B, T, K) long\n",
    "        probs: (B, T, K) float (will be cast to dtype)\n",
    "        returns rev: (B, T, D) in `dtype` on `device`\n",
    "        \"\"\"\n",
    "        E = self.transformer.wte.weight.to(dtype=dtype)   # (V, D)\n",
    "        B, T, K = idxs.shape\n",
    "        # gather embeddings: (B,T,K,D)\n",
    "        emb = E.index_select(0, idxs.reshape(-1)).reshape(B, T, K, E.size(1))\n",
    "        # noise per constituent\n",
    "        if self.noise_constituent > 0:\n",
    "            emb = emb + torch.randn_like(emb) * self.noise_constituent\n",
    "        # weighted sum\n",
    "        probs = probs.to(dtype=dtype, device=device)\n",
    "        rev = (emb * probs.unsqueeze(-1)).sum(dim=2)      # (B, T, D)\n",
    "        # final noise\n",
    "        if self.noise_final > 0:\n",
    "            rev = rev + torch.randn_like(rev) * self.noise_final\n",
    "        return rev\n",
    "\n",
    "    # --- replace ONLY the forward in GPT with this version ---\n",
    "    def forward(self, idx, targets=None, zb=None):\n",
    "        \"\"\"\n",
    "        idx: (B, T) Long\n",
    "        targets: (B, T) Long or None\n",
    "        zb: list of 4 tuples, each (idxs, probs) as numpy arrays with shape (B, T, 32)\n",
    "            order: 0=bigram, 1=4gram, 2=8gram, 3=16gram\n",
    "        \"\"\"\n",
    "        device = idx.device\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size\n",
    "    \n",
    "        # embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "    \n",
    "        aux_loss = None\n",
    "        dtype = x.dtype\n",
    "        ignore = {0: 1, 1: 3, 2: 7, 3: 15}  # warmup ignores\n",
    "    \n",
    "        # block loop with vectorized aux\n",
    "        for bidx, block in enumerate(self.transformer.h):\n",
    "            x = block(x)  # (B, T, D)\n",
    "    \n",
    "            if zb is not None and bidx < 4:\n",
    "                idxs_np, probs_np = zb[bidx]  # numpy arrays (B, T, K)\n",
    "                # to tensors on the right device/dtype\n",
    "                idxs  = torch.from_numpy(idxs_np).to(device=device, dtype=torch.long)\n",
    "                probs = torch.from_numpy(probs_np).to(device=device, dtype=dtype)\n",
    "    \n",
    "                # gather embeddings: E[idxs] -> (B,T,K,D)\n",
    "                E = self.transformer.wte.weight.to(dtype=dtype)\n",
    "                BTK = idxs.reshape(-1)\n",
    "                emb = E.index_select(0, BTK).reshape(B, T, probs.size(-1), E.size(1))\n",
    "    \n",
    "                # tiny noise per constituent, then weighted sum -> (B,T,D)\n",
    "                if self.noise_constituent > 0:\n",
    "                    emb = emb + torch.randn_like(emb) * self.noise_constituent\n",
    "                rev = (emb * probs.unsqueeze(-1)).sum(dim=2)\n",
    "                if self.noise_final > 0:\n",
    "                    rev = rev + torch.randn_like(rev) * self.noise_final\n",
    "                rev = rev + pos_emb\n",
    "                # per-block projector and masked MSE\n",
    "                mapped = rev# self.aux_maps[bidx](rev)  # (B,T,D)\n",
    "                mask = (torch.arange(T, device=device).expand(B, T) >= ignore[bidx]).unsqueeze(-1)  # (B,T,1)\n",
    "                diff2 = (x - mapped) ** 2\n",
    "                diff2 = diff2 * mask  # bool -> broadcast\n",
    "                denom = (mask.sum() * diff2.size(-1)).clamp_min(1)\n",
    "                block_loss = diff2.sum() / denom\n",
    "    \n",
    "                aux_loss = block_loss if aux_loss is None else aux_loss + block_loss\n",
    "    \n",
    "        # head + CE\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "    \n",
    "        ce_loss = None\n",
    "        if targets is not None:\n",
    "            ce_loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "                                      targets.view(-1), ignore_index=-100)\n",
    "    \n",
    "        # total\n",
    "        if ce_loss is None and aux_loss is None:\n",
    "            loss = None\n",
    "        elif aux_loss is None:\n",
    "            loss = ce_loss\n",
    "        elif ce_loss is None:\n",
    "            loss = self.aux_scale * aux_loss\n",
    "        else:\n",
    "            loss = ce_loss + self.aux_scale * aux_loss\n",
    "            loss = loss /5.0 #scale appropriately\n",
    "    \n",
    "        if targets is None:\n",
    "            logits = logits[:, [-1], :]\n",
    "    \n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d76ace-c3c1-4a68-adce-851c842dbbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading aochildes.txt...\n",
      "üì• Downloading cbt.txt...\n",
      "üì• Downloading children_stories.txt...\n",
      "üì• Downloading gutenberg.txt...\n",
      "üì• Downloading qed.txt...\n",
      "üì• Downloading simple_wikipedia.txt...\n",
      "üì• Downloading switchboard.txt...\n",
      "üì• Downloading wikipedia.txt...\n",
      "üì• Downloading shakespeare.txt...\n",
      "‚úÖ Done. Files saved to ./babylm_10m_cleaned\n"
     ]
    }
   ],
   "source": [
    "import requests, os\n",
    "\n",
    "base_url = \"https://huggingface.co/datasets/cambridge-climb/BabyLM/resolve/main/clean/10M/\"\n",
    "target_dir = \"./babylm_10m_cleaned\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"aochildes.txt\",\n",
    "    \"cbt.txt\",\n",
    "    \"children_stories.txt\",\n",
    "    \"gutenberg.txt\",\n",
    "    \"qed.txt\",\n",
    "    \"simple_wikipedia.txt\",\n",
    "    \"switchboard.txt\",\n",
    "    \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# Optional addition: Shakespeare from another dataset\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "shakespeare_fname = \"shakespeare.txt\"\n",
    "\n",
    "# Combined download logic\n",
    "all_files = [(base_url + fname, fname) for fname in file_names]\n",
    "all_files.append((shakespeare_url, shakespeare_fname))  # Add Shakespeare\n",
    "\n",
    "\n",
    "# Download loop\n",
    "for url, fname in all_files:\n",
    "    out_path = os.path.join(target_dir, fname)\n",
    "    print(f\"üì• Downloading {fname}...\")\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(resp.text)\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {fname} ({resp.status_code})\")\n",
    "\n",
    "print(f\"‚úÖ Done. Files saved to {target_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95a695ab-753c-4b35-8834-a1d4f59859bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Char tokenizer finalized.\n",
      "üßæ Train tokens: 1016242 | Val tokens: 99152\n",
      "üî§ Vocab size: 66\n",
      "Loaded 1016242 train tokens and 99152 val tokens | vocab=66\n",
      "Building 4-token Markov chain...\n",
      "Building 8-token Markov chain...\n",
      "Building 16-token Markov chain...\n",
      "Building bigram probability distribution...\n",
      "‚úÖ Markov and Bigram models saved.\n",
      "Chains: ['order=4', 'order=8', 'order=16']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# === Paths ===\n",
    "source_dir = \"./babylm_10m_cleaned\"\n",
    "out_dir    = \"./babylm_char_tokenized\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    \"shakespeare.txt\"#,\"aochildes.txt\", \"cbt.txt\", \"children_stories.txt\", \"gutenberg.txt\",\n",
    "    #\"qed.txt\", \"simple_wikipedia.txt\", \"switchboard.txt\", \"wikipedia.txt\"\n",
    "]\n",
    "\n",
    "# === Load and split ===\n",
    "train_texts, val_texts = [], []\n",
    "char_set = set()\n",
    "\n",
    "for fname in file_names:\n",
    "    with open(os.path.join(source_dir, fname), encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        n = len(lines)\n",
    "        split = int(0.9 * n)\n",
    "        train_part = \"\".join(lines[:split])\n",
    "        val_part   = \"\".join(lines[split:])\n",
    "        train_texts.append(train_part)\n",
    "        val_texts.append(val_part)\n",
    "        char_set.update(train_part)\n",
    "        char_set.update(val_part)\n",
    "\n",
    "full_train = \"\\n\".join(train_texts)\n",
    "full_val   = \"\\n\".join(val_texts)\n",
    "\n",
    "# === Final vocab ===\n",
    "char_set = sorted(set(char_set))\n",
    "vocab_chars = [\"<unk>\"] + [c for c in char_set if c != \"<unk>\"]\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(vocab_chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "# === Encode function ===\n",
    "def encode(text):\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "train_ids = np.array(encode(full_train), dtype=np.uint16)\n",
    "val_ids   = np.array(encode(full_val),   dtype=np.uint16)\n",
    "\n",
    "# === Save ===\n",
    "train_ids.tofile(os.path.join(out_dir, \"train.bin\"))\n",
    "val_ids.tofile(os.path.join(out_dir, \"val.bin\"))\n",
    "\n",
    "with open(os.path.join(out_dir, \"meta.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"vocab_size\": len(stoi),\n",
    "        \"stoi\": stoi,\n",
    "        \"itos\": itos\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Char tokenizer finalized.\")\n",
    "print(f\"üßæ Train tokens: {len(train_ids)} | Val tokens: {len(val_ids)}\")\n",
    "print(f\"üî§ Vocab size: {len(stoi)}\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# === Load data ===\n",
    "data_dir = \"./babylm_char_tokenized\"\n",
    "train_path = os.path.join(data_dir, \"train.bin\")\n",
    "val_path   = os.path.join(data_dir, \"val.bin\")\n",
    "meta_path  = os.path.join(data_dir, \"meta.pkl\")\n",
    "\n",
    "train_ids = np.fromfile(train_path, dtype=np.uint16)\n",
    "val_ids   = np.fromfile(val_path,   dtype=np.uint16)\n",
    "\n",
    "with open(meta_path, \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "vocab_size = meta[\"vocab_size\"]\n",
    "\n",
    "print(f\"Loaded {len(train_ids)} train tokens and {len(val_ids)} val tokens | vocab={vocab_size}\")\n",
    "\n",
    "# === Build Markov Models ===\n",
    "def build_markov_chain(data, window):\n",
    "    \"\"\"\n",
    "    Builds a Markov chain of given window size.\n",
    "    Returns: dict mapping tuple(context) -> Counter(next_token)\n",
    "    \"\"\"\n",
    "    chain = defaultdict(Counter)\n",
    "    for i in range(len(data) - window):\n",
    "        context = tuple(data[i : i + window])\n",
    "        nxt = data[i + window]\n",
    "        chain[context][nxt] += 1\n",
    "    return chain\n",
    "\n",
    "windows = [4, 8, 16]\n",
    "markov_models = {}\n",
    "\n",
    "for w in windows:\n",
    "    print(f\"Building {w}-token Markov chain...\")\n",
    "    markov_models[w] = build_markov_chain(train_ids, w)\n",
    "\n",
    "# === Build Bigram Continuation Probabilities ===\n",
    "import numpy as np\n",
    "\n",
    "def build_bigram_distribution_fixed(data, vocab_size, top_k=16, seed=1337, epsilon=1e-6):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # counts\n",
    "    bigram_counts = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "    a = data[:-1]\n",
    "    b = data[1:]\n",
    "    np.add.at(bigram_counts, (a, b), 1)\n",
    "\n",
    "    out_idx = np.empty((vocab_size, top_k), dtype=np.int32)\n",
    "    out_p   = np.empty((vocab_size, top_k), dtype=np.float32)\n",
    "\n",
    "    all_ids = np.arange(vocab_size, dtype=np.int32)\n",
    "\n",
    "    for tok in range(vocab_size):\n",
    "        counts = bigram_counts[tok]\n",
    "        total = counts.sum()\n",
    "\n",
    "        if total == 0:\n",
    "            # no observations ‚Äî choose k random unique tokens and make them uniform\n",
    "            idx = rng.choice(vocab_size, size=top_k, replace=False)\n",
    "            p = np.full(top_k, 1.0 / top_k, dtype=np.float32)\n",
    "            out_idx[tok] = idx\n",
    "            out_p[tok] = p\n",
    "            continue\n",
    "\n",
    "        probs_full = counts.astype(np.float64) / float(total)\n",
    "        observed = np.flatnonzero(counts)\n",
    "\n",
    "        if observed.size >= top_k:\n",
    "            # get top_k among observed only (fast top-k)\n",
    "            obs_p = probs_full[observed]\n",
    "            kth = np.argpartition(obs_p, -top_k)[-top_k:]\n",
    "            idx = observed[kth]\n",
    "            p = probs_full[idx].astype(np.float32)\n",
    "            # normalize in case of numerical drift\n",
    "            s = p.sum()\n",
    "            p = p / s if s > 0 else np.full(top_k, 1.0 / top_k, dtype=np.float32)\n",
    "        else:\n",
    "            # take all observed, randomly fill the rest from unobserved\n",
    "            need = top_k - observed.size\n",
    "            mask = np.ones(vocab_size, dtype=bool)\n",
    "            mask[observed] = False\n",
    "            pool = all_ids[mask]\n",
    "            # sample without replacement to avoid duplicates\n",
    "            extra = rng.choice(pool, size=need, replace=False)\n",
    "            idx = np.concatenate([observed, extra])\n",
    "\n",
    "            p = probs_full[idx].astype(np.float32)\n",
    "            # give a tiny positive mass to the extras that were unobserved (counts==0)\n",
    "            unobs = (counts[idx] == 0)\n",
    "            if unobs.any():\n",
    "                p = p + unobs.astype(np.float32) * epsilon\n",
    "            p = p / p.sum()\n",
    "\n",
    "        # ensure a consistent ordering (optional): sort descending prob\n",
    "        order = np.argsort(-p)\n",
    "        out_idx[tok] = idx[order]\n",
    "        out_p[tok]   = p[order]\n",
    "\n",
    "    # return as simple dict-of-tuples for backward compatibility\n",
    "    bigram_db = {int(t): (out_idx[t], out_p[t]) for t in range(vocab_size)}\n",
    "    return bigram_db\n",
    "\n",
    "print(\"Building bigram probability distribution...\")\n",
    "bigram_db = build_bigram_distribution_fixed(train_ids, vocab_size)\n",
    "\n",
    "# === Save ===\n",
    "model_dir = \"./markov_bigram_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(model_dir, \"markov_models.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(markov_models, f)\n",
    "\n",
    "with open(os.path.join(model_dir, \"bigram_db.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(bigram_db, f)\n",
    "\n",
    "print(\"‚úÖ Markov and Bigram models saved.\")\n",
    "print(f\"Chains: {[f'order={w}' for w in windows]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57eff45-c482-4e59-ace0-5c35e65c879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd4dd64a-8f8e-471e-97f2-7ffd1df4c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "# === Load tokenizer metadata ===\n",
    "with open(os.path.join(data_dir, 'meta.pkl'), 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "vocab_size = meta['vocab_size']\n",
    "\n",
    "# === Load mmap edata (char-level tokens, uint16) ===\n",
    "train_ids = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_ids   = np.memmap(os.path.join(data_dir, 'val.bin'),   dtype=np.uint16, mode='r')\n",
    "\n",
    "# === Replacement dataloader that uses SAVED bigram + markov models and yields (X, Y, Z) ===\n",
    "import os, pickle, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# expects `vocab_size` and `device` already defined in the outer scope\n",
    "# expects saved models at ./markov_bigram_models/{bigram_db.pkl, markov_models.pkl}\n",
    "\n",
    "\n",
    "\n",
    "class ZPack:\n",
    "    __slots__ = (\"blocks\",)\n",
    "    def __init__(self, blocks):\n",
    "        # blocks = [(idxs_np, probs_np), ...] length 4, each np arrays shape (B,T,32)\n",
    "        self.blocks = blocks\n",
    "    def __getitem__(self, i):\n",
    "        return self.blocks[i]  # allows model to access zb[bidx] -> (idxs_np, probs_np)\n",
    "    # no __len__ and no Sequence inheritance => collate treats this as an opaque object\n",
    "\n",
    "class GPUBatchDataset(Dataset):\n",
    "    def __init__(self, mmap_file, block_size, batch_size, device,\n",
    "                 model_dir=\"./markov_bigram_models\", jitter=63, p_aligned=0.5, pad_len=0,\n",
    "                 top_k=16, seed=1337):\n",
    "        self.data = mmap_file\n",
    "        self.block_size = int(block_size)\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.device = device\n",
    "        self.pad_len = int(pad_len)\n",
    "        self.sample_len = self.block_size + self.pad_len\n",
    "        self.total = len(self.data) - self.sample_len - 1\n",
    "        self.n_blocks = max(1, self.total // self.sample_len)\n",
    "        self.jitter = int(jitter)\n",
    "        self.p_aligned = float(p_aligned)\n",
    "        self.top_k = int(top_k)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        with open(os.path.join(model_dir, \"bigram_db.pkl\"), \"rb\") as f:\n",
    "            self.bigram_db = pickle.load(f)\n",
    "        with open(os.path.join(model_dir, \"markov_models.pkl\"), \"rb\") as f:\n",
    "            self.markov_models = pickle.load(f)  # {4:..., 8:..., 16:...}\n",
    "\n",
    "        assert isinstance(vocab_size, int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total // self.batch_size\n",
    "\n",
    "    def _sample_block(self):\n",
    "        base_block = self.rng.integers(0, self.n_blocks)\n",
    "        start = base_block * self.sample_len\n",
    "        if self.rng.random() > self.p_aligned:\n",
    "            j = self.rng.integers(0, self.jitter + 1)\n",
    "            start = min(start + j, self.total)\n",
    "        return start\n",
    "\n",
    "    def _finalize_topk_from_counts(self, counter, top_k=16, epsilon=1e-6):\n",
    "        rng = self.rng\n",
    "        if not counter:\n",
    "            idxs = rng.choice(vocab_size, size=top_k, replace=False)\n",
    "            probs = np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "            return idxs.astype(np.int64), probs\n",
    "        items = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        obs_idxs = np.fromiter((t for t, _ in items), dtype=np.int64, count=len(items))\n",
    "        obs_cnts = np.fromiter((c for _, c in items), dtype=np.float64, count=len(items))\n",
    "        if len(obs_idxs) >= top_k:\n",
    "            idxs = obs_idxs[:top_k]\n",
    "            probs = (obs_cnts[:top_k] / obs_cnts[:top_k].sum()).astype(np.float32)\n",
    "            return idxs, probs\n",
    "        need = top_k - len(obs_idxs)\n",
    "        mask = np.ones(vocab_size, dtype=bool); mask[obs_idxs] = False\n",
    "        extras = rng.choice(np.nonzero(mask)[0], size=need, replace=False).astype(np.int64)\n",
    "        idxs = np.concatenate([obs_idxs, extras])\n",
    "        probs = np.concatenate([obs_cnts, np.full(need, epsilon, dtype=np.float64)]).astype(np.float32)\n",
    "        probs = probs / probs.sum()\n",
    "        return idxs, probs\n",
    "\n",
    "    def _finalize_topk_from_bigram(self, entry, top_k=16, epsilon=1e-6):\n",
    "        rng = self.rng\n",
    "        if entry is None:\n",
    "            idxs = rng.choice(vocab_size, size=top_k, replace=False).astype(np.int64)\n",
    "            probs = np.full(top_k, 1.0/top_k, dtype=np.float32)\n",
    "            return idxs, probs\n",
    "        idxs, probs = entry\n",
    "        idxs = np.asarray(idxs, dtype=np.int64)\n",
    "        probs = np.asarray(probs, dtype=np.float32)\n",
    "        if idxs.shape[0] > top_k:\n",
    "            order = np.argsort(-probs)[:top_k]\n",
    "            idxs, probs = idxs[order], probs[order]\n",
    "        elif idxs.shape[0] < top_k:\n",
    "            need = top_k - idxs.shape[0]\n",
    "            mask = np.ones(vocab_size, dtype=bool); mask[idxs] = False\n",
    "            extras = rng.choice(np.nonzero(mask)[0], size=need, replace=False).astype(np.int64)\n",
    "            idxs = np.concatenate([idxs, extras])\n",
    "            probs = np.concatenate([probs, np.full(need, epsilon, dtype=np.float32)])\n",
    "        probs = probs / probs.sum()\n",
    "        return idxs, probs\n",
    "\n",
    "    def _dist_bigram(self, tok):\n",
    "        entry = self.bigram_db.get(int(tok), None)\n",
    "        return self._finalize_topk_from_bigram(entry, top_k=self.top_k)\n",
    "\n",
    "    def _dist_markov(self, ctx_tuple, backoff_tok):\n",
    "        counter = None\n",
    "        if ctx_tuple is not None:\n",
    "            chain = self.markov_models.get(len(ctx_tuple), {})\n",
    "            counter = chain.get(ctx_tuple, None)\n",
    "        if counter:\n",
    "            return self._finalize_topk_from_counts(counter, top_k=self.top_k)\n",
    "        return self._dist_bigram(backoff_tok)\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        B, T, K = self.batch_size, self.block_size, self.top_k\n",
    "        X = np.empty((B, self.sample_len), dtype=np.int64)\n",
    "        Y = np.empty((B, T), dtype=np.int64)\n",
    "\n",
    "        # preallocate Z blocks\n",
    "        bi_idx  = np.empty((B, T, K), dtype=np.int64); bi_p  = np.empty((B, T, K), dtype=np.float32)\n",
    "        m4_idx  = np.empty((B, T, K), dtype=np.int64); m4_p  = np.empty((B, T, K), dtype=np.float32)\n",
    "        m8_idx  = np.empty((B, T, K), dtype=np.int64); m8_p  = np.empty((B, T, K), dtype=np.float32)\n",
    "        m16_idx = np.empty((B, T, K), dtype=np.int64); m16_p = np.empty((B, T, K), dtype=np.float32)\n",
    "\n",
    "        for i in range(B):\n",
    "            start = self._sample_block()\n",
    "            X[i] = self.data[start : start + self.sample_len]\n",
    "            Y[i] = self.data[start + 1 + self.pad_len : start + 1 + self.pad_len + T]\n",
    "\n",
    "            for j in range(T):\n",
    "                tok_now = int(X[i, j])\n",
    "\n",
    "                # bigram\n",
    "                idxs, probs = self._dist_bigram(tok_now)\n",
    "                bi_idx[i, j, :] = idxs; bi_p[i, j, :] = probs\n",
    "\n",
    "                # contexts for markov\n",
    "                ctx4  = tuple(int(x) for x in X[i, j-3 :  j+1]) if j >= 3  else None\n",
    "                ctx8  = tuple(int(x) for x in X[i, j-7 :  j+1]) if j >= 7  else None\n",
    "                ctx16 = tuple(int(x) for x in X[i, j-15:  j+1]) if j >= 15 else None\n",
    "\n",
    "                idxs, probs = self._dist_markov(ctx4,  tok_now);  m4_idx[i, j, :]  = idxs; m4_p[i, j, :]  = probs\n",
    "                idxs, probs = self._dist_markov(ctx8,  tok_now);  m8_idx[i, j, :]  = idxs; m8_p[i, j, :]  = probs\n",
    "                idxs, probs = self._dist_markov(ctx16, tok_now);  m16_idx[i, j, :] = idxs; m16_p[i, j, :] = probs\n",
    "\n",
    "        # wrap Z so collate doesn't decompose it\n",
    "        Z = ZPack([\n",
    "            (bi_idx,  bi_p),\n",
    "            (m4_idx,  m4_p),\n",
    "            (m8_idx,  m8_p),\n",
    "            (m16_idx, m16_p),\n",
    "        ])\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(X).to(self.device, non_blocking=True),\n",
    "            torch.from_numpy(Y).to(self.device, non_blocking=True),\n",
    "            Z,  # opaque: collate will return [Z] for batch_size=1, so training uses zb=zb[0]\n",
    "        )\n",
    "\n",
    "# instantiate (unchanged outer config)\n",
    "model_dir = \"./markov_bigram_models\"\n",
    "train_dataset = GPUBatchDataset(\n",
    "    np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r'),\n",
    "    block_size=block_size,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    model_dir=model_dir,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efacb560-8e61-4e4e-9399-3c1611b476ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_keep_z(batch):\n",
    "    # batch: list of N items; each item is (X, Y, Z)\n",
    "    # X: (B, T_x) tensor on device\n",
    "    # Y: (B, T)   tensor on device\n",
    "    # Z: [(idxs, probs)] * 4, each np arrays (B, T, 32)\n",
    "    Xs, Ys, Zs = zip(*batch)  # tuples of length N\n",
    "\n",
    "    # stack X/Y across the outer dataloader batch (keeps them on the same device)\n",
    "    X = torch.cat(Xs, dim=0)\n",
    "    Y = torch.cat(Ys, dim=0)\n",
    "\n",
    "    # merge Z by concatenating along batch axis (axis=0) for each of the 4 blocks\n",
    "    merged_blocks = []\n",
    "    for b in range(4):\n",
    "        idxs_list  = [Z[b][0] for Z in Zs]  # list of np arrays (B_i, T, 32)\n",
    "        probs_list = [Z[b][1] for Z in Zs]\n",
    "        idxs  = np.concatenate(idxs_list,  axis=0)  # (sum B_i, T, 32)\n",
    "        probs = np.concatenate(probs_list, axis=0)  # (sum B_i, T, 32)\n",
    "        merged_blocks.append((idxs, probs))\n",
    "\n",
    "    return X, Y, merged_blocks\n",
    "\n",
    "# --- use the custom collate in your DataLoader (keep batch_size=1 as you have) ---\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_keep_z\n",
    ")\n",
    "\n",
    "\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb, zb in train_loader:\n",
    "        # xb: (B, T), yb: (B, T), zb: list of 4 tuples (np arrays (B,T,32))\n",
    "        logits, loss = model(xb, yb, zb)   # model unchanged\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        print(loss.item())\n",
    "    return total_loss / len(train_loader)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f66f46f4-d5f4-4085-ae51-a92758adb506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.88M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# === Config ===\n",
    "data_dir = \"./babylm_char_tokenized\"  # <- char-tokenized data\n",
    "block_size = 1024\n",
    "batch_size = 8\n",
    "\n",
    "config =  GPTConfig(\n",
    "    block_size,\n",
    "    vocab_size,\n",
    "    n_layer=4,      \n",
    "    n_head = 8,\n",
    "    n_embd =128)\n",
    "\n",
    "model = GPT(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "losses = []\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c43d3e5c-ba7e-4a1e-9b59-1b593316e2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006848\n"
     ]
    }
   ],
   "source": [
    "print(sum(param.numel() for param in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2187f4-6a37-4a77-9a37-b0dae6751d8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2866063714027405\n",
      "0.2804186940193176\n",
      "0.29430702328681946\n",
      "0.28242185711860657\n",
      "0.27890118956565857\n",
      "0.28030818700790405\n",
      "0.29218801856040955\n",
      "0.282606303691864\n",
      "0.29292792081832886\n",
      "0.2955491542816162\n",
      "0.28361839056015015\n",
      "0.2983800768852234\n",
      "0.28566843271255493\n",
      "0.2994258403778076\n",
      "0.29237082600593567\n",
      "0.2799804210662842\n",
      "0.28826794028282166\n",
      "0.2796483337879181\n",
      "0.2796468138694763\n",
      "0.2895428538322449\n",
      "0.2943086326122284\n",
      "0.27300262451171875\n",
      "0.28833654522895813\n",
      "0.2865556478500366\n",
      "0.298909991979599\n",
      "0.2812435030937195\n",
      "0.29573437571525574\n",
      "0.2899649143218994\n",
      "0.27059102058410645\n",
      "0.29005515575408936\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === Run Training ===\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Epoch {epoch:2d} | Train loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe76faa0-3933-416d-b923-993c67cfcf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dinguse your to vow.\n",
      "What king, sterpeatent, washint.\n",
      "'Tis come it a suw it.\n",
      "\n",
      "MARCIUS:\n",
      "I is nothict my deaths, let themselves then.\n",
      "\n",
      "MARCIUS:\n",
      "Ratclets not, Wasliets and to dose,\n",
      "pretiniof of the kings of mere a seclarm\n",
      "was to peacestioes of Sicille this success!\n",
      "\n",
      "MARCIUS:\n",
      "Nay, brother that friung safeves belad breased?\n",
      "\n",
      "CORIOLANUS:\n",
      "He speak With pred your be rich make at our time,\n",
      "Nor I fate, now is so thee.\n",
      "\n",
      "MARCIUS:\n",
      "GoneflAhest You have you all, mattere, but Pompey,\n",
      "I, your to be sentrumped, to York niship highnes\n",
      "Are hot name to succest will the so my maid,\n",
      "He had siling good husband; we shall then bare\n",
      "Aback prithe up it; Marctnagus it thine weallive\n",
      "Were is hollips throught grave dusly of your so.\n",
      "Ay, and matttle doest and she so.\n",
      "Rome thy friends fay uncle.\n",
      "\n",
      "CAMILLO:\n",
      "Which she cary fool-extred to them: tie your chare!\n",
      "We with leter you at before no him: I'll would\n",
      "A have of Mown, and shall fre's it the gentleman:\n",
      "I am thou hard bornitest words, but me,\n",
      "For trnocently thing do a sues. You am namtid's e\n",
      "But exp\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def decode_chars(token_ids, itos):\n",
    "    \"\"\"\n",
    "    Decodes a list of character token IDs into a string.\n",
    "    \"\"\"\n",
    "    return ''.join([itos[i] for i in token_ids])\n",
    "\n",
    "def encode_chars(text, stoi):\n",
    "    \"\"\"\n",
    "    Encodes a string into a list of token IDs, one per character.\n",
    "    \"\"\"\n",
    "    return [stoi.get(c, 0) for c in text]\n",
    "\n",
    "\n",
    "def decode_sequence_char(\n",
    "    model, stoi, itos, prompt, max_new_tokens=100, block_size=256,\n",
    "    use_fenchel=False, tau=1.0, fenchel_iters=3, temperature=1.0\n",
    "):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    max_ctx = int(block_size)\n",
    "\n",
    "    # ?1 assume space token exists; fallback to 0 if missing\n",
    "    space_id = stoi.get(' ', 0)\n",
    "\n",
    "    # encode prompt\n",
    "    start_ids = torch.tensor([encode_chars(prompt, stoi)], dtype=torch.long, device=device)\n",
    "\n",
    "    # prepend the pad once; from now on the window just slides\n",
    "\n",
    "    idx = start_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        context = idx[:, -max_ctx:]  # rotating buffer: last pad_len+block_size tokens\n",
    "        logits, _ = model(context, None)\n",
    "        last_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "\n",
    "    # drop the initial pad when returning the string\n",
    "    out_tokens = idx[0].tolist()\n",
    "    return decode_chars(out_tokens, itos)\n",
    "with open(\"./babylm_char_tokenized/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi = meta[\"stoi\"]\n",
    "itos = meta[\"itos\"]\n",
    "\n",
    "prompt = \"dingus\"\n",
    "generated = decode_sequence_char(\n",
    "    model=model,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=1024,\n",
    "    block_size=1024,\n",
    "    use_fenchel=False,\n",
    "    tau=1.5,\n",
    "    fenchel_iters=2,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee122bcb-2bba-41b4-8e62-8a1783c1f574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08025ccd-88ac-46f0-bbb6-7b5b3749ce6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056acdf-22c3-4e83-bdaa-dd6d2fbbeb83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
